{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f30b3909",
      "metadata": {
        "id": "f30b3909"
      },
      "source": [
        "# Echo cancelling\n",
        "\n",
        "In this assignment you will be asked to implement several flavours of convolution and implement a couple of basic acoustic echo cancelling (AEC) algorithms.\n",
        "\n",
        "This assigment does not require extensive computations.\n",
        "\n",
        "## Plan:\n",
        "1. Convolution in waveform and frequency domains \\[1 point\\]\n",
        "2. Waveform domain LMS \\[1 point\\]\n",
        "3. Spectral domain LMS \\[1 point\\]\n",
        "4. Convolution in time-frequency domain \\[2 points: this guy is hard\\]\n",
        "\n",
        "## Info\n",
        "This task includes not only code, but theoretical questions, too. They are marked like \"**YOUR ANSWER HERE**\". Please, fill in your answers there.\n",
        "\n",
        "## Sumbission:\n",
        "Your submission should include:\n",
        "1. The filled notebook\n",
        "2. Outputs of your echo cancelling methods for both real and synthetic examples\n",
        "3. Please, make a search by \"YOUR ANSWER\" to make sure you've found all the theoretical questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "959b360d",
      "metadata": {
        "id": "959b360d"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "Let's download it now and discuss it in the echo cancellong section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ae40f2",
      "metadata": {
        "id": "25ae40f2"
      },
      "outputs": [],
      "source": [
        "from io import BytesIO\n",
        "import os\n",
        "import requests\n",
        "from urllib.parse import urlencode\n",
        "from zipfile import ZipFile\n",
        "\n",
        "base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
        "public_key = 'https://disk.yandex.ru/d/C-o9WtGaLt8xow'\n",
        "\n",
        "final_url = base_url + urlencode(dict(public_key=public_key))\n",
        "response = requests.get(final_url)\n",
        "download_url = response.json()['href']\n",
        "response = requests.get(download_url)\n",
        "\n",
        "path_to_dataset = 'data/homework_vqe_2'    # Choose any appropriate local path\n",
        "\n",
        "zipfile = ZipFile(BytesIO(response.content))\n",
        "zipfile.extractall(path=path_to_dataset)\n",
        "\n",
        "ROOT_DATA = os.path.join(path_to_dataset, \"homework_2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9ba6057",
      "metadata": {
        "id": "f9ba6057"
      },
      "source": [
        "## Convolution in waveform domain \\[0.5 pts\\]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fef1bd9f",
      "metadata": {
        "id": "fef1bd9f"
      },
      "source": [
        "### Convolution\n",
        "\n",
        "Discrete convolution is defined for discrete finite or infinite sequences $x = (x_0, x_1, ...)$, $h=(h_0,h_1,...)$ by:\n",
        "\n",
        "$$(x * h)_k = \\sum_{i+j=k}x_i h_j$$\n",
        "\n",
        "\n",
        "Convolution can also be expressed in the operator form:\n",
        "\n",
        "$$x * h = \\sum_{t}x_t \\cdot T^t h \\text{, } t \\text { spans all the valid values}$$,\n",
        "\n",
        "where $T$ is the right shift operator, i.e.\n",
        "\n",
        "$$\\forall y=(y_0, y_1, ...) ~~ (Ty)_t = \\left\\{\n",
        "  \\begin{array}{ c l }\n",
        "    y_{t-1} & \\quad \\textrm{if } t \\geq 1 \\\\\n",
        "    0                 & \\quad \\textrm{if } t=0\n",
        "  \\end{array}\n",
        "\\right.$$\n",
        "\n",
        "and $T^t y$ means $T$ applied to $y$ $t$ times, i.e. $T^0 y=y$, $T^1 y=T(y)$, $T^2 y=T(T(y))$, etc.\n",
        "\n",
        "Now we are going to implement convolution by definition and in the operator form. It is not the fastest way to calculate it but in this way we can experience how it works."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c3017b",
      "metadata": {
        "id": "b6c3017b"
      },
      "source": [
        "First of all, let's prepare the inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51edcdd7",
      "metadata": {
        "id": "51edcdd7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.signal as sig\n",
        "import soundfile as sf\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a9381b2",
      "metadata": {
        "id": "8a9381b2"
      },
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(seed=17)\n",
        "RANDOM_SIGNAL = rng.uniform(-1, 1, 1600)\n",
        "RANDOM_KERNEL = rng.uniform(-1, 1, 800)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce067678",
      "metadata": {
        "id": "ce067678"
      },
      "source": [
        "This is how convolution is done in scipy, you could have seen it in previous assignments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d8dce79",
      "metadata": {
        "id": "8d8dce79"
      },
      "outputs": [],
      "source": [
        "convolved_scipy = sig.convolve(RANDOM_SIGNAL, RANDOM_KERNEL)\n",
        "print(len(RANDOM_SIGNAL), len(RANDOM_KERNEL), len(convolved_scipy))\n",
        "assert len(convolved_scipy) == len(RANDOM_SIGNAL) + len(RANDOM_KERNEL) - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4082daf",
      "metadata": {
        "id": "d4082daf"
      },
      "source": [
        "For finite-length signals $\\text{len}(x * h) = \\text{len}(x) + \\text{len}(h) - 1$\n",
        "Why? Please, try to explain it:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "713d2a03",
      "metadata": {
        "id": "713d2a03"
      },
      "source": [
        "**YOUR ANSWER HERE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e77f7cad",
      "metadata": {
        "id": "e77f7cad"
      },
      "outputs": [],
      "source": [
        "def convolve_by_definition(x: np.ndarray, h: np.ndarray) -> np.ndarray:\n",
        "    assert x.ndim == h.ndim == 1, (x.shape, h.shape)\n",
        "    result = np.zeros(len(x) + len(h) - 1)\n",
        "    for out_idx in tqdm(range(len(result))):\n",
        "        x_idx_min = max(0,  # your code\n",
        "        x_idx_max = min(len(x) - 1,  # your code\n",
        "        for x_idx in range(x_idx_min, x_idx_max + 1):\n",
        "            h_idx =  # your code\n",
        "            result[out_idx] += x[x_idx] * h[h_idx]\n",
        "    return result\n",
        "\n",
        "\n",
        "convolved = convolve_by_definition(RANDOM_SIGNAL, RANDOM_KERNEL)\n",
        "diff = np.abs(convolved - convolved_scipy).max()\n",
        "assert np.abs(diff) < 1e-10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "060e7dc2",
      "metadata": {
        "id": "060e7dc2"
      },
      "source": [
        "The inner cycle can be replaced with dot-product, just remember to flip the kernel part:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58ed82b0",
      "metadata": {
        "id": "58ed82b0"
      },
      "outputs": [],
      "source": [
        "def convolve_dot_product(x: np.ndarray, h: np.ndarray) -> np.ndarray:\n",
        "    assert x.ndim == h.ndim == 1, (x.shape, h.shape)\n",
        "    result = np.zeros(len(x) + len(h) - 1)\n",
        "    for out_idx in tqdm(range(len(result))):\n",
        "        x_idx_min =  # your code\n",
        "        x_idx_max =  # your code\n",
        "\n",
        "        h_idx_min =  # your code\n",
        "        h_idx_max =  # your code\n",
        "\n",
        "        x_crop = x[x_idx_min: x_idx_max + 1]\n",
        "        h_crop = h[h_idx_min: h_idx_max + 1]\n",
        "\n",
        "        h_crop_flipped =   # your code: use np.flip\n",
        "\n",
        "        result[out_idx] = np.dot(x_crop, h_crop_flipped)\n",
        "    return result\n",
        "\n",
        "convolved = convolve_dot_product(RANDOM_SIGNAL, RANDOM_KERNEL)\n",
        "diff = np.abs(convolved - convolved_scipy).max()\n",
        "assert np.abs(diff) < 1e-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "907c370c",
      "metadata": {
        "id": "907c370c"
      },
      "outputs": [],
      "source": [
        "def convololve_operator_form(x: np.ndarray, h: np.ndarray) -> np.ndarray:\n",
        "    assert x.ndim == h.ndim == 1, (x.shape, h.shape)\n",
        "    result = np.zeros(len(x) + len(h) - 1)\n",
        "    for x_idx in tqdm(range(len(x))):\n",
        "        result[x_idx: x_idx + len(h)] +=  # your code\n",
        "    return result\n",
        "\n",
        "\n",
        "convolved = convololve_operator_form(RANDOM_SIGNAL, RANDOM_KERNEL)\n",
        "diff = np.abs(convolved - convolved_scipy).max()\n",
        "assert np.abs(diff) < 1e-10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bf2be49",
      "metadata": {
        "id": "9bf2be49"
      },
      "source": [
        "**Question:** what is the complexity asymptotics of direct convolution computation which you have implemented above?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4302407",
      "metadata": {
        "id": "f4302407"
      },
      "source": [
        "**YOUR ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65564cb0",
      "metadata": {
        "id": "65564cb0"
      },
      "source": [
        "**Note:** What will happen if we zero-pad $x$ or $h$ in a convolution? It can be seen from the operator form: the output will be the same, but equally padded:\n",
        "\n",
        "$$\n",
        "\\forall \\text{ Zero-padding } P ~~~ x * P(h) = P(x) * h = P(x * h)\n",
        "$$\n",
        "Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfc3ef19",
      "metadata": {
        "id": "dfc3ef19"
      },
      "outputs": [],
      "source": [
        "left_padding = np.zeros(10)\n",
        "right_padding = np.zeros(20)\n",
        "h_padded = np.concatenate([left_padding, RANDOM_KERNEL, right_padding])\n",
        "x_padded = np.concatenate([left_padding, RANDOM_SIGNAL, right_padding])\n",
        "\n",
        "convolved_padded_kernel = sig.convolve(RANDOM_SIGNAL, h_padded)\n",
        "convolved_padded_signal = sig.convolve(x_padded, RANDOM_KERNEL)\n",
        "\n",
        "convolve_padded_result = np.concatenate([left_padding, convolved_scipy, right_padding])\n",
        "\n",
        "assert np.abs(convolved_padded_kernel - convolve_padded_result).max() < 1e-10\n",
        "assert np.abs(convolved_padded_signal - convolve_padded_result).max() < 1e-10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57e75f15",
      "metadata": {
        "id": "57e75f15"
      },
      "source": [
        "## Convolution in frequency domain \\[0.5 pts\\]\n",
        "\n",
        "Let $\\mathcal F$ denote **DFT, i.e. discrete Fourier transform**. Typically the **FFT, i.e. Fast Fourier Transform** algorithm is used to calculate DFT. FFT has a complexity asymptotics of $O(n \\log n)$, while the naive DFT computation as matrix-vector multiplication has the complexity asymptotics of $(n^2)$, where $n$ is the size of an input. FFT is so popular that DFT is often referred to as FFT.\n",
        "\n",
        "One of the properties of DFT is that it \"turns convolution into multiplication\":\n",
        "\n",
        "$$x * h = \\mathcal F^{-1} (\\mathcal F(x) \\cdot \\mathcal F(h))$$\n",
        "\n",
        "The equality holds true for mathematical reasons. However, we need to clarify what $\\mathcal F$ means here. Let's take a closer look at that."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11523603",
      "metadata": {
        "id": "11523603"
      },
      "source": [
        "**This is how DFT and inverse DFT (IDFT) can be done in numpy.** Note that later we will exploit symmetry and use a better option:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46047d03",
      "metadata": {
        "id": "46047d03"
      },
      "outputs": [],
      "source": [
        "x_dft = np.fft.fft(RANDOM_SIGNAL)\n",
        "x_restored = np.fft.ifft(x_dft)\n",
        "\n",
        "print(x_dft.shape, x_dft.dtype)\n",
        "\n",
        "assert np.abs(x_restored - RANDOM_SIGNAL).max() < 1e-10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73ae40f5",
      "metadata": {
        "id": "73ae40f5"
      },
      "source": [
        "DFT operates on complex numbers and preserves the size of its input. So we cannot just multiply the DFT's of $x$ and $h$.\n",
        "\n",
        "But we have seen above that zero padding does not essentially modify the result of a convolution. Let's try to pad $h$ to the length of $x$ so that multiplication would become possible. Will our frequency domain convolution formula work now?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "828abbe1",
      "metadata": {
        "id": "828abbe1"
      },
      "outputs": [],
      "source": [
        "h_padded = np.concatenate([RANDOM_KERNEL, np.zeros(len(RANDOM_SIGNAL) - len(RANDOM_KERNEL))])\n",
        "x_dft = np.fft.fft(RANDOM_SIGNAL)\n",
        "h_dft = np.fft.fft(h_padded)\n",
        "\n",
        "prod_dft = x_dft * h_dft\n",
        "\n",
        "convolved_from_dft = np.fft.ifft(prod_dft)\n",
        "\n",
        "print(convolved_scipy.shape, convolved_from_dft.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e237a09f",
      "metadata": {
        "id": "e237a09f"
      },
      "source": [
        "That doesn't work: the output shapes do not match!\n",
        "\n",
        "Our naive spectral domain convolution output has the length equal to $\\text{len}(x)$ while the real convolution output has the length of $\\text{len}(x) + \\text{len}(h) - 1$.\n",
        "\n",
        "The spectral domain representation should be long enough to represent a sequence of length $\\text{len}(x) + \\text{len}(h) - 1$. So let's zero-pad both $x$ and $h$ to that size!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c337e89e",
      "metadata": {
        "id": "c337e89e"
      },
      "outputs": [],
      "source": [
        "target_length = len(RANDOM_SIGNAL) + len(RANDOM_KERNEL) - 1\n",
        "h_padded =  # your code: zero-pad to target_length\n",
        "x_padded =  # your code: zero-pad to target_length\n",
        "x_dft = np.fft.fft(x_padded)\n",
        "h_dft = np.fft.fft(h_padded)\n",
        "prod_dft =  # your code: evaluate the product of the DFT's\n",
        "convolved_from_dft = np.fft.ifft(prod_dft)\n",
        "\n",
        "assert np.abs(convolved_from_dft - convolved_scipy).max() < 1e-10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2184f4e4",
      "metadata": {
        "id": "2184f4e4"
      },
      "source": [
        "It works, we did it!\n",
        "\n",
        "Of course there is a mathematical proof behind it, but this is what we should remember when we do convolution in spectral domain in practice.\n",
        "\n",
        "Technically the output is complex with a very small imaginary part:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "861c21ae",
      "metadata": {
        "id": "861c21ae"
      },
      "outputs": [],
      "source": [
        "print(convolved_from_dft.dtype, np.abs(convolved_from_dft.imag).max())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb3b6a1f",
      "metadata": {
        "id": "eb3b6a1f"
      },
      "source": [
        "But for real-values inputs, DFT features a symmetry:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b278e9c",
      "metadata": {
        "id": "3b278e9c"
      },
      "outputs": [],
      "source": [
        "_, axes = plt.subplots(ncols=2, figsize=(16, 5))\n",
        "\n",
        "sym_pt = (len(x_dft) - 2) / 2\n",
        "\n",
        "ax = axes[0]\n",
        "ax.set_title(\"Real\")\n",
        "vals = x_dft[1:].real\n",
        "ax.plot(vals)\n",
        "ax.axvline(sym_pt, color=\"red\", linestyle=\"--\")\n",
        "ax.grid()\n",
        "\n",
        "ax = axes[1]\n",
        "vals = x_dft[1:].imag\n",
        "ax.set_title(\"Imaginary\")\n",
        "ax.plot(vals)\n",
        "ax.plot(sym_pt, 0, \"ro\")\n",
        "ax.axvline(sym_pt, color=\"red\", linestyle=\"--\")\n",
        "ax.axhline(0, color=\"red\", linestyle=\"--\")\n",
        "ax.grid()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ed5d973",
      "metadata": {
        "id": "7ed5d973"
      },
      "source": [
        "The real part features an axial symmetry, the imaginary part features point symmetry (greetings the first lectutre!):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36a95c53",
      "metadata": {
        "id": "36a95c53"
      },
      "outputs": [],
      "source": [
        "assert np.abs(x_dft[1:].real - np.flip(x_dft[1:]).real).max() < 1e-10\n",
        "assert np.abs(x_dft[1:].imag + np.flip(x_dft[1:]).imag).max() < 1e-10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b371f0f",
      "metadata": {
        "id": "4b371f0f"
      },
      "source": [
        "Why do we truncate-out the 0-th element? It is just the sum of elements in $x$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e8a0726",
      "metadata": {
        "id": "1e8a0726"
      },
      "outputs": [],
      "source": [
        "x_dft[0], np.sum(RANDOM_SIGNAL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b7d01ec",
      "metadata": {
        "id": "9b7d01ec"
      },
      "source": [
        "Thanks to the symmetry we can drop half of the elements of complex fft. Numpy and PyTorch provide such an option. In \"rfft\" \"r\" stands for \"real\".\n",
        "\n",
        "It is more convenient to have lengths divisible by 2 here which can always be achieved by a small padding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89eb9a3a",
      "metadata": {
        "id": "89eb9a3a"
      },
      "outputs": [],
      "source": [
        "target_length = len(RANDOM_SIGNAL) + len(RANDOM_KERNEL) - 1\n",
        "if target_length % 2 == 1:\n",
        "    target_length_even = target_length + 1\n",
        "else:\n",
        "    target_length_even = target_length\n",
        "h_padded =  # zero-pad to target_length_even\n",
        "x_padded =  # zero-pad to target_length_even\n",
        "\n",
        "x_dft_re = np.fft.rfft(x_padded)\n",
        "h_dft_re = np.fft.rfft(h_padded)\n",
        "prod_dft =  # your code: evaluate the product of the DFT's\n",
        "convolved_from_dft = np.fft.irfft(prod_dft)\n",
        "\n",
        "assert np.abs(convolved_from_dft[:target_length] - convolved_scipy).max() < 1e-10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ae0d78",
      "metadata": {
        "id": "22ae0d78"
      },
      "source": [
        "### Convolution in PyTorch\n",
        "\n",
        "In the future echo cancelling sections we are going to run gradient descent, differentiating through convolution.\n",
        "We can calculate the gradients manually, however it is not a goal of this task.\n",
        "So, let's use PyTorch for gradient calculation. For that we need to understand how to implement a regular convolution in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3a3eeee",
      "metadata": {
        "id": "a3a3eeee"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "969551b5",
      "metadata": {
        "id": "969551b5"
      },
      "source": [
        "Convolution in PyTorch requires:\n",
        "- *batch* and *channel* dimensions for the signal\n",
        "- *in_channels* and *out_channels* dimensions for the kernel.\n",
        "For a regular convolution of 1D signals all those channels sizes should be set to 1.\n",
        "PyTorch makes a convolution with the \"valid\" mode, the mode is controlled by padding.\n",
        "\n",
        "Let's try and see what we get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "117cf34c",
      "metadata": {
        "id": "117cf34c"
      },
      "outputs": [],
      "source": [
        "x_pt = torch.from_numpy(RANDOM_SIGNAL).reshape(1, 1, -1)\n",
        "h_pt = torch.from_numpy(RANDOM_KERNEL).reshape(1, 1, -1)\n",
        "\n",
        "convolved_from_pt = F.conv1d(x_pt, h_pt).reshape(-1).numpy()\n",
        "convolved_scipy_valid = sig.convolve(RANDOM_SIGNAL, RANDOM_KERNEL, mode=\"valid\")\n",
        "\n",
        "print(\"difference:\", np.square(convolved_from_pt - convolved_scipy_valid).mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "303a0ec2",
      "metadata": {
        "id": "303a0ec2"
      },
      "source": [
        "Pretty bad, isn't it? We missed one detail.\n",
        "\n",
        "PyTorch defines convolution primarily for neural network training. If we want to match the mathematical definition of convolution in PyTorch, we should flip the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f28b3c44",
      "metadata": {
        "id": "f28b3c44"
      },
      "outputs": [],
      "source": [
        "def convolve_pt(x: torch.Tensor, h: torch.tensor, mode: str = \"valid\"):\n",
        "    assert x.ndim in [1, 3], x.shape\n",
        "    assert h.ndim in [1, 3], h.shape\n",
        "    if x.ndim != 3:\n",
        "        x = x.reshape(1, 1, -1)\n",
        "    if h.ndim != 3:\n",
        "        h = h.reshape(1, 1, -1)\n",
        "    h =  # your code: use torch.flip\n",
        "    if mode == \"full\":\n",
        "        x = F.pad(x, pad=[h.shape[-1] - 1] * 2)\n",
        "    else:\n",
        "        assert mode == \"valid\", mode\n",
        "    result = F.conv1d(x, h).reshape(-1)\n",
        "    return result\n",
        "\n",
        "\n",
        "for mode in [\"full\", \"valid\"]:\n",
        "    convolved_from_pt = convolve_pt(\n",
        "        torch.from_numpy(RANDOM_SIGNAL),\n",
        "        torch.from_numpy(RANDOM_KERNEL),\n",
        "        mode,\n",
        "    ).numpy()\n",
        "    ref = sig.convolve(RANDOM_SIGNAL, RANDOM_KERNEL, mode=mode)\n",
        "    diff = np.square(convolved_from_pt - ref).mean()\n",
        "    print(f\"Mode: {mode}\", \", difference:\", diff)\n",
        "    assert diff < 1e-10, diff"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a6f0b4a",
      "metadata": {
        "id": "6a6f0b4a"
      },
      "source": [
        "## Echo cancelling as it is\n",
        "\n",
        "In echo cancelling we are provided the signal on a mic $y$, and the reference signal $s$ which was played by the speaker. The goal is to restore the near-end signal $n$ i.e. the sound on the mic which did not originate from the speaker.\n",
        "\n",
        "The sound on the mic, originating from the speaker is called echo, let's denote is as $e$.\n",
        "\n",
        "Similar to noise reduction:\n",
        "$y = e + n$\n",
        "\n",
        "We will make the following assmuptions:\n",
        "1. $y = s * h + n$: the acoustics is linear, the loudspeaker and the microphone are ideal\n",
        "2. $n$ and $e$ are uncorrelated"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48152eaa",
      "metadata": {
        "id": "48152eaa"
      },
      "source": [
        "### Data\n",
        "\n",
        "We will use 2 sets of files: one is fully real, the other one is a mixture of a real echo with clean speech and thus we call it synthetic.\n",
        "\n",
        "Echo was captured from a laptop by simultaneous play of a known audio file and recording. The played file is used as reference, the microphone and reference channels were aligned manually.\n",
        "\n",
        "For the real mixture a video was played on a smartphone simultaneously and it represents the near-end speech we aim to restore.\n",
        "\n",
        "Why do we use a synthetic mixture when we have a real file? We need an access to the near-end and echo channels for evaluation, and those are only available for synthetic files.\n",
        "\n",
        "Please, take your time to listen to the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09112e18",
      "metadata": {
        "id": "09112e18"
      },
      "outputs": [],
      "source": [
        "SR = 16_000\n",
        "\n",
        "\n",
        "def read_wav(path) -> np.ndarray:\n",
        "    x, sr = sf.read(path)\n",
        "    assert sr == SR, (path, sr)\n",
        "    return x\n",
        "\n",
        "\n",
        "DATA = {\n",
        "    \"real\": {\n",
        "        \"mic\": read_wav(os.path.join(ROOT_DATA, \"real/mic.wav\")),\n",
        "        \"ref\": read_wav(os.path.join(ROOT_DATA, \"real/ref.wav\")),\n",
        "        \"near_end\": None,\n",
        "        \"echo\": None,\n",
        "    },\n",
        "    \"synth\": {\n",
        "        \"mic\": read_wav(os.path.join(ROOT_DATA, \"synth/mic.wav\")),\n",
        "        \"ref\": read_wav(os.path.join(ROOT_DATA, \"synth/ref.wav\")),\n",
        "        \"near_end\": read_wav(os.path.join(ROOT_DATA, \"synth/near.wav\")),\n",
        "    }\n",
        "}\n",
        "\n",
        "for key in [\"synth\"]:\n",
        "    DATA[key][\"echo\"] = DATA[key][\"mic\"] - DATA[key][\"near_end\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "680420fd",
      "metadata": {
        "id": "680420fd"
      },
      "source": [
        "### Waveform Domain  LMS \\[0.5 pt\\]:\n",
        "\n",
        "LMS stands for least mean squares.\n",
        "For AEC we are given:\n",
        "- $y \\in \\mathbb R^T$: signal on the mic\n",
        "- $s \\in \\mathbb R^T$: signal on the speaker, also referred to as reference\n",
        "\n",
        "Least mean squares solves finds the impulse response $\\hat h$ which minimizes the following expression:\n",
        "\n",
        "$$\\hat h = \\underset{h}{\\operatorname{argmin}} ||y - s * h||_2 = \\underset{h}{\\operatorname{argmin}} \\frac{1}{T}\\sum_{t=0}^{T-1}(y_t - (s * h)_t)^2$$\n",
        "\n",
        "With $\\hat h$ we can estimate echo: $\\hat e = s * \\hat h$ and the near-end signal: $\\hat n = y - \\hat e$.\n",
        "\n",
        "Wait, but the size of $s*h$ should be\n",
        "$\\text{len}(s) + \\text{len}(h) - 1 > \\text{len}(s)$ if $\\text{len}(h)>1$!\n",
        "\n",
        "It's ok, we ignore the last $\\text{len}(h) - 1$ samples of $s * h$. Why? Because we only need to suppress the echo, e.g. during a call of given length $T$.\n",
        "\n",
        "**We'll use gradient descent for this task**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d59f217",
      "metadata": {
        "id": "1d59f217"
      },
      "outputs": [],
      "source": [
        "class Convolution(nn.Module):\n",
        "    \"\"\"\n",
        "    This class will be used to find the convolutional kernel\n",
        "    (a.k.a. room impulse response) by gradient descent.\n",
        "\n",
        "    Instead of flipping the kernel on each self.forward\n",
        "    we will use the PyTorch-style convolution in self.forward\n",
        "    and flip the PyTorch-style kernel when we will need to access the classical kernel.\n",
        "    \"\"\"\n",
        "    def __init__(self, kernel_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.padding = nn.ConstantPad1d([kernel_size - 1, 0], 0)\n",
        "        self.conv = nn.Conv1d(  # your code, bias=False)\n",
        "        nn.init.zeros_(self.conv.weight)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        assert x.ndim == 3, x.shape\n",
        "        # your code: apply self.padding and self.conv\n",
        "        assert result.shape == x.shape, (result.shape, x.shape)\n",
        "        return result\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def apply(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Like self.forward, but for 1-D np.ndarrays\n",
        "        \"\"\"\n",
        "        assert x.ndim == 1, x.shape\n",
        "        x = torch.tensor(x, dtype=self.conv.weight.dtype, device=self.conv.weight.device)\n",
        "        x = x.reshape(1, 1, -1)\n",
        "        result = self.forward(x)\n",
        "        result = result[0, 0].cpu().numpy()\n",
        "        return result\n",
        "\n",
        "    def get_kernel(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Transforms the convolutional kernel from PyTorch respresentation\n",
        "        to the classical one.\n",
        "        \"\"\"\n",
        "        return torch.flip(self.conv.weight.data[0, 0], [-1]).detach().cpu().numpy()\n",
        "\n",
        "    def set_kernel(self, kernel: np.ndarray) -> None:\n",
        "        kernel_cur = self.conv.weight\n",
        "        kernel = torch.tensor(kernel, dtype=kernel_cur.dtype, device=kernel_cur.device)\n",
        "        assert kernel.ndim == 1, kernel.shape\n",
        "        assert len(kernel) == kernel_cur.shape[-1], (len(kernel), kernel_cur.shape)\n",
        "        kernel = torch.flip(kernel, [-1]).reshape(1, 1, -1)\n",
        "        self.conv.weight.data = kernel\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    conv = Convolution(len(RANDOM_KERNEL))\n",
        "    conv.set_kernel(torch.tensor(RANDOM_KERNEL, dtype=torch.float))\n",
        "\n",
        "    x_pt = torch.tensor(RANDOM_SIGNAL, dtype=torch.float).reshape(1, 1, -1)\n",
        "    convolved = conv(x_pt).squeeze()\n",
        "    diff = np.abs(convolved.numpy() - convolved_scipy[:len(convolved)]).mean()\n",
        "    assert diff < 1e-5\n",
        "    print(\"shapes:\", convolved.shape, RANDOM_SIGNAL.shape)\n",
        "\n",
        "    convolved = conv.apply(RANDOM_SIGNAL)\n",
        "    diff = np.abs(convolved - convolved_scipy[:len(convolved)]).mean()\n",
        "    assert diff < 1e-5\n",
        "    print(\"shapes:\", convolved.shape, RANDOM_SIGNAL.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32de3352",
      "metadata": {
        "id": "32de3352"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.audio import SignalNoiseRatio\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a97511c",
      "metadata": {
        "id": "2a97511c"
      },
      "outputs": [],
      "source": [
        "def fit_waveform_lms(\n",
        "    mic: np.ndarray,\n",
        "    ref: np.ndarray,\n",
        "    echo: np.ndarray | None = None,\n",
        "    near_end: np.ndarray | None = None,\n",
        "    verbose=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Fits Waveform-domain LMS for mic and ref and calculates metrics during training.\n",
        "\n",
        "    Echo and near_end are unavailable in practice and should only be used for metrics calculation\n",
        "    If not provided, echo will be replaced by mic, near_end will be replaced by zeros\n",
        "    verbose may be useful for debug\n",
        "    \"\"\"\n",
        "    n_steps = 1_500\n",
        "    kernel_size_frames = int(0.25 * SR)\n",
        "    conv = Convolution(kernel_size_frames).to(device)\n",
        "    opt = torch.optim.Adam(conv.parameters(), lr=1e-2, weight_decay=0)\n",
        "\n",
        "    if echo is None:\n",
        "        echo = mic\n",
        "    if near_end is None:\n",
        "        near_end = np.zeros_like(mic)\n",
        "\n",
        "    mic_pt = torch.tensor(mic, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
        "    ref_pt = torch.tensor(ref, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
        "\n",
        "    echo_pt = torch.tensor(echo, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
        "    near_end_pt = torch.tensor(near_end, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
        "\n",
        "    criterion = nn.MSELoss().to(device)\n",
        "    snr_evaluator = SignalNoiseRatio().to(device)\n",
        "\n",
        "    logs = []\n",
        "\n",
        "    best_loss = None\n",
        "    best_loss_snapshot = None\n",
        "\n",
        "    for idx in tqdm(range(n_steps)):\n",
        "        echo_est =  # your code: run the conv module\n",
        "        loss =  # your code: calculate the loss using mic_pt as ground truth\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        # saving the best fit\n",
        "        if best_loss is None or loss < best_loss:\n",
        "            best_loss = loss.item()\n",
        "            best_loss_snapshot = conv.state_dict()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            loss_oracle =  # your code: calculate the loss using echo_pt as ground truth\n",
        "            snr_echo = snr_evaluator(  # your code: calculate SNR between echo_est and echo_pt\n",
        "            near_end_est = # your code: calculate as the difference of mic and echo_est\n",
        "            snr_near = snr_evaluator(  # your code: calculate SNR between near_end_est and near_end_pt\n",
        "\n",
        "        logs.append(\n",
        "            {\n",
        "                \"loss\": loss.item(),\n",
        "                \"loss_oracle\": loss_oracle.item(),\n",
        "                \"snr_echo\": snr_echo.item(),\n",
        "                \"snr_near\": snr_near.item(),\n",
        "                \"lr\": opt.param_groups[0][\"lr\"],\n",
        "            }\n",
        "        )\n",
        "\n",
        "        if verbose and (idx % 100 == 0 or idx + 1 == n_steps):\n",
        "            print(logs[-1])\n",
        "\n",
        "    # loading the best snapshot\n",
        "    final_model = Convolution(kernel_size_frames).to(device)\n",
        "    final_model.load_state_dict(best_loss_snapshot)\n",
        "    return final_model, logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c66bd0a",
      "metadata": {
        "id": "0c66bd0a"
      },
      "outputs": [],
      "source": [
        "ALL_LOGS = {}\n",
        "ALL_LOGS[\"waveform_lms\"] = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aacf6fa3",
      "metadata": {
        "id": "aacf6fa3"
      },
      "outputs": [],
      "source": [
        "convolution, logs = fit_waveform_lms(**DATA[\"synth\"])\n",
        "ALL_LOGS[\"waveform_lms\"][\"synth\"] = (convolution.cpu(), logs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "513cdb2e",
      "metadata": {
        "id": "513cdb2e"
      },
      "outputs": [],
      "source": [
        "_, axes = plt.subplots(ncols=2, figsize=(8, 4))\n",
        "\n",
        "ax = axes[0]\n",
        "ax.set_title(\"Loss\")\n",
        "for key in [\"loss\", \"loss_oracle\"]:\n",
        "    ax.plot([x[key] for x in logs], label=key)\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "ax.set_yscale(\"log\")\n",
        "\n",
        "ax = axes[1]\n",
        "ax.set_title(\"Impulse response estimate\")\n",
        "ax.plot(convolution.get_kernel())\n",
        "ax.grid()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74969f2a",
      "metadata": {
        "id": "74969f2a"
      },
      "source": [
        "**Question:** Is loss_oracle lower than loss? Why? Please, comment on the stability of the training procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da8a2aef",
      "metadata": {
        "id": "da8a2aef"
      },
      "source": [
        "**YOUR ANSWER HERE**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some useful methods\n",
        "\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "import IPython.display as ipd\n",
        "import os\n",
        "\n",
        "def calculate_metrics(mic, echo, near_end, output, echo_estimate):\n",
        "    echo_power = np.mean(echo**2) + 1e-10\n",
        "    error_power = np.mean((echo - echo_estimate)**2) + 1e-10\n",
        "    erle = 10 * np.log10(echo_power / error_power)\n",
        "\n",
        "    if np.any(near_end):\n",
        "        signal_power = np.mean(near_end**2) + 1e-10\n",
        "        noise_power = np.mean((near_end - output)**2) + 1e-10\n",
        "        snr = 10 * np.log10(signal_power / noise_power)\n",
        "    else:\n",
        "        snr = float(\"inf\")\n",
        "\n",
        "    echo_norm = np.sum(echo**2) + 1e-10\n",
        "    error_norm = np.sum((echo - echo_estimate)**2) + 1e-10\n",
        "    misalignment = 10 * np.log10(error_norm / echo_norm)\n",
        "\n",
        "    return {\n",
        "        \"ERLE\": erle,\n",
        "        \"SNR\": snr,\n",
        "        \"Misalignment\": misalignment\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_impulse_response(impulse_response):\n",
        "    \"\"\"Визуализация импульсного отклика фильтра\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(impulse_response)\n",
        "    plt.title(\"Impulse response\")\n",
        "    plt.xlabel(\"Position\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_results(mic, ref, near_end, echo, output, echo_estimate, use_spectral=False, fs=16000):\n",
        "    tmp_dir = 'tmp_audio'\n",
        "    if not os.path.exists(tmp_dir):\n",
        "        os.makedirs(tmp_dir)\n",
        "\n",
        "    wavfile.write(f\"{tmp_dir}/mic.wav\", fs, mic.astype(np.float32))\n",
        "    wavfile.write(f\"{tmp_dir}/ref.wav\", fs, ref.astype(np.float32))\n",
        "    wavfile.write(f\"{tmp_dir}/near_end.wav\", fs, near_end.astype(np.float32))\n",
        "    wavfile.write(f\"{tmp_dir}/echo.wav\", fs, echo.astype(np.float32))\n",
        "    wavfile.write(f\"{tmp_dir}/output.wav\", fs, output.astype(np.float32))\n",
        "    wavfile.write(f\"{tmp_dir}/echo_estimate.wav\", fs, echo_estimate.astype(np.float32))\n",
        "\n",
        "    if use_spectral:\n",
        "        num_rows = 8\n",
        "        plt.figure(figsize=(15, 20))\n",
        "    else:\n",
        "        num_rows = 4\n",
        "        plt.figure(figsize=(15, 12))\n",
        "\n",
        "    print(\"mic (echo + speech):\")\n",
        "    display(ipd.Audio(f\"{tmp_dir}/mic.wav\"))\n",
        "\n",
        "    print(\"ref (far end):\")\n",
        "    display(ipd.Audio(f\"{tmp_dir}/ref.wav\"))\n",
        "\n",
        "    print(\"near end\")\n",
        "    display(ipd.Audio(f\"{tmp_dir}/near_end.wav\"))\n",
        "\n",
        "    print(\"echo\")\n",
        "    display(ipd.Audio(f\"{tmp_dir}/echo.wav\"))\n",
        "\n",
        "    print(\"near end est.\")\n",
        "    display(ipd.Audio(f\"{tmp_dir}/output.wav\"))\n",
        "\n",
        "    print(\"echo est\")\n",
        "    display(ipd.Audio(f\"{tmp_dir}/echo_estimate.wav\"))\n",
        "\n",
        "    def plot_spectrogram(signal_data, ax, title):\n",
        "        f, t, Sxx = signal.spectrogram(signal_data, fs=fs, nperseg=1024, noverlap=512)\n",
        "        im = ax.pcolormesh(t, f, 10 * np.log10(Sxx + 1e-10), shading='gouraud', cmap='viridis')\n",
        "        ax.set_ylabel('Frequency [Hz]')\n",
        "        ax.set_xlabel('Time [sec]')\n",
        "        ax.set_title(title)\n",
        "        plt.colorbar(im, ax=ax, label='Magnitude [dB]')\n",
        "\n",
        "    plt.subplot(num_rows, 1, 1)\n",
        "    plt.plot(mic, label=\"Mic (echo + near_end)\")\n",
        "    plt.plot(ref, label=\"Reference\")\n",
        "    plt.plot(near_end, label=\"Near end\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Input signals\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    if use_spectral:\n",
        "        ax = plt.subplot(num_rows, 1, 2)\n",
        "        plot_spectrogram(mic, ax, \"Spectrogram of Mic signal\")\n",
        "\n",
        "    plt.subplot(num_rows, 1, 3 if use_spectral else 2)\n",
        "    plt.plot(echo_estimate, label=\"Estimated echo\")\n",
        "    plt.plot(echo, label=\"Real echo\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Echo signals\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    if use_spectral:\n",
        "        ax = plt.subplot(num_rows, 1, 4)\n",
        "        plot_spectrogram(echo, ax, \"Spectrogram of Real echo\")\n",
        "\n",
        "        ax = plt.subplot(num_rows, 1, 5)\n",
        "        plot_spectrogram(echo_estimate, ax, \"Spectrogram of Estimated echo\")\n",
        "\n",
        "    plt.subplot(num_rows, 1, 6 if use_spectral else 3)\n",
        "    plt.plot(output, label=\"Output signal (Estimated near-end)\")\n",
        "    plt.plot(near_end, label=\"True near-end\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Output signal\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if use_spectral:\n",
        "        ax = plt.subplot(num_rows, 1, 7)\n",
        "        plot_spectrogram(near_end, ax, \"Spectrogram of True near-end\")\n",
        "\n",
        "        ax = plt.subplot(num_rows, 1, 8)\n",
        "        plot_spectrogram(output, ax, \"Spectrogram of Output signal\")\n",
        "        plt.figure(figsize=(15, 10))\n",
        "\n",
        "        plt.subplot(2, 1, 1)\n",
        "        f_echo, t_echo, Sxx_echo = signal.spectrogram(echo, fs=fs, nperseg=1024, noverlap=512)\n",
        "        f_echo_est, t_echo_est, Sxx_echo_est = signal.spectrogram(echo_estimate, fs=fs, nperseg=1024, noverlap=512)\n",
        "\n",
        "        min_t = min(len(t_echo), len(t_echo_est))\n",
        "        min_f = min(len(f_echo), len(f_echo_est))\n",
        "\n",
        "        diff_echo = 10 * np.log10(Sxx_echo[:min_f, :min_t] + 1e-10) - 10 * np.log10(Sxx_echo_est[:min_f, :min_t] + 1e-10)\n",
        "        im = plt.pcolormesh(t_echo[:min_t], f_echo[:min_f], diff_echo, cmap='coolwarm', shading='gouraud')\n",
        "        plt.colorbar(im, label='Magnitude difference [dB]')\n",
        "        plt.title('Spectral difference: Real echo - Estimated echo')\n",
        "        plt.ylabel('Frequency [Hz]')\n",
        "        plt.xlabel('Time [sec]')\n",
        "\n",
        "        plt.subplot(2, 1, 2)\n",
        "        f_near, t_near, Sxx_near = signal.spectrogram(near_end, fs=fs, nperseg=1024, noverlap=512)\n",
        "        f_out, t_out, Sxx_out = signal.spectrogram(output, fs=fs, nperseg=1024, noverlap=512)\n",
        "\n",
        "        min_t = min(len(t_near), len(t_out))\n",
        "        min_f = min(len(f_near), len(f_out))\n",
        "\n",
        "        diff_near = 10 * np.log10(Sxx_near[:min_f, :min_t] + 1e-10) - 10 * np.log10(Sxx_out[:min_f, :min_t] + 1e-10)\n",
        "        im = plt.pcolormesh(t_near[:min_t], f_near[:min_f], diff_near, cmap='coolwarm', shading='gouraud')\n",
        "        plt.colorbar(im, label='Magnitude difference [dB]')\n",
        "        plt.title('Spectral difference: True near-end - Estimated near-end')\n",
        "        plt.ylabel('Frequency [Hz]')\n",
        "        plt.xlabel('Time [sec]')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "PRQBS4QScTu6"
      },
      "id": "PRQBS4QScTu6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming waveform domain  LMS \\[0.5 pt\\]:\n",
        "\n",
        "The adaptive LMS filter updates the estimate $h$ as new data arrives (streaming/online). Assume a filter of length $L$ ($h = [h_0, ..., h_{L-1}]^\\top$).\n",
        "\n",
        "At each step $t$:\n",
        "- Form a vector of the latest $L$ values of $s$:\n",
        "  $$\n",
        "  \\mathbf{s}_t = [s_t, s_{t-1}, ..., s_{t-L+1}]^\\top\n",
        "  $$\n",
        "  (use zero-padding for $t-k < 0$)\n",
        "- Estimate the echo at time $t$:\n",
        "  $$\n",
        "  \\hat e_t = \\mathbf{s}_t^\\top h\n",
        "  $$\n",
        "- Compute the error:\n",
        "  $$\n",
        "  e_t = y_t - \\hat e_t\n",
        "  $$\n",
        "- LMS update:\n",
        "  $$\n",
        "  h \\leftarrow h + \\eta\\, e_t\\, \\mathbf{s}_t\n",
        "  $$\n",
        "  where $\\eta$ is the learning rate.\n",
        "\n",
        "#### NLMS (Normalized LMS)\n",
        "\n",
        "NLMS is a modification of LMS, normalizing the step by the energy of the input vector:\n",
        "$$\n",
        "h \\leftarrow h + \\frac{\\eta}{\\| \\mathbf{s}_t \\|^2 + \\epsilon} \\, e_t\\, \\mathbf{s}_t\n",
        "$$\n",
        "where $\\epsilon$ is a small constant to avoid division by zero.\n"
      ],
      "metadata": {
        "id": "0B1AwZdrj-Y_"
      },
      "id": "0B1AwZdrj-Y_"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Tuple, Union, Dict, List\n",
        "\n",
        "def fit_waveform_streaming_lms(\n",
        "    mic: np.ndarray,\n",
        "    ref: np.ndarray,\n",
        "    echo: np.ndarray = None,\n",
        "    near_end: np.ndarray = None,\n",
        "    filter_length: int = 512,\n",
        "    mu: float = 0.1,\n",
        "    normalize: bool = True,\n",
        "    n_iter: int = 1,\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    verbose: bool = False\n",
        ") -> Dict[str, Union[np.ndarray, Dict[str, float], List[float]]]:\n",
        "    \"\"\"\n",
        "    Fits streaming Waveform-domain LMS for mic and ref and calculates metrics during training.\n",
        "\n",
        "    Echo and near_end are unavailable in practice and should only be used for metrics calculation\n",
        "    If not provided, echo will be replaced by mic, near_end will be replaced by zeros\n",
        "    verbose may be useful for debug\n",
        "\n",
        "    \"\"\"\n",
        "    if echo is None:\n",
        "        echo = mic\n",
        "    if near_end is None:\n",
        "        near_end = np.zeros_like(mic)\n",
        "\n",
        "    mic_pt = torch.tensor(mic, dtype=torch.float32, device=device).reshape(1, 1, -1)\n",
        "    ref_pt = torch.tensor(ref, dtype=torch.float32, device=device).reshape(1, 1, -1)\n",
        "    echo_pt = torch.tensor(echo, dtype=torch.float32, device=device).reshape(1, 1, -1)\n",
        "    near_end_pt = torch.tensor(near_end, dtype=torch.float32, device=device).reshape(1, 1, -1)\n",
        "\n",
        "    batch_size, channels, signal_length = mic_pt.shape\n",
        "    signal_length = signal_length.item() if isinstance(signal_length, torch.Tensor) else signal_length\n",
        "\n",
        "    filter_weights = torch.zeros(filter_length, device=device, dtype=torch.float32)\n",
        "\n",
        "    final_estimated_echo = torch.zeros_like(mic_pt)\n",
        "    final_output_signal = torch.zeros_like(mic_pt)\n",
        "\n",
        "    estimated_echo = torch.zeros_like(mic_pt)\n",
        "    output_signal = torch.zeros_like(mic_pt) # near end est.\n",
        "\n",
        "    # LMS\n",
        "    for n in tqdm(range(filter_length, signal_length), disable=not verbose):\n",
        "        # Your code here. Fill output_signal\n",
        "\n",
        "        # NLMS\n",
        "        if normalize:\n",
        "             # Your code here\n",
        "        else:\n",
        "            filter_weights = filter_weights + mu * error * x_window\n",
        "\n",
        "    final_estimated_echo = estimated_echo\n",
        "    final_output_signal = output_signal\n",
        "\n",
        "    output_np = final_output_signal.cpu().numpy()[0, 0]\n",
        "    echo_estimate_np = final_estimated_echo.cpu().numpy()[0, 0]\n",
        "\n",
        "    impulse_response = filter_weights.cpu().numpy()\n",
        "\n",
        "    metrics = calculate_metrics(mic, echo, near_end, output_np, echo_estimate_np)\n",
        "\n",
        "    if verbose:\n",
        "        plot_results(mic, ref, near_end, echo, output_np, echo_estimate_np)\n",
        "        plot_impulse_response(impulse_response)\n",
        "\n",
        "    return {\n",
        "        \"near_end_est\": output_np,\n",
        "        \"echo_est\": echo_estimate_np,\n",
        "        \"metrics\": metrics,\n",
        "        \"impulse_response\": impulse_response,\n",
        "    }"
      ],
      "metadata": {
        "id": "Rs9vcn_hccCE"
      },
      "id": "Rs9vcn_hccCE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adapt_vals_synth = fit_waveform_streaming_lms(\n",
        "    filter_length=1024,\n",
        "    mu=1e-3,\n",
        "    verbose=True,\n",
        "    **DATA[\"synth\"]\n",
        ")\n",
        "metrics = adapt_vals_synth[\"metrics\"]\n",
        "print(f\"ERLE: {metrics['ERLE']:.2f} dB\")\n",
        "print(f\"SNR: {metrics['SNR']:.2f} dB\")\n",
        "print(f\"Misalignment: {metrics['Misalignment']:.2f} dB\")"
      ],
      "metadata": {
        "id": "U4JxZerhhhqD"
      },
      "id": "U4JxZerhhhqD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** What can you say about RTF in a streaming implementation? Is it feasible to use streaming waveform LMS?"
      ],
      "metadata": {
        "id": "MHxOwzcAiVrB"
      },
      "id": "MHxOwzcAiVrB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOUR ANSWER HERE**"
      ],
      "metadata": {
        "id": "9u3hRZCejVP7"
      },
      "id": "9u3hRZCejVP7"
    },
    {
      "cell_type": "code",
      "source": [
        "adapt_vals_real = fit_waveform_streaming_lms(\n",
        "    filter_length=1024,\n",
        "    mu=1e-3,\n",
        "    verbose=True,\n",
        "    **DATA[\"real\"]\n",
        ")\n",
        "metrics_real = adapt_vals_real[\"metrics\"]\n",
        "print(f\"ERLE: {metrics_real['ERLE']:.2f} dB\")\n",
        "print(f\"SNR: {metrics_real['SNR']:.2f} dB\")\n",
        "print(f\"Misalignment: {metrics_real['Misalignment']:.2f} dB\")"
      ],
      "metadata": {
        "id": "1p7rifu_huwh"
      },
      "id": "1p7rifu_huwh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving data for both LMS"
      ],
      "metadata": {
        "id": "YemZFvAviOv3"
      },
      "id": "YemZFvAviOv3"
    },
    {
      "cell_type": "code",
      "source": [
        "root_out = os.path.join(ROOT_OUT, \"streaming_waveform_lms_synth\")\n",
        "os.makedirs(root_out, exist_ok=True)\n",
        "\n",
        "sf.write(os.path.join(root_out, \"near_end_est.wav\"), adapt_vals_synth[\"near_end_est\"], SR)\n",
        "sf.write(os.path.join(root_out, \"echo_est.wav\"), adapt_vals_synth[\"echo_est\"], SR)\n",
        "\n",
        "for key in [\"mic\", \"ref\"]:\n",
        "    sf.write(os.path.join(root_out, f\"{key}.wav\"), data[key], SR)"
      ],
      "metadata": {
        "id": "7c2YtV2Mh1g8"
      },
      "id": "7c2YtV2Mh1g8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_out = os.path.join(ROOT_OUT, \"streaming_waveform_lms_real\")\n",
        "os.makedirs(root_out, exist_ok=True)\n",
        "\n",
        "sf.write(os.path.join(root_out, \"near_end_est.wav\"), adapt_vals_real[\"near_end_est\"], SR)\n",
        "sf.write(os.path.join(root_out, \"echo_est.wav\"), adapt_vals_real[\"echo_est\"], SR)\n",
        "\n",
        "for key in [\"mic\", \"ref\"]:\n",
        "    sf.write(os.path.join(root_out, f\"{key}.wav\"), data[key], SR)"
      ],
      "metadata": {
        "id": "nitFXz2shkVH"
      },
      "id": "nitFXz2shkVH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8dbdc2b",
      "metadata": {
        "id": "a8dbdc2b"
      },
      "outputs": [],
      "source": [
        "ROOT_OUT = \"data/output\"  # your local path to save the files\n",
        "\n",
        "data = DATA[\"synth\"]\n",
        "echo_est = convolution.apply(data[\"ref\"])\n",
        "near_end_est = data[\"mic\"] - echo_est\n",
        "\n",
        "root_out = os.path.join(ROOT_OUT, \"waveform_lms_synth\")\n",
        "os.makedirs(root_out, exist_ok=True)\n",
        "\n",
        "sf.write(os.path.join(root_out, \"near_end_est.wav\"),  near_end_est, SR)\n",
        "for key in [\"mic\", \"ref\"]:\n",
        "    sf.write(os.path.join(root_out, f\"{key}.wav\"),  data[key], SR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7709b372",
      "metadata": {
        "id": "7709b372"
      },
      "outputs": [],
      "source": [
        "data_type = \"real\"\n",
        "convolution, logs = fit_waveform_lms(**DATA[data_type])\n",
        "ALL_LOGS[\"waveform_lms\"][data_type] = (convolution.cpu(), logs)\n",
        "\n",
        "_, axes = plt.subplots(ncols=2, figsize=(8, 4))\n",
        "\n",
        "ax = axes[0]\n",
        "ax.set_title(\"Loss\")\n",
        "for key in [\"loss\"]:  # \"loss_oracle\" does not make sense here\n",
        "    ax.plot([x[key] for x in logs], label=key)\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "ax.set_yscale(\"log\")\n",
        "\n",
        "ax = axes[1]\n",
        "ax.set_title(\"Impulse response estimate\")\n",
        "ax.plot(convolution.get_kernel())\n",
        "ax.grid()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "data = DATA[data_type]\n",
        "echo_est = convolution.apply(data[\"ref\"])\n",
        "near_end_est = data[\"mic\"] - echo_est\n",
        "\n",
        "root_out = os.path.join(ROOT_OUT, f\"waveform_lms_{data_type}\")\n",
        "os.makedirs(root_out, exist_ok=True)\n",
        "\n",
        "sf.write(os.path.join(root_out, \"near_end_est.wav\"),  near_end_est, SR)\n",
        "for key in [\"mic\", \"ref\"]:\n",
        "    sf.write(os.path.join(root_out, f\"{key}.wav\"),  data[key], SR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6e85481",
      "metadata": {
        "id": "c6e85481"
      },
      "source": [
        "**Task:** Please, take a moment to listen to the output. What can you say about the quality of the methods?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd7855a",
      "metadata": {
        "id": "6fd7855a"
      },
      "source": [
        "**YOUR ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5c20ccd",
      "metadata": {
        "id": "c5c20ccd"
      },
      "source": [
        "### Time-Frequency Domain LMS \\[0.5 pt\\]\n",
        "\n",
        "Time-Frequency domain LMS is defined as follows:\n",
        "\n",
        "$X = STFT(x)$, $Y = STFT(y)$\n",
        "\n",
        "For each frequency f we find:\n",
        "\n",
        "$$\\hat h_f = \\underset{h}{\\operatorname{argmin}} ||T_f - S_f * h_f||_2 = \\underset{h}{\\operatorname{argmin}} \\frac{1}{t}\\sum_{\\tau=0}^{t-1}|Y_{f,\\tau} - (S_f * h_f)_\\tau|^2$$\n",
        "\n",
        "With $\\hat h_f$ the complex spectrum of echo is estimated as $\\hat E_f = S_f * \\hat h_f$, the complex spectrum of the near-end signal is estimated as $\\hat N_f = Y_f - \\hat E_f$, and waveforms can be obtained by ISTFT.\n",
        "\n",
        "Is there a theoretical basis for this approach? Yes, there is, and we will get down do it later in this assignment. However, this algorithm is still partly heuristical.\n",
        "\n",
        "First, let's implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25119e65",
      "metadata": {
        "id": "25119e65"
      },
      "outputs": [],
      "source": [
        "class SpectralConvolution(nn.Module):\n",
        "    \"\"\"\n",
        "    Applies parallel frequency-wise complex-valued convolutions.\n",
        "    Makes use of groupped convolutions in PyTorch.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_freqs: int, kernel_size: int) -> None:\n",
        "        super().__init__()\n",
        "        convs = {}\n",
        "        for mode in [\"real\", \"imag\"]:\n",
        "            conv = nn.Sequential(\n",
        "                nn.ConstantPad1d([kernel_size - 1, 0], 0),\n",
        "                nn.Conv1d(n_freqs, n_freqs, kernel_size, groups=n_freqs, bias=False)\n",
        "            )\n",
        "            convs[mode] = conv\n",
        "        self.convs = nn.ModuleDict(convs)\n",
        "        for _, conv_layer in self.convs.values():\n",
        "            nn.init.zeros_(conv_layer.weight)\n",
        "\n",
        "    def forward(self, spec: torch.Tensor) -> torch.Tensor:\n",
        "        assert spec.ndim == 3, spec.shape\n",
        "        in_channels = {\n",
        "            \"real\": torch.real(spec),\n",
        "            \"imag\": torch.imag(spec),\n",
        "        }\n",
        "        out_real = self.convs[\"real\"](in_channels[\"real\"]) -  # your code: calculate the real part of the complex output\n",
        "        out_imag = self.convs[\"real\"](in_channels[\"imag\"]) +  # your code: calculate the imag part of the complex output\n",
        "        result = out_real + 1j * out_imag\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ccda0d3",
      "metadata": {
        "id": "3ccda0d3"
      },
      "outputs": [],
      "source": [
        "from torchaudio.transforms import Spectrogram, InverseSpectrogram\n",
        "\n",
        "\n",
        "class EndToEndSpectralConvolution(nn.Module):\n",
        "    \"\"\"\n",
        "    Combines stft, istft and SpectralConvolution\n",
        "    \"\"\"\n",
        "    def __init__(self, kernel_size: int, n_fft=2048, win_length=None, **stft_kwargs):\n",
        "        super().__init__()\n",
        "        assert n_fft % 2 == 0, n_fft\n",
        "        self.stft = Spectrogram(n_fft=n_fft, win_length=win_length, power=None, **stft_kwargs)\n",
        "        self.istft = InverseSpectrogram(n_fft=n_fft, win_length=win_length, **stft_kwargs)\n",
        "        self.conv = SpectralConvolution(n_freqs=n_fft // 2 + 1, kernel_size=kernel_size)\n",
        "\n",
        "    def prepare_spectrogram(self, x: np.ndarray) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Builds a spectrogram which is suitable for self.conv.forward\n",
        "        \"\"\"\n",
        "        assert x.ndim == 1, x.shape\n",
        "        device = next(iter(self.parameters())).device\n",
        "        dtype = next(iter(self.parameters())).dtype\n",
        "        x = torch.tensor(x, device=device, dtype=dtype).reshape(1, -1)  # batch dim\n",
        "        spec = self.stft(x)\n",
        "        return spec\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def apply(self, ref: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Estimates echo from 1-D np.ndarray waveforms\n",
        "        \"\"\"\n",
        "        assert ref.ndim == 1, ref.shape\n",
        "        target_length = ref.shape[-1]\n",
        "        device = next(iter(self.parameters())).device\n",
        "        dtype = next(iter(self.parameters())).dtype\n",
        "        ref = torch.tensor(ref, device=device, dtype=dtype).reshape(1, -1)  # batch dim\n",
        "        spec_ref = self.stft(ref)\n",
        "        spec_echo_est = self.conv(spec_ref)\n",
        "        echo_est_wave = self.istft(spec_echo_est, length=target_length)\n",
        "        echo_est_wave_np = echo_est_wave[0].cpu().numpy()\n",
        "        return echo_est_wave_np\n",
        "\n",
        "\n",
        "end_to_end = EndToEndSpectralConvolution(kernel_size=15).to(device)\n",
        "end_to_end.apply(data[\"ref\"]).shape, data[\"ref\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72df832c",
      "metadata": {
        "id": "72df832c"
      },
      "outputs": [],
      "source": [
        "class ComplexMse(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, est: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        result = self.mse(torch.real(est), torch.real(target)) + self.mse(torch.imag(est), torch.imag(target))\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: PyTorch natively supports complex-valued operations, you just need to remember to use absolute value when calculating losses for complex numbers.\n"
      ],
      "metadata": {
        "id": "VCWsBhEIEDuG"
      },
      "id": "VCWsBhEIEDuG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c224c2a1",
      "metadata": {
        "scrolled": false,
        "id": "c224c2a1"
      },
      "outputs": [],
      "source": [
        "def fit_spectral_lms(\n",
        "    mic: np.ndarray,\n",
        "    ref: np.ndarray,\n",
        "    echo: np.ndarray | None = None,\n",
        "    near_end: np.ndarray | None = None,\n",
        "    loss_type: str = \"spectral\",  # \"spectral\" or \"waveform\",\n",
        "    verbose: bool = False,  # helpful for debug\n",
        "):\n",
        "    \"\"\"\n",
        "    Fits time-frequency-domain LMS for mic and ref and calculates metrics during training.\n",
        "\n",
        "    Echo and near_end are unavailable in practice and should only be used for metrics calculation\n",
        "    If not provided, echo will be replaced by mic, near_end will be replaced by zeros\n",
        "\n",
        "    loss_type: either STFT-domain MSE or waveform-domain MSE\n",
        "    \"\"\"\n",
        "    n_steps = 1_500\n",
        "    kernel_size = 15\n",
        "    conv_kwargs = dict(kernel_size=kernel_size, n_fft=2048, win_length=2048, hop_length=512)\n",
        "    conv = EndToEndSpectralConvolution(**conv_kwargs).to(device)\n",
        "    opt = torch.optim.Adam(conv.parameters(), lr=1e-2, weight_decay=0)\n",
        "\n",
        "    if echo is None:\n",
        "        echo = mic\n",
        "    if near_end is None:\n",
        "        near_end = np.zeros_like(mic)\n",
        "\n",
        "    spec_mic = conv.prepare_spectrogram(mic)\n",
        "    spec_ref = conv.prepare_spectrogram(ref)\n",
        "    spec_echo = conv.prepare_spectrogram(echo)\n",
        "\n",
        "    mic_pt = torch.tensor(mic, dtype=torch.float, device=device).reshape(1, -1)\n",
        "    echo_pt = torch.tensor(echo, dtype=torch.float, device=device).reshape(1, -1)\n",
        "    near_end_pt = torch.tensor(near_end, dtype=torch.float, device=device).reshape(1, -1)\n",
        "\n",
        "    if loss_type == \"spectral\":\n",
        "        criterion = ComplexMse().to(device)\n",
        "    elif loss_type == \"waveform\":\n",
        "        criterion = nn.MSELoss().to(device)\n",
        "    else:\n",
        "        assert False, loss_type\n",
        "    snr_evaluator = SignalNoiseRatio().to(device)\n",
        "    wave_mse_evaluator = nn.MSELoss().to(device)\n",
        "\n",
        "    logs = []\n",
        "\n",
        "    best_loss = None\n",
        "    best_loss_snapshot = None\n",
        "\n",
        "    for idx in tqdm(range(n_steps)):\n",
        "        echo_est_spec = conv.conv(spec_ref)\n",
        "        if loss_type == \"spectral\":\n",
        "            loss =  # your code: evaluate criterion for echo_est_spec and spec_mic\n",
        "        else:\n",
        "            echo_est_wave = # your code: apply conv.istft to echo_est_spec with output length matching that of mic_pt\n",
        "            loss =  # your code: evaluate criterion for echo_est_wave and mic_pt\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        # saving the best snapshot\n",
        "        if best_loss is None or loss < best_loss:\n",
        "            best_loss = loss.item()\n",
        "            best_loss_snapshot = conv.state_dict()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if loss_type == \"spectral\":\n",
        "                loss_oracle = criterion( # your code: loss between echo_est_spec and spec_echo\n",
        "                echo_est_wave =  # your code: just repeat the computation of echo_est_wave you have done above\n",
        "            else:\n",
        "                loss_oracle = criterion(  # your code: loss between echo_est_wave and echo_pt)\n",
        "\n",
        "            snr_echo = snr_evaluator(  # your code: SNR for echo_est_wave and echo_pt\n",
        "            near_end_est =  # your code: difference of mic_pt and echo_est_wave\n",
        "            snr_near = snr_evaluator(  # your code: SNR for near_end_est and near_end_pt\n",
        "\n",
        "            mse = wave_mse_evaluator(echo_est_wave, mic_pt)\n",
        "            mse_oracle = wave_mse_evaluator(echo_est_wave, echo_pt)\n",
        "\n",
        "        logs.append(\n",
        "            {\n",
        "                \"loss\": loss.item(),\n",
        "                \"loss_oracle\": loss_oracle.item(),\n",
        "                \"snr_echo\": snr_echo.item(),\n",
        "                \"snr_near\": snr_near.item(),\n",
        "                \"lr\": opt.param_groups[0][\"lr\"],\n",
        "                \"mse\": mse.item(),\n",
        "                \"mse_oracle\": mse_oracle.item(),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        if verbose and (idx % 100 == 0 or idx + 1 == n_steps):\n",
        "            print(logs[-1])\n",
        "\n",
        "    final_model = EndToEndSpectralConvolution(**conv_kwargs).to(device)\n",
        "    final_model.load_state_dict(best_loss_snapshot)\n",
        "    return final_model, logs\n",
        "\n",
        "data_type = \"synth\"\n",
        "convolution, logs = fit_spectral_lms(**DATA[data_type])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81ae72c4",
      "metadata": {
        "id": "81ae72c4"
      },
      "outputs": [],
      "source": [
        "algorithm = \"spectral_lms\"\n",
        "ALL_LOGS[algorithm] = {}\n",
        "\n",
        "for data_type in DATA.keys():\n",
        "    print(\"data type:\", data_type)\n",
        "    convolution, logs = fit_spectral_lms(**DATA[data_type], loss_type=\"spectral\")\n",
        "    ALL_LOGS[algorithm][data_type] = (convolution.cpu(), logs)\n",
        "\n",
        "    _, ax = plt.subplots(figsize=(4, 4))\n",
        "\n",
        "    ax.set_title(\"Loss\")\n",
        "    keys = [\"loss\"]\n",
        "    if data_type == \"synth\":\n",
        "        keys.append(\"loss_oracle\")\n",
        "    for key in keys:\n",
        "        ax.plot([x[key] for x in logs], label=key)\n",
        "    ax.grid()\n",
        "    ax.legend()\n",
        "    ax.set_yscale(\"log\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    data = DATA[data_type]\n",
        "    echo_est = convolution.apply(data[\"ref\"])\n",
        "    near_end_est = data[\"mic\"] - echo_est\n",
        "\n",
        "    root_out = os.path.join(ROOT_OUT, f\"{algorithm}_{data_type}\")\n",
        "    os.makedirs(root_out, exist_ok=True)\n",
        "\n",
        "    sf.write(os.path.join(root_out, \"near_end_est.wav\"),  near_end_est, SR)\n",
        "    for key in [\"mic\", \"ref\"]:\n",
        "        sf.write(os.path.join(root_out, f\"{key}.wav\"),  data[key], SR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99906969",
      "metadata": {
        "id": "99906969"
      },
      "source": [
        "Let's compare the MSE values for both of the methods. We'll use the synthetic data to access the oracle MSE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cf835be",
      "metadata": {
        "id": "7cf835be"
      },
      "outputs": [],
      "source": [
        "data_type = \"synth\"\n",
        "mse_waveform = [x[\"loss\"] for x in ALL_LOGS[\"waveform_lms\"][data_type][1]]\n",
        "mse_oracle_waveform = [x[\"loss_oracle\"] for x in ALL_LOGS[\"waveform_lms\"][data_type][1]]\n",
        "near_end_snr_waveform = [x[\"snr_near\"] for x in ALL_LOGS[\"waveform_lms\"][data_type][1]]\n",
        "\n",
        "mse_spectral = [x[\"mse\"] for x in ALL_LOGS[\"spectral_lms\"][data_type][1]]\n",
        "mse_oracle_spectral = [x[\"mse_oracle\"] for x in ALL_LOGS[\"spectral_lms\"][data_type][1]]\n",
        "near_end_snr_spectral = [x[\"snr_near\"] for x in ALL_LOGS[\"spectral_lms\"][data_type][1]]\n",
        "\n",
        "_, axes = plt.subplots(figsize=(12, 4), ncols=3)\n",
        "\n",
        "ax = axes[0]\n",
        "ax.set_title(\"mse\")\n",
        "ax.plot(mse_waveform, label=\"waveform\")\n",
        "ax.plot(mse_spectral, label=\"spectral\")\n",
        "ax.set_yscale(\"log\")\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "\n",
        "ax = axes[1]\n",
        "ax.set_title(\"mse_oracle\")\n",
        "ax.plot(mse_oracle_waveform, label=\"waveform\")\n",
        "ax.plot(mse_oracle_spectral, label=\"spectral\")\n",
        "ax.set_yscale(\"log\")\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "\n",
        "ax = axes[2]\n",
        "ax.set_title(\"snr-near-end\")\n",
        "ax.plot(near_end_snr_waveform, label=\"waveform\")\n",
        "ax.plot(near_end_snr_spectral, label=\"spectral\")\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"MSE oracle min values:\")\n",
        "print(\"Waveform LMS:\", min(mse_oracle_waveform), \", Spectral LMS:\", min(mse_oracle_spectral))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a7251d1",
      "metadata": {
        "id": "8a7251d1"
      },
      "source": [
        "**Question:** what can you observe from your implementation of waveform-domain and time-frequency domain LMS? Please, comment on the speed, stability and quality of the algorithms. It may be helpful to listen to the outputs to make up a subjective opinion. You may want to fit time-frequency LMS with waveform domain loss."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24769829",
      "metadata": {
        "id": "24769829"
      },
      "source": [
        "**YOUR ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a513b55",
      "metadata": {
        "id": "3a513b55"
      },
      "source": [
        "What's the theory behind Time-frequency domain LMS? Let's get down to it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time-Frequency Domain Adaptive LMS / NLMS \\[0.5 pt\\]\n",
        "\n",
        "The adaptive LMS algorithm can also be applied in the time-frequency domain, i.e., to time-frequency (STFT) representations of the signals.\n",
        "\n",
        "- $Y = STFT(y)$: complex spectrogram of the microphone signal\n",
        "- $S = STFT(s)$: complex spectrogram of the reference (speaker) signal\n",
        "\n",
        "Let $Y_{f, \\tau}$ and $S_{f, \\tau}$ denote the complex values at frequency bin $f$ and time frame $\\tau$.\n",
        "\n",
        "For each frequency bin $f$, we estimate a filter $h_f$ of length $L$ that models the echo path in the complex spectral domain.\n",
        "\n",
        "#### Adaptive LMS in the Spectral Domain\n",
        "\n",
        "At each time frame $\\tau$ and frequency $f$:\n",
        "\n",
        "- Form the input vector for the filter at frequency $f$:\n",
        "  $$\n",
        "  \\mathbf{s}_{f, \\tau} = [S_{f, \\tau},\\; S_{f, \\tau-1},\\; \\ldots,\\; S_{f, \\tau-L+1}]^\\top\n",
        "  $$\n",
        "  (zero pad if needed).\n",
        "- Estimated echo:\n",
        "  $$\n",
        "  \\hat E_{f, \\tau} = h_f^\\mathrm{H}  \\mathbf{s}_{f, \\tau}\n",
        "  $$\n",
        "  where $h_f$ is the current filter for frequency $f$, and ${}^\\mathrm{H}$ denotes Hermitian (complex conjugate transpose).\n",
        "- Error:\n",
        "  $$\n",
        "  e_{f, \\tau} = Y_{f, \\tau} - \\hat E_{f, \\tau}\n",
        "  $$\n",
        "- LMS update for $h_f$:\n",
        "  $$\n",
        "  h_f \\leftarrow h_f + \\eta\\, e_{f, \\tau}^*\\, \\mathbf{s}_{f, \\tau}\n",
        "  $$\n",
        "  where $e_{f, \\tau}^*$ denotes the complex conjugate, and $\\eta$ is the learning rate. You can calculate the gradient yourself, and at the same time make sure that I'm not mistaken.\n",
        "\n",
        "#### NLMS (Normalized LMS) in the Spectral Domain\n",
        "\n",
        "To improve convergence, step size can be normalized by the input energy:\n",
        "$$\n",
        "h_f \\leftarrow h_f + \\frac{\\eta}{\\|\\mathbf{s}_{f, \\tau}\\|^2 + \\epsilon}\\; e_{f, \\tau}^*\\, \\mathbf{s}_{f, \\tau}\n",
        "$$\n",
        "where $\\epsilon$ is a small constant to avoid division by zero.\n",
        "\n"
      ],
      "metadata": {
        "id": "m4zUEXpeqXcT"
      },
      "id": "m4zUEXpeqXcT"
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_spectral_streaming_lms(\n",
        "    mic: np.ndarray,\n",
        "    ref: np.ndarray,\n",
        "    echo: np.ndarray = None,\n",
        "    near_end: np.ndarray = None,\n",
        "    mu: float = 1e-3,\n",
        "    n_iter: int = 1,\n",
        "    win_length: int = 2048,\n",
        "    hop_length: int = 512,\n",
        "    history_frames: int = 1,\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    verbose: bool = False,\n",
        "    use_spectral_plots: bool = False,\n",
        "    fs: int = 16000,\n",
        "    normalize = False\n",
        ") -> Dict[str, Union[np.ndarray, Dict[str, float], List[float]]]:\n",
        "    \"\"\"\n",
        "    Fits time-frequency-domain LMS for mic and ref and calculates metrics during training.\n",
        "\n",
        "    Echo and near_end are unavailable in practice and should only be used for metrics calculation\n",
        "    If not provided, echo will be replaced by mic, near_end will be replaced by zeros\n",
        "    \"\"\"\n",
        "    if echo is None:\n",
        "        echo = mic\n",
        "    if near_end is None:\n",
        "        near_end = np.zeros_like(mic)\n",
        "\n",
        "    original_length = len(mic)\n",
        "    padding_length = win_length + hop_length\n",
        "\n",
        "    mic_padded = np.pad(mic, (0, padding_length))\n",
        "    ref_padded = np.pad(ref, (0, padding_length))\n",
        "    echo_padded = np.pad(echo, (0, padding_length))\n",
        "    near_end_padded = np.pad(near_end, (0, padding_length))\n",
        "\n",
        "    mic_pt = torch.tensor(mic_padded, dtype=torch.float32, device=device)\n",
        "    ref_pt = torch.tensor(ref_padded, dtype=torch.float32, device=device)\n",
        "    echo_pt = torch.tensor(echo_padded, dtype=torch.float32, device=device)\n",
        "    near_end_pt = torch.tensor(near_end_padded, dtype=torch.float32, device=device)\n",
        "\n",
        "    window = torch.hann_window(win_length, device=device)\n",
        "\n",
        "    n_freqs = win_length // 2 + 1\n",
        "\n",
        "    mic_stft = torch.stft(mic_pt, n_fft=win_length, hop_length=hop_length,\n",
        "                          win_length=win_length, window=window,\n",
        "                          return_complex=True, normalized=False, onesided=True)\n",
        "\n",
        "    ref_stft = torch.stft(ref_pt, n_fft=win_length, hop_length=hop_length,\n",
        "                          win_length=win_length, window=window,\n",
        "                          return_complex=True, normalized=False, onesided=True)\n",
        "\n",
        "    num_freq_bins = mic_stft.shape[0]\n",
        "    num_frames = mic_stft.shape[1]\n",
        "\n",
        "    assert num_freq_bins == n_freqs, f\"Expected {n_freqs} frequency bins, but got {num_freq_bins}\"\n",
        "\n",
        "    # Init filter for each freq. bin\n",
        "    filter_weights = torch.zeros((num_freq_bins, history_frames),\n",
        "                                dtype=torch.complex64, device=device)\n",
        "\n",
        "    output_stft = torch.zeros_like(mic_stft)\n",
        "    echo_estimate_stft = torch.zeros_like(mic_stft)\n",
        "\n",
        "    ref_history = torch.zeros((num_freq_bins, history_frames),\n",
        "                             dtype=torch.complex64, device=device)\n",
        "\n",
        "    for frame_idx in tqdm(range(num_frames), disable=not verbose):\n",
        "        ref_history = torch.roll(ref_history, shifts=-1, dims=1)\n",
        "        ref_history[:, 0] = ref_stft[:, frame_idx]\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # update\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    output_signal = torch.istft(output_stft, n_fft=win_length,\n",
        "                                hop_length=hop_length, win_length=win_length,\n",
        "                                window=window)\n",
        "\n",
        "    echo_estimate = torch.istft(echo_estimate_stft, n_fft=win_length,\n",
        "                                hop_length=hop_length, win_length=win_length,\n",
        "                                window=window)\n",
        "\n",
        "    output_signal = output_signal[:original_length]\n",
        "    echo_estimate = echo_estimate[:original_length]\n",
        "\n",
        "    output_np = output_signal.cpu().numpy()\n",
        "    echo_estimate_np = echo_estimate.cpu().numpy()\n",
        "\n",
        "    if len(output_np) != len(mic) or len(echo_estimate_np) != len(echo):\n",
        "        min_length = min(len(mic), len(output_np), len(echo), len(echo_estimate_np))\n",
        "\n",
        "        mic_truncated = mic[:min_length]\n",
        "        echo_truncated = echo[:min_length]\n",
        "        near_end_truncated = near_end[:min_length]\n",
        "        output_np_truncated = output_np[:min_length]\n",
        "        echo_estimate_np_truncated = echo_estimate_np[:min_length]\n",
        "\n",
        "        metrics = calculate_metrics(\n",
        "            mic_truncated,\n",
        "            echo_truncated,\n",
        "            near_end_truncated,\n",
        "            output_np_truncated,\n",
        "            echo_estimate_np_truncated\n",
        "        )\n",
        "    else:\n",
        "        metrics = calculate_metrics(mic, echo, near_end, output_np, echo_estimate_np)\n",
        "\n",
        "    if verbose:\n",
        "        min_length = min(len(mic), len(ref), len(near_end), len(echo), len(output_np), len(echo_estimate_np))\n",
        "        plot_results(\n",
        "            mic[:min_length],\n",
        "            ref[:min_length],\n",
        "            near_end[:min_length],\n",
        "            echo[:min_length],\n",
        "            output_np[:min_length],\n",
        "            echo_estimate_np[:min_length],\n",
        "            use_spectral=use_spectral_plots,\n",
        "            fs=fs\n",
        "        )\n",
        "\n",
        "        if history_frames > 0:\n",
        "            plt.figure(figsize=(15, 5 * min(history_frames, 3)))\n",
        "            for h in range(min(history_frames, 6)):\n",
        "                plt.subplot(min(history_frames, 3), 2, h+1)\n",
        "                plt.plot(np.abs(filter_weights.cpu().numpy()[:, h]))\n",
        "                plt.title(f'Magnitude response (history frame {h})')\n",
        "                plt.xlabel('Frequency bin')\n",
        "                plt.ylabel('Magnitude')\n",
        "                plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    freq_response = {\n",
        "        \"freqs\": np.linspace(0, fs/2, n_freqs),\n",
        "        \"magnitude\": np.abs(filter_weights[:, 0].cpu().numpy()),\n",
        "        \"phase\": np.angle(filter_weights[:, 0].cpu().numpy())\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"near_end_est\": output_np,\n",
        "        \"echo_est\": echo_estimate_np,\n",
        "        \"metrics\": metrics,\n",
        "        \"filter_weights\": filter_weights.cpu().numpy(),\n",
        "        \"freq_response\": freq_response\n",
        "    }\n"
      ],
      "metadata": {
        "id": "9byJtz94ysuH"
      },
      "id": "9byJtz94ysuH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adapt_vals_spectral = fit_spectral_streaming_lms(\n",
        "    history_frames=4,\n",
        "    mu=0.00043,\n",
        "    verbose=True,\n",
        "    use_spectral_plots=True,\n",
        "    hop_length=512,\n",
        "    win_length=2048,\n",
        "    normalize=False,\n",
        "    **DATA[\"synth\"]\n",
        ")\n",
        "\n",
        "# Try your own params\n",
        "metrics_spectral = adapt_vals_spectral[\"metrics\"]\n",
        "print(f\"ERLE: {metrics_spectral['ERLE']:.2f} dB\")\n",
        "print(f\"SNR: {metrics_spectral['SNR']:.2f} dB\")\n",
        "print(f\"Misalignment: {metrics_spectral['Misalignment']:.2f} dB\")"
      ],
      "metadata": {
        "id": "Hr_mVQxdzM_l"
      },
      "id": "Hr_mVQxdzM_l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adapt_vals_spectral = fit_spectral_adaptation_lms(\n",
        "    history_frames=3,\n",
        "    mu=0.00043,\n",
        "    verbose=True,\n",
        "    use_spectral_plots=True,\n",
        "    n_iter=2,\n",
        "    fft_size=2048,\n",
        "    hop_length=256,\n",
        "    win_length=2048,\n",
        "    **DATA[\"real\"]\n",
        ")\n",
        "metrics_spectral = adapt_vals_spectral[\"metrics\"]\n",
        "print(f\"ERLE: {metrics_spectral['ERLE']:.2f} dB\")"
      ],
      "metadata": {
        "id": "QY30c_F-zWia"
      },
      "id": "QY30c_F-zWia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion of using spectral methods:**"
      ],
      "metadata": {
        "id": "2WrKA8vlzuef"
      },
      "id": "2WrKA8vlzuef"
    },
    {
      "cell_type": "markdown",
      "id": "4c64fcd0",
      "metadata": {
        "id": "4c64fcd0"
      },
      "source": [
        "### Convolution in Time-Frequency (a.k.a. STFT) domain \\[2 pts\\]\n",
        "\n",
        "We have seen that convolution in time domain can be replaced with multiplication in frequency (a.k.a. DFT) domain and it can save some computations. However, in order to get DFT from an input signal we need to access the whole signal which is only available for offline applications.\n",
        "\n",
        "To make it online let's switch to time-frequency (a.k.a. STFT) domain.\n",
        "\n",
        "**First, let's switch to block-wise processing.**\n",
        "\n",
        "The idea is very simple: let's split the input signal into small windows (\\~20-100 ms) and convolve them with the impulse response.\n",
        "\n",
        "![title](assets/block_conv_operator_trunc_v1.png)\n",
        "\n",
        "In the picture above we split the input signal into windows, convolve the windows with the impulse response and sum the resulting components. The result is equal to the convolution of the full signal with the impulse response.\n",
        "\n",
        "It would be great to process the windows independently, but we can see that each window affects several future windows after convolution:\n",
        "\n",
        "![title](assets/block_conv_final.png)\n",
        "\n",
        "The output block $Y_t$ depends not only on the corresponding input block $X_t$ but also on the blocks $X_{t-1}$ and $X_{t-2}$ in this example.\n",
        "\n",
        "**Let's describe it more formally.** Let $y = x * r$, let $w \\in \\mathbb N$ be the window size.\n",
        "\n",
        "Window transform: $Y_t = (y_{wt}, y_{wt + 1}, ... ,y_{wt+w-1})$, $X_t = (x_{wt}, x_{wt + 1}, ... ,x_{wt + w -1})$, $R_k = (r_{wk}, r_{wk+1}, ..., r_{wk+w-1})$\n",
        "\n",
        "It can be shown mathematically that:\n",
        "$$\\forall t ~~ Y_t = (X_t * R_0)[:w] + (X_{t-1} * R_0)[w:] + (X_{t-1} * R_1)[:w] + (X_{t-2} * R_1)[w:] + ... = \\sum_{\\tau + k=t}(X_\\tau * R_k)[:w] + \\sum_{\\tau + k=t-1}(X_\\tau * R_k)[w:],$$ where $[:w]$ stands for the first $w$ elements of a vector and $[w:]$ stands for the elements starting from $w$-th with one right-most element appended as zero to match the shapes. Let's look at this expression once again:\n",
        "\n",
        "$$Y_t = \\sum_{\\tau + k=t}(X_\\tau * R_k)[:w] + \\sum_{\\tau + k=t-1}(X_\\tau * R_k)[w:]$$\n",
        "\n",
        "Looks like a convolution with convolutions inside, doesn't it? The 2 sum terms correspond to the overlap-add procedure.\n",
        "\n",
        "Here is what it looks like:\n",
        "![title](assets/block_conv_ola_v3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1383ecc1",
      "metadata": {
        "id": "1383ecc1"
      },
      "source": [
        "**Now we can formulate the stft-domain convolution algorithm:**\n",
        "\n",
        "X = stft(x, win_size, n_fft=2 * win_size)\n",
        "\n",
        "H = stft(h, win_size, n_fft=2 * win_size)\n",
        "\n",
        "Y = convolve(X, H): frequency-wise convolution, i.e. convolution is done separately for each frequency\n",
        "\n",
        "y = istft(y, 2 x win_size, n_fft=2 * win_size): note that the win-size is different from the original one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11634d63",
      "metadata": {
        "id": "11634d63"
      },
      "outputs": [],
      "source": [
        "def stft_domain_convolution(x: torch.Tensor, h: torch.Tensor, win_size: int = 512) -> torch.Tensor:\n",
        "    assert x.ndim == h.ndim == 1\n",
        "    assert win_size % 2 == 0, win_size\n",
        "    assert len(x) >= len(h), (x.shape, h.shape)\n",
        "    size_final = len(x) + len(h) - 1\n",
        "    x = F.pad(x, [0, -len(x) % win_size])  # making the length divisible by win_size\n",
        "    h =  # your code: make len(h) divisible by win_size\n",
        "\n",
        "    x_wins = x.unfold(-1, win_size, win_size)  # window transform\n",
        "    h_wins = h.unfold(-1, win_size, win_size)\n",
        "\n",
        "    x_wins =  # your code, zero padding: (n, win_size) -> (n, 2 x win_size)\n",
        "    h_wins =  # your code, zero padding: (n, win_size) -> (n, 2 x win_size)\n",
        "\n",
        "    x_fft = torch.fft.rfft(x_wins).T  # (n_fft, n)\n",
        "    h_fft = torch.fft.rfft(h_wins).T\n",
        "    x_fft = F.pad(x_fft, [h_fft.shape[-1] - 1, h_fft.shape[-1] - 1])  # mode: full\n",
        "\n",
        "    h_fft = torch.flip(h_fft, dims=[-1])  # for PyTorch or dot-product convolution\n",
        "\n",
        "    # here you may choose either to do dot-product convolution\n",
        "    # or to use F.conv1d for complex tensors\n",
        "\n",
        "    # dot-product:\n",
        "    convolved = []\n",
        "    for x_start_idx in range(x_fft.shape[-1] - h_fft.shape[-1] + 1):\n",
        "        block_x = x_fft[..., x_start_idx: x_start_idx + h_fft.shape[-1]]\n",
        "        prod = (block_x * h_fft).sum(  # your code: specify the dimension\n",
        "        convolved.append(prod)\n",
        "    convolved = torch.stack(convolved, -1)\n",
        "\n",
        "    # F.conv1d for complex tensors:\n",
        "    convolved = (\n",
        "        + F.conv1d(x_fft.real.unsqueeze(0), h_fft.real.unsqueeze(1), groups=x_fft.shape[0]).squeeze(0)\n",
        "        -  # your code: finilize the real part\n",
        "        + 1j *  # your code: imaginary part\n",
        "        + 1j *  # your code: imaginary part\n",
        "    )\n",
        "\n",
        "    windows_to_overlap = torch.fft.irfft(convolved.T)  # (n', 2 x win_size)\n",
        "\n",
        "    # overlap-add operation\n",
        "    out_chunks = []\n",
        "    last_chunk = None\n",
        "    for win in windows_to_overlap:\n",
        "        if last_chunk is None:\n",
        "            out_chunk = win[:win_size]\n",
        "        else:\n",
        "            out_chunk = last_chunk + win[:win_size]\n",
        "        out_chunks.append(out_chunk)\n",
        "        last_chunk = win[win_size:]\n",
        "    out_chunks.append(last_chunk)\n",
        "    result = torch.cat(out_chunks)\n",
        "    result = result[:size_final]\n",
        "    return result\n",
        "\n",
        "\n",
        "torch.manual_seed(879235)\n",
        "for _ in tqdm(range(20)):\n",
        "    x = torch.rand(16_000)\n",
        "    h = torch.rand(1000)\n",
        "    win_size = 200\n",
        "    out = stft_domain_convolution(x, h, win_size)\n",
        "    ref = convolve_pt(x, h, mode=\"full\")\n",
        "    diff = out - ref\n",
        "    assert diff.abs().max() < out.abs().mean() * 1e-5\n",
        "\n",
        "print(\"You rock!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c769b55",
      "metadata": {
        "id": "0c769b55"
      },
      "source": [
        "**Again, what we've just done:**\n",
        "\n",
        "X = stft(x, win_size, n_fft=2 * win_size)\n",
        "\n",
        "H = stft(h, win_size, n_fft=2 * win_size)\n",
        "\n",
        "Y = convolve(X, H): frequency-wise convolution, i.e. convolution is done separately for each frequency\n",
        "\n",
        "y = istft(y, 2 x win_size, n_fft=2 * win_size): note that the win-size is different from the original one\n",
        "\n",
        "**It is not exactly what we did in frequency domain LMS:**\n",
        "1. In freq-domain LMS we did stft with hann window and overlaps\n",
        "2. We minized mean-squares in stft domain. This operation is pretty fair, as DFT is an isometrical transform (*)\n",
        "3. We did not do the double-win-size trick in freq-domain LMS\n",
        "4. Even if we did the double-win-size trick, it would be tricky to control that the time-frequency domain kernel we estimate can be transformed back to time domain\n",
        "\n",
        "(*) What does \"isometrical transform\" mean? It preserves the distances, in our case the L2-norm. To be precise, in our situation, the norm is multiplied by $\\sqrt{\\text{win-size}}$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a92de81",
      "metadata": {
        "id": "0a92de81"
      },
      "outputs": [],
      "source": [
        "size = 2048\n",
        "x = torch.rand(size)\n",
        "s = torch.fft.fft(x)\n",
        "\n",
        "norm_wave = x.abs().square().mean().sqrt()\n",
        "norm_spec = s.abs().square().mean().sqrt()\n",
        "\n",
        "(norm_spec / norm_wave) ** 2, size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64500e5e",
      "metadata": {
        "id": "64500e5e"
      },
      "source": [
        "We said above: \"it would be tricky to control that the time-frequency domain kernel we estimate can be transformed back to time domain\". What does it mean?\n",
        "\n",
        "STFT is an invertible operation with ISTFT being its inverse (given that hop size is sufficiently small and ignoring the edge effects which are perfectly handled by zero padding).\n",
        "\n",
        "If we estimate a time-frequency domain kernel $H$, why don't we just take $h=ISTFT(H)$ as its time-domain counterpart?\n",
        "\n",
        "It turns out that even though STFT is invertible, ISTFT is not. Tricky?\n",
        "\n",
        "**Question:** provide an example of a complex spectrogram $S$, such that $STFT(ISTFT(S)) \\neq S$ for STFT and ISTFT given in the code below.\n",
        "\n",
        "**Tip:** set nfft > win_size. What is the probability that a random spectrogram satisfies the condition reqired?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d01ec1e9",
      "metadata": {
        "id": "d01ec1e9"
      },
      "outputs": [],
      "source": [
        "stft_params = dict(win_length=512, n_fft=1024, hop_length=128)\n",
        "stft = Spectrogram(**stft_params, power=None)\n",
        "istft = InverseSpectrogram(**stft_params)\n",
        "\n",
        "torch.manual_seed(682)\n",
        "x = torch.rand(48_000)\n",
        "\n",
        "s = stft(x)\n",
        "x_restored = istft(s, length=len(x))\n",
        "\n",
        "diff_wave = (x - x_restored).abs().mean()\n",
        "print(f\"diff wave: {diff_wave.item()}\")\n",
        "assert diff_wave < 1e-6\n",
        "\n",
        "s_tricky = s\n",
        "\n",
        "# your code: make up your s_tricky\n",
        "s_tricky = torch.rand_like(s)\n",
        "\n",
        "s_tricky_restored = stft(istft(s_tricky))\n",
        "\n",
        "diff_spec = (s_tricky - s_tricky_restored).abs().mean()\n",
        "norm_spec = (s_tricky).abs().mean()\n",
        "diff_spec_rel = diff_spec.item() / norm_spec.item()\n",
        "\n",
        "print(f\"diff spec relative: {diff_spec_rel}\")\n",
        "assert diff_spec_rel > 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b64d1ae",
      "metadata": {
        "id": "2b64d1ae"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}