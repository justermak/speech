{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4667f501-a4b8-4cca-9d9f-bf1952f02679",
   "metadata": {},
   "source": [
    "# Pretraining for ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c28407e-2d86-4b6f-a5de-8f6660a24c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing libs\n",
    "# !pip3 install torch torchvision torchaudio datasets transformers soundfile jiwer --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip3 install librosa --index-url https://pypi.org/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23424d0d-66db-4a73-accb-4b5e1320a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, disable_caching\n",
    "from evaluate import load\n",
    "from transformers import Wav2Vec2ForPreTraining, Wav2Vec2FeatureExtractor, Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa8ef5-4b17-4faa-a1a9-5435234f3e46",
   "metadata": {},
   "source": [
    "## Finetuning Wav2Vec2 model on CTC loss (5 points)\n",
    "\n",
    "\n",
    "In this task you have to create pipeline for finetuning pretrained multilingual Wav2Vec2 model on belarusian audio from [Fleurs](https://huggingface.co/datasets/google/fleurs) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c568-8567-497b-983c-c38c45642a9f",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8eeaa1-62f5-4ccc-8ace-74b023719835",
   "metadata": {},
   "outputs": [],
   "source": [
    "fleurs = load_dataset(\"google/fleurs\", \"be_by\", split=[\"train\", \"validation\", \"test\"], trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03b35708-f6a7-4d98-b26f-da8f70d87997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'вышыня двух пілонаў складае 83 метры даўжыня моста - 378 метраў праезная частка складаецца з дзвюх палос шырыня кожнай - 3,50 м'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fleurs[0][\"transcription\"][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f62b76cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 396,\n",
       " 'num_samples': 250560,\n",
       " 'path': 'C:\\\\Users\\\\andre\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\4a7cb41bec2f9e3bb08125197d8a953f6e2e9fecf18e75e5746ee8f65b3da558\\\\10009414287632395082.wav',\n",
       " 'audio': {'path': 'train/10009414287632395082.wav',\n",
       "  'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00031281,\n",
       "         -0.00038069, -0.00132966]),\n",
       "  'sampling_rate': 16000},\n",
       " 'transcription': 'у той жа час паблізу ад верагодных маршрутаў уварвання базіравалася вельмі мала караблёў каралеўскага флоту таму што адміралы асцерагаліся іх патаплення нямецкімі паветранымі сіламі',\n",
       " 'raw_transcription': 'У той жа час паблізу ад верагодных маршрутаў уварвання базіравалася вельмі мала караблёў каралеўскага флоту, таму што адміралы асцерагаліся іх патаплення нямецкімі паветранымі сіламі.',\n",
       " 'gender': 1,\n",
       " 'lang_id': 6,\n",
       " 'language': 'Belarusian',\n",
       " 'lang_group_id': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fleurs[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd9aaf9-a7e8-460f-a51b-611f2bd7aaf2",
   "metadata": {},
   "source": [
    "In this task, you should:\n",
    "\n",
    "* filter all samples, where `transcription` includes digits. Hint: take care of specific belarussian symbols \"і\", \"ў\";\n",
    "* remove punctuation from `transcription`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2df4169-4fce-48f0-881d-ff366ee20077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "has_digit = re.compile(r\"\\d\")\n",
    "\n",
    "def filter_f(x):\n",
    "    print(x)\n",
    "    return x is not None\n",
    "\n",
    "\n",
    "preprocessed_train = fleurs[0].filter(lambda x: has_digit.search(x['transcription']) is None)\n",
    "preprocessed_val = fleurs[1].filter(lambda x: has_digit.search(x['transcription']) is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ec7bc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2433, 1927, 408, 355)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fleurs[0]), len(preprocessed_train), len(fleurs[1]), len(preprocessed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60dd623-c031-4066-b408-4337b67056e9",
   "metadata": {},
   "source": [
    "#### Train tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3bdfb6-91fd-4b3f-b1f4-acb10f122a5b",
   "metadata": {},
   "source": [
    "There you should train your own BPE tokenizer based on texts from Fleurs dataset using [HuggingFace tokenizer](https://huggingface.co/docs/tokenizers/en/training_from_memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42affbc0-7b19-4fb4-ad36-faf51a211ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import models, trainers, tokenizers, normalizers, pre_tokenizers, decoders\n",
    "\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "BOS_TOKEN = \"[BOS]\"\n",
    "EOS_TOKEN = \"[EOS]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "tokenizer = tokenizers.Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.NFKC()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = trainers.BpeTrainer(special_tokens=[PAD_TOKEN, BOS_TOKEN, EOS_TOKEN, UNK_TOKEN], vocab_size=VOCAB_SIZE, show_progress=True)\n",
    "tokenizer.train_from_iterator(preprocessed_train['transcription'], trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68d8876b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"[PAD]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":1, \"content\":\"[BOS]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":2, \"content\":\"[EOS]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":3, \"content\":\"[UNK]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}], normalizer=NFKC(), pre_tokenizer=ByteLevel(add_prefix_space=True, trim_offsets=True, use_regex=True), post_processor=None, decoder=ByteLevel(add_prefix_space=True, trim_offsets=True, use_regex=True), model=BPE(dropout=None, unk_token=None, continuing_subword_prefix=None, end_of_word_suffix=None, fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={\"[PAD]\":0, \"[BOS]\":1, \"[EOS]\":2, \"[UNK]\":3, \"!\":4, \"'\":5, \",\":6, \"-\":7, \".\":8, \"/\":9, \":\":10, \";\":11, \"?\":12, \"[\":13, \"]\":14, \"a\":15, \"b\":16, \"c\":17, \"d\":18, \"e\":19, \"f\":20, \"g\":21, \"h\":22, \"i\":23, \"k\":24, \"l\":25, \"m\":26, \"n\":27, \"o\":28, \"p\":29, \"r\":30, \"s\":31, \"t\":32, \"v\":33, \"x\":34, \"y\":35, \"z\":36, \"«\":37, \"°\":38, \"±\":39, \"²\":40, \"³\":41, \"´\":42, \"µ\":43, \"¶\":44, \"·\":45, \"¸\":46, \"¹\":47, \"º\":48, \"»\":49, \"¼\":50, \"½\":51, \"¾\":52, \"¿\":53, \"Â\":54, \"Ã\":55, \"Ð\":56, \"Ñ\":57, \"â\":58, \"Ġ\":59, \"Ģ\":60, \"ģ\":61, \"Ĥ\":62, \"ĥ\":63, \"Ħ\":64, \"ħ\":65, \"Ĩ\":66, \"ĩ\":67, \"Ī\":68, \"ĭ\":69, \"Į\":70, \"į\":71, \"İ\":72, \"ı\":73, \"ĳ\":74, \"Ķ\":75, \"ĸ\":76, \"ŀ\":77, \"Ð°\":78, \"ĠÐ\":79, \"Ð°Ð\":80, \"ĠÑ\":81, \"Ð°Ñ\":82, \"Ñĭ\":83, \"Ð½\":84, \"Ñĸ\":85, \"Ð¾\":86, \"Ðµ\":87, \"ÑĢ\":88, \"Ñı\":89, \"ÑĤ\":90, \"ÑĨ\":91, \"Ðº\":92, \"Ð»\":93, \"Ñĥ\":94, \"ĠÐ¿\":95, \"Ñģ\":96, \"ÑĮ\":97, \"Ð¼\":98, ...}, merges=[(\"Ð\", \"°\"), (\"Ġ\", \"Ð\"), (\"Ð°\", \"Ð\"), (\"Ġ\", \"Ñ\"), (\"Ð°\", \"Ñ\"), (\"Ñ\", \"ĭ\"), (\"Ð\", \"½\"), (\"Ñ\", \"ĸ\"), (\"Ð\", \"¾\"), (\"Ð\", \"µ\"), (\"Ñ\", \"Ģ\"), (\"Ñ\", \"ı\"), (\"Ñ\", \"Ĥ\"), (\"Ñ\", \"Ĩ\"), (\"Ð\", \"º\"), (\"Ð\", \"»\"), (\"Ñ\", \"ĥ\"), (\"ĠÐ\", \"¿\"), (\"Ñ\", \"ģ\"), (\"Ñ\", \"Į\"), (\"Ð\", \"¼\"), (\"Ð\", \"´\"), (\"Ð\", \"²\"), (\"ĠÐ\", \"½\"), (\"Ñ\", \"į\"), (\"Ð°Ð\", \"½\"), (\"Ð°Ð\", \"´\"), (\"Ð\", \"·\"), (\"ĠÐ\", \"·\"), (\"ĠÑ\", \"ģ\"), (\"Ð°Ñ\", \"ģ\"), (\"Ð°Ð\", \"¼\"), (\"Ð°Ð\", \"»\"), (\"ĠÐ\", \"¼\"), (\"ĠÐ\", \"²\"), (\"Ð°Ð\", \"²\"), (\"Ð°Ñ\", \"Ģ\"), (\"ĠÐ\", \"º\"), (\"Ð\", \"¿\"), (\"Ð½\", \"Ñĭ\"), (\"ĠÑ\", \"ĸ\"), (\"Ñ\", \"ĩ\"), (\"Ñ\", \"ħ\"), (\"ĠÐ\", \"´\"), (\"Ð\", \"³\"), (\"Ñ\", \"ŀ\"), (\"ĠÑ\", \"Ĥ\"), (\"ÑĢ\", \"Ñĭ\"), (\"ÑĨ\", \"ÑĮ\"), (\"ĠÐ\", \"±\"), (\"ĠÑ\", \"ŀ\"), (\"Ð°Ñ\", \"Ĩ\"), (\"Ð°Ñ\", \"ŀ\"), (\"ĠÐ\", \"³\"), (\"Ð°Ð\", \"³\"), (\"ĠÑ\", \"ı\"), (\"Ð°Ð\", \"º\"), (\"Ð°Ð\", \"±\"), (\"Ðº\", \"Ñĸ\"), (\"Ð°Ð\", \"¹\"), (\"Ð°Ñ\", \"Ĥ\"), (\"Ñ\", \"Ī\"), (\"Ð\", \"¹\"), (\"ĠÑ\", \"ĥ\"), (\"Ð\", \"¶\"), (\"Ð½\", \"Ñĸ\"), (\"ĠÑ\", \"Ģ\"), (\"Ñ\", \"İ\"), (\"Ð\", \"±\"), (\"Ð»\", \"Ñĸ\"), (\"ÑĨ\", \"Ð°\"), (\"Ðµ\", \"ÑĢ\"), (\"Ð»\", \"ÑĮ\"), (\"ĠÐ¿\", \"ÑĢ\"), (\"ĠÑ\", \"Ī\"), (\"Ġ\", \"Ð°Ð´\"), (\"ÑĤ\", \"Ð¾\"), (\"Ð¾\", \"Ð´\"), (\"Ð°Ð\", \"·\"), (\"ĠÐ½\", \"Ðµ\"), (\"Ð½\", \"Ð°\"), (\"ĠÐ²\", \"Ñĭ\"), (\"Ð»\", \"Ñı\"), (\"ÑĨ\", \"ÑĨÐ°\"), (\"Ð½\", \"Ðµ\"), (\"Ð°Ð\", \"¿\"), (\"ĠÐ½\", \"Ð°\"), (\"Ð°Ð\", \"µ\"), (\"Ð°Ñ\", \"ħ\"), (\"Ð½\", \"Ñı\"), (\"Ñģ\", \"ÑĤ\"), (\"Ð°Ð»\", \"Ñĸ\"), (\"Ð¾\", \"Ñŀ\"), (\"ĠÑ\", \"ĩ\"), (\"Ð·\", \"Ðµ\"), (\"Ñ\", \"ĳ\"), (\"Ð»\", \"Ðµ\"), (\"Ñį\", \"ÑĤ\"), (\"ĠÑĪ\", \"ÑĤÐ¾\"), ...]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bee0f1-e988-477f-b608-56a5afba8f0d",
   "metadata": {},
   "source": [
    "#### Loading model and preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9feb4b5b-8b94-4114-ab60-600bf50c69b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "   \"facebook/wav2vec2-xls-r-300m\"\n",
    ")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-xls-r-300m\", \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=tokenizer.token_to_id(PAD_TOKEN),\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d04550-d56a-4496-9a48-ea5164233fc5",
   "metadata": {},
   "source": [
    "#### Data processor and data collator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b0be5db-079f-4589-9c21-80cdb9a94afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CtcDataProcessor:\n",
    "    def __init__(self, tokenizer, feature_extractor):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __call__(self, row):\n",
    "        \"\"\"\n",
    "            Function applies tokenizer on row['transcription'] and applies feature extractor on audio column in row.\n",
    "            Input: dict with transcription and audio fields\n",
    "            Output: original dict includes `labels` column with tokenized sequence and `input_values` column with computed spectrogram.\n",
    "        \"\"\"\n",
    "        row['labels'] = torch.tensor(self.tokenizer.encode(row['transcription'], add_special_tokens=True).ids)\n",
    "        row['input_values'] = torch.tensor(self.feature_extractor(row['audio']['array'], sampling_rate=row['audio']['sampling_rate']).input_values)\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04b4daf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_14676\\1797745473.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  row['input_values'] = torch.tensor(self.feature_extractor(row['audio']['array'], sampling_rate=row['audio']['sampling_rate']).input_values)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 396,\n",
       " 'num_samples': 250560,\n",
       " 'path': 'C:\\\\Users\\\\andre\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\4a7cb41bec2f9e3bb08125197d8a953f6e2e9fecf18e75e5746ee8f65b3da558\\\\10009414287632395082.wav',\n",
       " 'audio': {'path': 'train/10009414287632395082.wav',\n",
       "  'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00031281,\n",
       "         -0.00038069, -0.00132966]),\n",
       "  'sampling_rate': 16000},\n",
       " 'transcription': 'у той жа час паблізу ад верагодных маршрутаў уварвання базіравалася вельмі мала караблёў каралеўскага флоту таму што адміралы асцерагаліся іх патаплення нямецкімі паветранымі сіламі',\n",
       " 'raw_transcription': 'У той жа час паблізу ад верагодных маршрутаў уварвання базіравалася вельмі мала караблёў каралеўскага флоту, таму што адміралы асцерагаліся іх патаплення нямецкімі паветранымі сіламі.',\n",
       " 'gender': 1,\n",
       " 'lang_id': 6,\n",
       " 'language': 'Belarusian',\n",
       " 'lang_group_id': 1,\n",
       " 'labels': tensor([141, 631, 232,  78, 304,  95, 805, 834, 153, 581, 132, 155, 210, 748,\n",
       "         139, 229, 460, 141, 585, 100, 302, 127, 156,  85, 300, 835, 572, 111,\n",
       "         338, 340, 135,  93, 173, 123, 340, 243, 123, 782, 221, 298, 246, 419,\n",
       "         176, 153, 208,  88, 110,  83, 297,  91, 149, 132, 642, 318, 371, 163,\n",
       "         667, 532, 283,  91, 136, 208, 255, 775, 319, 537, 399,  93, 220]),\n",
       " 'input_values': tensor([[ 0.0002,  0.0002,  0.0002,  ..., -0.0091, -0.0111, -0.0393]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processor = CtcDataProcessor(tokenizer, feature_extractor)\n",
    "data_processor(preprocessed_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f42e7d4-e719-411b-bbfb-8eb9d4fb7ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4979d74a04a43119bff6c84664b807b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1927 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57302882d7d405fa13da436e00343f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_processor = CtcDataProcessor(tokenizer, feature_extractor)\n",
    "train = preprocessed_train.map(data_processor, keep_in_memory=True, remove_columns=preprocessed_train.column_names)\n",
    "val = preprocessed_val.map(data_processor, keep_in_memory=True, remove_columns=preprocessed_val.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d5e826f-2e25-41e0-be1f-330a2b14a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCDataCollator:\n",
    "    # HuggingFace requires pad transcript tokens with this value\n",
    "    LABELS_PAD_IDX = -100\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_tokens(tokens_batch, type, pad_value=0.0):\n",
    "        \"\"\"\n",
    "            Function collates list of tokens\n",
    "        \"\"\"\n",
    "        return torch.nn.utils.rnn.pad_sequence(tokens_batch, batch_first=True, padding_value=pad_value)\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "            Function collates `input_values` and `labels` into one tensor respectively\n",
    "            Input: list with dicts, output of CTCDataProcessor\n",
    "            Output row includes `labels` column with tokenized sequence, `input_values` column with computed spectrogram and \n",
    "            `attention_mask` (0 for not-attending position, 1 for attending)\n",
    "        \"\"\"\n",
    "        input_values = self.collate_tokens([row['input_values'] for row in batch], type='input_values')\n",
    "        labels = self.collate_tokens([row['labels'] for row in batch], type='labels', pad_value=self.LABELS_PAD_IDX)\n",
    "        attention_mask = (labels != self.LABELS_PAD_IDX).int()\n",
    "        return {\"input_values\": input_values, \"labels\": labels, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec64935-37c3-4947-a5d3-22842ca6f6ce",
   "metadata": {},
   "source": [
    "#### Inference and metrics computing\n",
    "\n",
    "There you should use simple greedy straregy for CTC output decoding. \n",
    "\n",
    "Hint: Don't forget about padding value -100 in reference.\n",
    "\n",
    "Hint: Don't forget about CTC output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c3a17-2256-4b56-8c2c-1cb0b6ae4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load(\"wer\")\n",
    "\n",
    "class MetricsComputer:\n",
    "    def __call__(self, pred):\n",
    "        \"\"\"\n",
    "            Input: object with fields `predictions` for CTC model output and `label_ids` for tokenized reference;\n",
    "            Output: dict with key `wer` and computed wer\n",
    "        \"\"\"\n",
    "        # model prediction tensor, tensor batch_size x max_seq_len x vocab_size\n",
    "        preds_logits = pred.predictions\n",
    "        # reference, tensor batch_size x max_seq_len\n",
    "        label_ids = pred.label_ids\n",
    "        \n",
    "        preds = torch.argmax(preds_logits, dim=-1)\n",
    "        pred_str = tokenizer.batch_decode(preds)\n",
    "        label_str = [tokenizer.decode(ids[torch.where(ids != -100)]) for ids in label_ids]\n",
    "        \n",
    "    \n",
    "        print(f\"Prediction: {pred_str[0]}\")\n",
    "        print(f\"Reference: {label_str[0]}\")\n",
    "        \n",
    "        wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "        return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fa62c-b147-4fa7-9b47-77069b7e4fb3",
   "metadata": {},
   "source": [
    "#### Overfitting on train batch\n",
    "\n",
    "In this task you should check pipeline correctness by overfitting on you need to finetune Wav2Vec2 model and achieve 50 WER or lower accuracy on val set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00dd9bc-8449-4ae2-924c-660a2217cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test\",\n",
    "    per_device_train_batch_size=2, # you could increase batch size\n",
    "    gradient_accumulation_steps=8, \n",
    "    evaluation_strategy=\"steps\",\n",
    "    max_steps=3000,\n",
    "    fp16=True,\n",
    "    save_steps=50,\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=# YOUR CODE HERE, \n",
    "    weight_decay=# YOUR CODE HERE,\n",
    "    warmup_steps=# YOUR CODE HERE,\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad6f56-50d2-4788-8d6c-c38a279a5597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=CTCDataCollator(),\n",
    "    args=training_args,\n",
    "    compute_metrics=MetricsComputer(),\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c362d922-6f53-4faf-820f-348674bb25a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
