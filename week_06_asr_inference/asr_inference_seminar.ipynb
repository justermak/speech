{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8302d850-e97e-4393-8d5d-9021035d2340",
   "metadata": {},
   "source": [
    "# Week 12: ASR Inference\n",
    "\n",
    "In this seminar we are going to implement chunked-streaming for Conformer model Encoder.\n",
    "\n",
    "Some useful links (it is not *necessary* to read them right now &mdash; but they are here if you need them):\n",
    "\n",
    "* [Conformer paper](https://arxiv.org/pdf/2005.08100)\n",
    "* [Chunked streaming paper](https://arxiv.org/pdf/2312.17279)\n",
    "* [NeMo repository](https://github.com/NVIDIA/NeMo) &mdash; source of model weights and basic idea for our seminar\n",
    "* [Google streaming kws](https://github.com/google-research/google-research/tree/master/kws_streaming) &mdash; Implementation of streaming architectures for keyword spotting tasks by Google\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8eb8f-d7e1-4aad-bbc6-e5468a308ed7",
   "metadata": {},
   "source": [
    "## 0. Preparation.\n",
    "\n",
    "First we are going to install necessary libraries and import them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c087ac4-6abf-4815-9021-8bc70b4b2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: almost any version of pytorch will do. Cpu is enough for this seminar.\n",
    "\n",
    "!pip install librosa==0.10.1\n",
    "!pip install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c0cdd-bf03-409a-91d3-1951d805ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import librosa\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import queue\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import wave\n",
    "\n",
    "from IPython.display import Audio\n",
    "from typing import Callable, Optional\n",
    "from urllib.parse import urlencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd7ce0-9f0d-467a-b067-961b5c6605ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this may speed up cpu-inference\n",
    "torch.set_num_threads(1)\n",
    "torch.set_num_interop_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2017f92-6e75-4679-9ca5-ec7ef7d9c830",
   "metadata": {},
   "source": [
    "## 1. Basic streaming framework\n",
    "\n",
    "How are we going to implement streaming? Layer-by-layer.\n",
    "\n",
    "We will add a couple of methods to each layer that needs streaming:\n",
    "\n",
    "1) `streaming_forward`: Streaming forward will take an additional argument &mdash; `state`, which will be passed from call to call (in sort of an autoregressive manner). `state` can be anything, depending on a layer: tensor, list of tensors or some more complicated structure.\n",
    "\n",
    "2) `get_initial_state`:  This method will return state, that we will pass to the first invocation of `streaming_forward`.\n",
    "So, overall streaming layer (and model) usage will look something like this:\n",
    "\n",
    "```python\n",
    "state = model.get_initial_state()\n",
    "for chunk in chunk_iterator:\n",
    "    output, state = model.streaming_forward(chunk, state)\n",
    "    process_output(output)\n",
    "```\n",
    "\n",
    "#### Simplification:\n",
    "Right now we will implement streaming for `batch_size = 1`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fcd12b-a70e-4b0d-923a-c3fe81d75e1b",
   "metadata": {},
   "source": [
    "## 2. First example\n",
    "\n",
    "Let's look at an example and implement streaming for 1D causal convolution.\n",
    "\n",
    "Causal convolution is basically a convolution with $left\\_padding = kernel\\_size - 1$ and $right\\_padding = 0$\n",
    "\n",
    "<img src=\"./images/CausalConv.png\" style=\"margin-left:auto; margin-right:auto; height: 200px; width: auto\" />\n",
    "\n",
    "In order to produce an output at frame $i$, we need not only current frame $x_i$, but also $kernel\\_size - 1$ previous frames: \n",
    "$x_{i - kernel\\_size + 1} \\ldots, x_{i - 1}$\n",
    "\n",
    "(if $i < kernel\\_size - 1$ then some of these frames are set to zero &mdash; perfectly emulating left padding)\n",
    "\n",
    "So let's store these previous $kernel\\_size - 1$ frames in our state! Initial state then will be just $kernel\\_size - 1$ zeros\n",
    "\n",
    "Go ahead and implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e4d2d-7edc-45d2-8b0b-701a135f9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1D(torch.nn.Conv1d):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        self._in_channels = in_channels\n",
    "        self._left_padding = kernel_size - 1\n",
    "        self._right_padding = 0\n",
    "        self._stride = stride\n",
    "\n",
    "        super().__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=0,\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, pad=(self._left_padding, self._right_padding))\n",
    "        return super().forward(x)\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            (torch.Tensor): [1, in_channels, kernel_size - 1] - initial state (all zeros)\n",
    "        \"\"\"\n",
    "        # Note: do not forget to set right device and dtype\n",
    "        device = self.weight.device\n",
    "        dtype = self.weight.dtype\n",
    "\n",
    "        # Your code goes here:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def streaming_forward(self, x, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): [1, in_channels, time] - chunk of data.\n",
    "                Time should be divisible by stide.\n",
    "            state (torch.Tensor): [1, in_channels, kernel_size - 1] previous input values.\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor] - output and new state.\n",
    "        \"\"\"\n",
    "\n",
    "        # we demand that each input chunk time dimention is divisible by stride\n",
    "        # for the sake of clarity and simplicity\n",
    "        assert x.size(2) % self._stride == 0\n",
    "\n",
    "        # Your code goes here:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f8b6a3-cd20-4a32-af15-629382fd226f",
   "metadata": {},
   "source": [
    "Let's run a simple test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78634be5-468a-42b6-8cac-bef1be61a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_streaming(\n",
    "    layer: torch.nn.Module,\n",
    "    input_shape: tuple,\n",
    "    time_dimention: int,\n",
    "    chunk_size: int,\n",
    "    tolerance: float = 1e-5,\n",
    "    additional_inputs_fn: Optional[Callable[torch.Tensor, list[torch.Tensor]]] = None,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        assert input_shape[time_dimention] % chunk_size == 0\n",
    "        test_input = torch.tensor(np.random.randn(*input_shape).astype(np.float32))\n",
    "        try:\n",
    "            # if layer has parameters, we should move input to the same device\n",
    "            device = next(iter(layer.parameters())).device\n",
    "            test_input.to(device)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "        if additional_inputs_fn is None:\n",
    "            regular_output = layer(test_input)\n",
    "        else:\n",
    "            # Some layers have additional inputs (e.g. lengths)\n",
    "            # which could be mocked in test settings via additional_inputs_fn\n",
    "            additional_inputs = additional_inputs_fn(test_input)\n",
    "            regular_output = layer(test_input, *additional_inputs)\n",
    "\n",
    "        if isinstance(regular_output, tuple):\n",
    "            # some layers output several tensors, for test purposes we only need\n",
    "            # the first one\n",
    "            regular_output = regular_output[0]\n",
    "\n",
    "        \n",
    "        streaming_output = []\n",
    "        state = layer.get_initial_state()\n",
    "        for chunk_start in range(0, input_shape[time_dimention], chunk_size):\n",
    "            indices = [slice(None) for _ in range(len(input_shape))]\n",
    "            indices[time_dimention] = slice(chunk_start, chunk_start + chunk_size)\n",
    "            step_input = test_input[indices]\n",
    "            step_output, state = layer.streaming_forward(step_input, state)\n",
    "            streaming_output.append(step_output)\n",
    "        streaming_output = torch.cat(streaming_output, axis=time_dimention)\n",
    "        assert streaming_output.shape == regular_output.shape, (streaming_output.shape, regular_output.shape)\n",
    "        assert torch.abs(streaming_output - regular_output).max() < tolerance\n",
    "    print('Test OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf62f996-df60-4539-90c9-b9025b1ecea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_streaming(\n",
    "    layer=CausalConv1D(in_channels=3, out_channels=5, kernel_size=9, stride=1),\n",
    "    input_shape=(1, 3, 16),\n",
    "    time_dimention=2,\n",
    "    chunk_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b242192-222d-421e-bbd0-c18cefdecb3a",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully implemented a streaming layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c9151-abc2-49c0-baf0-314036a68a20",
   "metadata": {},
   "source": [
    "## 3. Implementing other streaming layers\n",
    "\n",
    "In \"chunked\"-conformer architecture, we have several types of layers:\n",
    "\n",
    "1) Activations, linear layers, convolutions with kernel_size=1 and other \"pointwise\" layers.\n",
    "2) CausalConv1D\n",
    "3) CausalConv2D\n",
    "4) RelPosMultiHeadAttention\n",
    "\n",
    "We do not need to do anything with 1) &mdash; these layers work as-is in streaming mode.\n",
    "\n",
    "We have already implemented streming mode for 2).\n",
    "\n",
    "Now let's implement streaming mode for 3) and 4) !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6b666-bd7d-4ac9-81e6-af1532317750",
   "metadata": {},
   "source": [
    "### 3.1 CausalConv2D.\n",
    "\n",
    "This is basically the same as CausalConv1D, but with extra dimention. Note, that this extra-dimention is not time-related, so we almost do not need to care about it for streaming purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e513a45-428b-4dd4-ad84-5b5c48fe2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv2D(torch.nn.Conv2d):\n",
    "    \"\"\"\n",
    "    A causal version of nn.Conv2d where each location in the 2D matrix would have no access to locations on its right or down\n",
    "    All arguments are the same as nn.Conv2d except padding which should be set as None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feats: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int = 1,\n",
    "        groups: int = 1,\n",
    "    ) -> None:\n",
    "        self._in_feats = in_feats\n",
    "        self._in_channels = in_channels\n",
    "        self._stride = stride\n",
    "\n",
    "        # Side note: originally (in NeMo repo) right_padding = bottom_padding = stride - 1\n",
    "        # but we change right_padding to 0 for better streaming consistency\n",
    "        # and keep _bottom_padding at stride - 1 to have matching weights shape\n",
    "        self._left_padding = kernel_size - 1\n",
    "        self._right_padding = 0\n",
    "        self._top_padding = kernel_size - 1\n",
    "        self._bottom_padding = stride - 1\n",
    "\n",
    "        super(CausalConv2D, self).__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=0,\n",
    "            groups=groups,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): [batch, channels, time, in_feats]\n",
    "        Returns:\n",
    "            torch.Tensor - output\n",
    "        \"\"\"\n",
    "        x = F.pad(x, pad=(self._top_padding, self._bottom_padding, self._left_padding, self._right_padding))\n",
    "        x = super().forward(x)\n",
    "        return x\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            (???): ??? - initial state\n",
    "        \"\"\"\n",
    "        # Your code goes here:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def streaming_forward(self, x, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): [1, in_channels, time, in_feats].\n",
    "                Time should be divisible by stride.\n",
    "            state (???): ???\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, ???] - output and new state.\n",
    "        \"\"\"\n",
    "        assert x.shape[2] % self._stride == 0\n",
    "\n",
    "        # Your code goes here:\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97eb00-7208-425e-88e9-abf8eb03d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_streaming(\n",
    "    layer=CausalConv2D(in_channels=3, in_feats=7, out_channels=5, kernel_size=3, stride=2),\n",
    "    input_shape=(1, 3, 16, 7),\n",
    "    time_dimention=2,\n",
    "    chunk_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9f2483-c370-486b-ae6a-5c1d58f95679",
   "metadata": {},
   "source": [
    "### 3.2 RelPosMultiHeadAttention\n",
    "\n",
    "\n",
    "#### 3.2.1 Recap\n",
    "\n",
    "First, let's recap how a (non-streaming) chunked relative position multi-head self-attention works.\n",
    "\n",
    "For simplicity of notation, in this recap we will assume batch=1 and num_heads=1.\n",
    "\n",
    "RelPosMultiHeadAttention performs following steps:\n",
    "\n",
    "1. Given vectors $x_0, \\cdots, x_{t - 1}$ transform them to three other \"sets\" of vectors: $q, k, v$\n",
    "\n",
    "<img src=\"./images/Attention-QKV.png\" style=\"margin-left:auto; margin-right:auto; height: 200px; width: auto\" />\n",
    "\n",
    "2. Using vectors $q$, pre-computed vector $U$ and vectors $k$, compute first attention matrix AC: $AC_{ij} := (q_i + U) \\cdot k_j^T$\n",
    "\n",
    "<img src=\"./images/Attention-AC.png\" style=\"margin-left:auto; margin-right:auto; height: 300px; width: auto\" />\n",
    "\n",
    "3. (start of \"relative position\" part): Given relative positional embeddings $PE_{t - 1}, \\ldots, PE_0, \\ldots PE_{-(t - 1)}$ transform them (linearly) to vectors $p_{t - 1}, \\ldots, p_{-(t - 1)}$\n",
    "\n",
    "<img src=\"./images/Attention-PE.png\" style=\"margin-left:auto; margin-right:auto; height: 150px; width: auto\" />\n",
    "\n",
    "\n",
    "4. Using vectors $q$, pre-computed vector $V$ (do not confuse this for value vectors $v$) and vectors $p$ compute matrix BD: $BD_{ij} := (q_i + V) \\cdot p_{i - j}^T$\n",
    "\n",
    "    4.1. To get BD we can first compute preliminary matrix BD': $BD'_{ij} := (q_i + V) \\cdot p_j^T$\n",
    "    <img src=\"./images/Attention-BD'.png\" style=\"margin-left:auto; margin-right:auto; height: 700px; width: auto\" />\n",
    "    <h5 align=\"center\">(Dark-colored squares represent BD matrix we want to extract) </h5>\n",
    "    4.2. And then we extract BD matrix from BD'. We will talk about specific operation a bit later\n",
    "\n",
    "<img src=\"./images/Attention-BD.png\" style=\"margin-left:auto; margin-right:auto; height: 250px; width: auto\" />\n",
    "\n",
    "5. We compute attention scores matrix $A := \\frac{AC + BD}{\\sqrt{d_k}}$, where $d_k$ is dimention of one attention head.\n",
    "\n",
    "6. (start of \"chunked\" part). What does \"chunked\" attention mean? It means, that we split all input into chunks, and each query vector will attend only to key/value vectors either in the same chunk, or in the several previous chunks:\n",
    "\n",
    "<img src=\"./images/Chunked-paper.png\" style=\"margin-left:auto; margin-right:auto; height: 250px; width: auto\" />\n",
    "<h5 align=\"center\">Note: this image is taked from <a href=\"https://arxiv.org/pdf/2312.17279\">paper</a>, so it's notation is a bit different from all other images </h5>\n",
    "\n",
    "We can achieve this result by using a mask for attention scores (in this example chunk_size = 2 and each element of chunk will also attend to previous 2 chunks):\n",
    "\n",
    "<img src=\"./images/Mask.png\" style=\"margin-left:auto; margin-right:auto; height: 250px; width: auto\" />\n",
    "<h5 align=\"center\">In this mask 1 means valid value and 0 means invalid. In the implementation we will use an inverse notation, with <span>True</span> indicating that value should be masked.</h5>\n",
    "\n",
    "Implementation detail: since mask will be identical for all MHA layers in the model, it will be computed externally (at the top-level of our model) and passed to MHA layer as an argument.\n",
    "\n",
    "7. What's left is basic attention stuff: mask attention scores, use softmax, combine with values\n",
    "\n",
    "```\n",
    "maked_A = A.masked_fill(~Mask, -1e4)\n",
    "attn = softmax(maked_A).masked_fill(~Mask, 0.0)\n",
    "result = matmul(attn, v)\n",
    "```\n",
    "8. Combine all the heads\n",
    "\n",
    "\n",
    "Note: in step 3 we have taken relative positional embeddings for distances $-(t - 1), \\ldots, t - 1$. Although we only need embeddings for distances $ - (left\\_context\\_in\\_chunks + 1) \\cdot chunk\\_size + 1, \\ldots, chunk\\_size - 1$ (other values will be canceled by mask) - extracting BD matrix is a bit easier with the whole range of positional embeddings.\n",
    "\n",
    "If you want, you may try to optimize this.\n",
    "\n",
    "\n",
    "### 3.2.2 Streaming RelPosMultiHeadAttention - ideas\n",
    "\n",
    "So, let's figure out how to stream RelPosMultiHeadAttention layer.\n",
    "\n",
    "Let's denote chunk_size as $C$ and left context (number of chunks to attend to on the left) as $L$.\n",
    "\n",
    "Imagine we have several (maybe 1) new chunks of input $x_{t}, \\ldots, x_{t + m \\cdot C - 1}$.\n",
    "\n",
    "We can compute queries, keys and values for these input $q_{t}, \\ldots, q_{t + m \\cdot C - 1}; k_{t}, \\ldots, k_{t + m \\cdot C - 1}; v_{t}, \\ldots v_{t + m \\cdot C - 1}$.\n",
    "\n",
    "\n",
    "In order to compute our attention matrices (AC and BD) we need\n",
    "\n",
    "* keys for previous $L \\cdot C$ inputs\n",
    "* relative positional embeddings for distances $C - 1, \\ldots, -(L + 1) \\cdot C + 1$\n",
    "\n",
    "Positional embeddings could be provided by caller (same as in case for non-streaming inference), but we have to get previous keys from somewhere &mdash; so let's store them in our state.\n",
    "\n",
    "After computing attention matrices we need to apply mask, use softmax and combine attention probabilities with values.\n",
    "\n",
    "As in case with keys we need to access values for previous $L \\cdot C$ inputs &mdash; we can also store them in our state.\n",
    "\n",
    "Mask is basically identical to the one in non-streaming version. However, we need to consider early time frames, where there are less then $L \\cdot C$ previous inputs. There are two ways to handle this:\n",
    "\n",
    "* Either have store variable-length tensors of previous keys and values in the state, where state size grows from 0 to $L \\cdot C$\n",
    "* Or have state store tensors of constant size ($L \\cdot C$), but modify the mask, so that we do not attend to non-existent inputs. In this case, we should keep track of number of processed inputs so far (so we would know what to mask). We may store them in a state &mdash; but not the state of attention layer, but the state of layer forming the mask.\n",
    "\n",
    "Variable-length tensors could be tricky to work with: e.g, if one were to convert their streaming model to other format (onnx, tflite, etc.), having variable-length tensors would make the process harder, if not impossible. So let's stick with the latter approach.\n",
    "\n",
    "#### To sum up\n",
    "Our state consists of:\n",
    "* Previous $L \\cdot C$ keys\n",
    "* Previous $L \\cdot C$ values\n",
    "\n",
    "We should also store number of processed inputs in the state of a layer that creates the mask.\n",
    "\n",
    "### 3.2.3 Streaming RelPosMultiHeadAttention - implementation preparation \n",
    "\n",
    "Let's start our implementation with `create_streaming_mask` function &mdash; since mask is the same for all attention layers in our architecture, we will pass it to `streaming_forward` function - the same way mask is passed to `forward` function.\n",
    "\n",
    "First, let's look at `create_attn_mask` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b74e0-45ff-4028-9d56-8a2c59797a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attn_mask(chunk_size: int, left_chunks_num: int, input_size: int, device: torch.device):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        chunk_size (int): chunk size\n",
    "        left_chunks_num (int): number of chunks to attend to to the left\n",
    "        input_size (int): number of inputs. Should be divisible by chunk size.\n",
    "        device (torch.device): device to store mask on.\n",
    "    Returns:\n",
    "        torch.Tensor: [1, input_size, input_size], bool, True means value should be masked.\n",
    "    \"\"\"\n",
    "\n",
    "    assert input_size % chunk_size == 0\n",
    "\n",
    "    # chunk_idx is tensor of shape [input_size]\n",
    "    chunk_idx = torch.arange(0, input_size, dtype=torch.int, device=device)\n",
    "    chunk_idx = torch.div(chunk_idx, chunk_size, rounding_mode=\"floor\")\n",
    "\n",
    "    # diff_chunks is tensor of shape [input_size, input_size]: diff_chunks[i, j] = chunk_idx[i] - chunk_idx[j]\n",
    "    diff_chunks = chunk_idx.unsqueeze(1) - chunk_idx.unsqueeze(0)\n",
    "\n",
    "    mask = torch.logical_and(\n",
    "        torch.le(diff_chunks, left_chunks_num), torch.ge(diff_chunks, 0)\n",
    "    )\n",
    "    return ~mask.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca76628-b5a9-4775-b24c-71ebc605da10",
   "metadata": {},
   "source": [
    "Let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466468df-068b-48ed-a98f-460c49b1c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_attn_mask(chunk_size=2, left_chunks_num=2, input_size=10, device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73933e-5fd7-40b9-8e64-080c429b3cd1",
   "metadata": {},
   "source": [
    "Let's look at some streaming mask examples:\n",
    "\n",
    "1) `processed_inputs` is low (we are at the beginning of an utterance)\n",
    "\n",
    "<img src=\"./images/StreamingMaskStart.png\" style=\"margin-left:auto; margin-right:auto; height: 600px; width: auto\" />\n",
    "\n",
    "2) `processed_inputs` is high (we are in the middle of an utterance)\n",
    "\n",
    "<img src=\"./images/StreamingMaskMiddle.png\" style=\"margin-left:auto; margin-right:auto; height: 600px; width: auto\" />\n",
    "\n",
    "Now it's your turn to implement `create_streaming_attn_mask`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f8fdc-90a3-48f0-bf23-699f509e5b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_streaming_attn_mask(\n",
    "    chunk_size: int,\n",
    "    left_chunks_num: int,\n",
    "    new_inputs_size: int,\n",
    "    processed_inputs: int,\n",
    "    device: torch.device\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        chunk_size (int): chunk size\n",
    "        left_chunks_num (int): number of chunks to attend to on the left\n",
    "        new_inputs_size (int): number of new inputs to process. Should be divisible by chunk size.\n",
    "        processed_inputs (int): number of inputs already processed. Should be divisible by chunk size.\n",
    "        device (torch.device): device to store mask on.\n",
    "    Returns:\n",
    "        torch.Tensor: [1, new_inputs_size, chunk_size * left_chunks_num + new_inputs_size], bool, True means value should be used.\n",
    "    \"\"\"\n",
    "    assert new_inputs_size % chunk_size == 0\n",
    "    assert processed_inputs % chunk_size == 0\n",
    "\n",
    "    # Your code goes here...\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96616ec-216d-4b44-b431-f719f7873728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run some tests.\n",
    "# converting result from bool to int here to make constants more compact\n",
    "\n",
    "assert (create_streaming_attn_mask(\n",
    "    chunk_size=2, \n",
    "    left_chunks_num=2,\n",
    "    new_inputs_size=4,\n",
    "    processed_inputs=0,\n",
    "    device=torch.device('cpu')\n",
    ").int() == torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 1, 0, 0, 1, 1],\n",
    "        [1, 1, 1, 1, 0, 0, 1, 1],\n",
    "        [1, 1, 1, 1, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 0, 0, 0, 0],\n",
    "    ]\n",
    ").unsqueeze(0)).all()\n",
    "\n",
    "assert (create_streaming_attn_mask(\n",
    "    chunk_size=1, \n",
    "    left_chunks_num=5,\n",
    "    new_inputs_size=3,\n",
    "    processed_inputs=3,\n",
    "    device=torch.device('cpu')\n",
    ").int() == torch.tensor(\n",
    "    [\n",
    "        [1, 1, 0, 0, 0, 0, 1, 1],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 1],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0],\n",
    "    ]\n",
    ").unsqueeze(0)).all()\n",
    "\n",
    "assert (create_streaming_attn_mask(\n",
    "    chunk_size=2, \n",
    "    left_chunks_num=2,\n",
    "    new_inputs_size=4,\n",
    "    processed_inputs=10,\n",
    "    device=torch.device('cpu')\n",
    ").int() == torch.tensor(\n",
    "    [\n",
    "        [0, 0, 0, 0, 0, 0, 1, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 1, 1],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0],\n",
    "    ]\n",
    ").unsqueeze(0)).all()\n",
    "\n",
    "assert (create_streaming_attn_mask(\n",
    "    chunk_size=2,\n",
    "    left_chunks_num=2,\n",
    "    new_inputs_size=4,\n",
    "    processed_inputs=6,\n",
    "    device=torch.device('cpu')\n",
    ").int() == torch.tensor(\n",
    "    [\n",
    "        [0, 0, 0, 0, 0, 0, 1, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 1, 1],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0],\n",
    "    ]\n",
    ").unsqueeze(0)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acff060-125e-4088-b1a6-589e75b1b6f5",
   "metadata": {},
   "source": [
    "Now let's implement extraction of BD from BD'.\n",
    "\n",
    "As we've already mentioned, although we only need positional embeddings for distances $C - 1, \\ldots, -((L + 1) \\cdot C - 1)$, we will require embeddings for distances $(num\\_keys - 1, \\ldots, -(num\\_queries - 1))$ &mdash; it will simplify implementation and add little (if any) overhead.\n",
    "\n",
    "* $num\\_queries$ is just number of inputs (either $input\\_length$ in case of non-streaming, or $new\\_input\\_size$ in case of streaming)\n",
    "* $num\\_keys$ is total number of keys in our attention (either $input\\_length$ is case of non-streaming or $L \\cdot C + num\\_new\\_inputs$ in case of streaming.\n",
    "\n",
    "Let's look how extractioin will work. First, in case of non-streaming:\n",
    "\n",
    "<img src=\"./images/Extraction-1.png\" style=\"margin-left:auto; margin-right:auto; height: 250px; width: auto\" />\n",
    "\n",
    "This is our BD' matrix. Dark-colored squares represent BD matrix we want to extract. Note, that if we look at the matrix as a contiguous array, distances between dark-colored values are the same &mdash; $num\\_queries - 1$.\n",
    "\n",
    "So let's perform the folowing transformations:\n",
    "1) view our matrix as an array\n",
    "2) drop first $num\\_queries - 1$ elements and one last element\n",
    "3) view array as a matrix ($num\\_queries$, $num\\_queries + num\\_keys - 2$)\n",
    "4) drop last $num\\_queries - 2$ columns\n",
    "\n",
    "Here is the visualization:\n",
    "\n",
    "<img src=\"./images/Extraction-2.png\" style=\"margin-left:auto; margin-right:auto; height: 800px; width: auto\" />\n",
    "\n",
    "Let's try the same with streaming setup:\n",
    "\n",
    "<img src=\"./images/Extraction-3.png\" style=\"margin-left:auto; margin-right:auto; height: 200px; width: auto\" />\n",
    "\n",
    "In this picture $C=2, L=2$, our new inputs are (6, 7, 8, 9, 10, 11) (3 chunks in total).\n",
    "\n",
    "Dark-colored and grey-colored squares represent BD matrix we want to extract: grey-colored are the values of BD matrix that will eventually be masked out by \"chunked\" masking.\n",
    "\n",
    "Let's see how our transformation look:\n",
    "\n",
    "<img src=\"./images/Extraction-4.png\" style=\"margin-left:auto; margin-right:auto; height: 650px; width: auto\" />\n",
    "\n",
    "Great. Now, let's write some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed89b05b-dbb3-4953-af02-d4fc096de0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bd_from_bd_prime(bd_prime: torch.Tensor):\n",
    "    \"\"\"Extract BD matrix from BD'\n",
    "    Args:\n",
    "        ad_prime (torch.Tensor): [batch, num_heads, num_queries, num_keys + num_queries - 1]\n",
    "            Reminder what BD' matrix is:\n",
    "            Given (for batch element b and head h)\n",
    "                - query vectors q_s, ..., q_{s + num_queries - 1}\n",
    "                - constant vector u\n",
    "                - (transformed) positional embeddings p_{(num_keys - 1)}, ..., p_0, ..., p_{-(num_keys - 1)}\n",
    "            BD'[b, h, i, j] = (q[b, h]_{s + i} + u) * (p_{j - num_keys + 1))^T\n",
    "            Note:\n",
    "                this is a bit different from notation is recap, specifically:\n",
    "                1) BD' last dimention is indexed with non-negative numbers, so we shift index in p by (num_keys - 1)\n",
    "                2) we allow queries first index to be non-zero - this will help us in streaming case\n",
    "    Returns:\n",
    "        torch.Tensor of shape [batch, num_heads, num_queries, num_keys]: BD matrix\n",
    "            BD[b, h, i, j] = BD'[b, h, i, i - j + num_keys - 1] = (q[b, h]_{s + i} + u) * p_{i - j}^T\n",
    "    \"\"\"\n",
    "    batch_size, num_heads, num_queries, num_pos_embeddings = bd_prime.size()\n",
    "    if num_queries == 1:\n",
    "        # in this case AD' and AD matrix are the same\n",
    "        return ad_prime\n",
    "\n",
    "    # we need input tensor to be contigent in memory for the next set of tricks\n",
    "    bd = bd_prime.contiguous()\n",
    "\n",
    "    # Your code goes here...\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc051f1-6ddd-4b36-b7b7-1692138a4797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run some tests.\n",
    "\n",
    "assert (extract_bd_from_bd_prime(torch.tensor(\n",
    "    [\n",
    "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0, -1, -2, -3, -4, -5],\n",
    "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0, -1, -2, -3, -4, -5],\n",
    "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0, -1, -2, -3, -4, -5],\n",
    "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0, -1, -2, -3, -4, -5],\n",
    "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0, -1, -2, -3, -4, -5],\n",
    "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0, -1, -2, -3, -4, -5],\n",
    "    ]\n",
    ").view(1, 1, 6, 6 + 10 - 1)) == torch.tensor(\n",
    "    [\n",
    "        [ 4,  3,  2,  1,  0, -1, -2, -3, -4, -5],\n",
    "        [ 5,  4,  3,  2,  1,  0, -1, -2, -3, -4],\n",
    "        [ 6,  5,  4,  3,  2,  1,  0, -1, -2, -3],\n",
    "        [ 7,  6,  5,  4,  3,  2,  1,  0, -1, -2],\n",
    "        [ 8,  7,  6,  5,  4,  3,  2,  1,  0, -1],\n",
    "        [ 9,  8,  7,  6,  5,  4,  3,  2,  1,  0]\n",
    "    ]\n",
    ").view(1, 1, 6, 10)).all()\n",
    "\n",
    "assert (extract_bd_from_bd_prime(torch.tensor(\n",
    "    [\n",
    "        [-7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7],\n",
    "        [-7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7],\n",
    "        [-7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7],\n",
    "        [-7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7],\n",
    "        [-7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7],\n",
    "        [-7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7],\n",
    "        [-7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7],\n",
    "        [-7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7],\n",
    "    ]\n",
    ").view(1, 1, 8, 8 + 8 - 1)) == torch.tensor(\n",
    "    [\n",
    "        [ 0,  1,  2,  3,  4,  5,  6,  7],\n",
    "        [-1,  0,  1,  2,  3,  4,  5,  6],\n",
    "        [-2, -1,  0,  1,  2,  3,  4,  5],\n",
    "        [-3, -2, -1,  0,  1,  2,  3,  4],\n",
    "        [-4, -3, -2, -1,  0,  1,  2,  3],\n",
    "        [-5, -4, -3, -2, -1,  0,  1,  2],\n",
    "        [-6, -5, -4, -3, -2, -1,  0,  1],\n",
    "        [-7, -6, -5, -4, -3, -2, -1,  0],\n",
    "    ]\n",
    ").view(1, 1, 8, 8)).all()\n",
    "\n",
    "\n",
    "assert (extract_bd_from_bd_prime(torch.tensor(\n",
    "    [\n",
    "        [5, 4, 3, 2, 1, 0, -1],\n",
    "        [5, 4, 3, 2, 1, 0, -1],\n",
    "    ]\n",
    ").view(1, 1, 2, 2 + 6 - 1)) == torch.tensor(\n",
    "    [\n",
    "        [ 4,  3,  2,  1,  0, -1],\n",
    "        [ 5,  4,  3,  2,  1,  0],\n",
    "    ]\n",
    ").view(1, 1, 2, 6)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84130cc7-a0b3-4f82-98d3-e351bcb193b5",
   "metadata": {},
   "source": [
    "Before implementing streaming layer let's define `RelPositionalEncoding` module &mdash; it will be useful for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea94a8a-d71d-4107-9ac7-d71c64c044f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelPositionalEncoding(torch.nn.Module):\n",
    "    \"\"\"Relative positional encoding for TransformerXL's layers\n",
    "    See : Appendix B in https://arxiv.org/abs/1901.02860\n",
    "    Args:\n",
    "        d_model (int): embedding dim\n",
    "        device (torch.device): device to store embeddings on.\n",
    "        max_len (int): maximum input length\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, device, max_len=5000):\n",
    "        \"\"\"Construct an PositionalEncoding object.\"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # [2 * max_len - 1, 1]\n",
    "        positions = torch.arange(max_len - 1, -max_len, -1, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "        # [2 * max_len - 1, d_model]\n",
    "        pe = torch.zeros(2 * max_len - 1, self.d_model, device=device)\n",
    "\n",
    "        # [1, d_model / 2]\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, self.d_model, 2, dtype=torch.float32, device=device)\n",
    "            * -(math.log(10000.0) / self.d_model)\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(positions * div_term)\n",
    "        pe[:, 1::2] = torch.cos(positions * div_term)\n",
    "\n",
    "        # [1, 2 * max_len - 1, d_model]\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, end_idx, start_idx):\n",
    "        \"\"\"Return positional embedding from end_idx to start_idx.\n",
    "        Args:\n",
    "            end_idx (int): end index\n",
    "            start_idx (int): start index\n",
    "        Note: it is required that start_idx <= end_idx\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor of shape [1, end_idx - start_idx + 1, d_model] - embeddings for \n",
    "                (end_idx, \\ldots, start_idx) distances\n",
    "        \"\"\"\n",
    "        center_pos = self.pe.size(1) // 2 + 1\n",
    "\n",
    "        end_idx = center_pos - end_idx\n",
    "        start_idx = center_pos - start_idx + 1\n",
    "\n",
    "        assert 0 <= end_idx < start_idx <= self.pe.size(1)\n",
    "        return self.pe[:, end_idx:start_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b24d70b-1ef1-4dbd-be7d-80462ec33363",
   "metadata": {},
   "source": [
    "### 3.2.4 Streaming RelPosMultiHeadAttention - implementation\n",
    "\n",
    "Now we are ready to implement streaming version of RelPosMultiHeadAttention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6924774-f30c-4e87-86c5-5eec2e0f872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelPositionMultiHeadAttention(torch.nn.Module):\n",
    "    \"\"\"Multi-Head Attention layer of Transformer-XL with support of relative positional encoding.\n",
    "    Paper: https://arxiv.org/abs/1901.02860\n",
    "    Args:\n",
    "        n_head (int): number of heads\n",
    "        n_feat (int): size of the features\n",
    "        chunk_size (int): chunk_size\n",
    "        left_chunks_num (int): number of chunks to attend to on the left\n",
    "        pos_bias_nonzero_init (bool, optional): initialize pos_bias vectors with nonzero values -- useful for testing\n",
    "    \n",
    "    Note: in forward method there is a mask arguments - it should already account for which elements to attend.\n",
    "    chunk_size and left_chunks_num parameters should mainly be used in get_init_state() method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_head: int,\n",
    "        n_feat: int,\n",
    "        chunk_size: int,\n",
    "        left_chunks_num: int,\n",
    "        pos_bias_nonzero_init: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert n_feat % n_head == 0\n",
    "        self.d_k = n_feat // n_head\n",
    "        self.s_d_k = math.sqrt(self.d_k)\n",
    "        self.n_head = n_head\n",
    "        self.chunk_size = chunk_size\n",
    "        self.left_chunks_num = left_chunks_num\n",
    "\n",
    "        self.linear_q = torch.nn.Linear(n_feat, n_feat)\n",
    "        self.linear_k = torch.nn.Linear(n_feat, n_feat)\n",
    "        self.linear_v = torch.nn.Linear(n_feat, n_feat)\n",
    "        self.linear_out = torch.nn.Linear(n_feat, n_feat)\n",
    "\n",
    "        # linear transformation for positional encoding\n",
    "        self.linear_pos = torch.nn.Linear(n_feat, n_feat, bias=False)\n",
    "\n",
    "        self.pos_bias_u = torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_k))\n",
    "        self.pos_bias_v = torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_k))\n",
    "\n",
    "        if pos_bias_nonzero_init:\n",
    "            torch.nn.init.xavier_uniform_(self.pos_bias_u)\n",
    "            torch.nn.init.xavier_uniform_(self.pos_bias_v)\n",
    "        else:\n",
    "            torch.nn.init.zeros_(self.pos_bias_u)\n",
    "            torch.nn.init.zeros_(self.pos_bias_v)\n",
    "\n",
    "    def forward_qkv(self, x):\n",
    "        \"\"\"Transforms query, key and value.\n",
    "        Args:\n",
    "            x (torch.Tensor): [batch, num_inputs, d_model] - input tensor\n",
    "        returns:\n",
    "            q (torch.Tensor): [batch, n_head, num_inputs, d_k]\n",
    "            k (torch.Tensor): [batch, n_head, num_inputs, d_k]\n",
    "            v (torch.Tensor): [batch, n_head, num_inputs, d_k]\n",
    "        \"\"\"\n",
    "        n_batch = x.size(0)\n",
    "        q = self.linear_q(x).view(n_batch, -1, self.n_head, self.d_k)\n",
    "        k = self.linear_k(x).view(n_batch, -1, self.n_head, self.d_k)\n",
    "        v = self.linear_v(x).view(n_batch, -1, self.n_head, self.d_k)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        return q, k, v\n",
    "\n",
    "    def forward_attention(self, value, scores, mask):\n",
    "        \"\"\"Compute attention context vector.\n",
    "        Args:\n",
    "            value (torch.Tensor): [batch, n_head, num_keys, d_k]\n",
    "                (num_keys is the same as num_values)\n",
    "            scores (torch.Tensor): [batch, n_head, num_queries, num_keys]\n",
    "            mask (torch.Tensor): [batch, num_queries, num_keys]\n",
    "                bool, True means value should be masked.\n",
    "        returns:\n",
    "            (torch.Tensor): [batch, n_head, num_queries, d_k] transformed `value` weighted by the attention scores\n",
    "        \"\"\"\n",
    "\n",
    "        n_batch = value.size(0)\n",
    "\n",
    "        # [batch, 1, num_queries, num_keys]\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask, -10000.0)\n",
    "\n",
    "        # [batch, n_head, num_queries, num_keys]\n",
    "        attn = torch.softmax(scores, dim=-1).masked_fill(mask, 0.0)\n",
    "\n",
    "        # [batch, n_head, num_queries, d_k]\n",
    "        x = torch.matmul(attn, value)  # (batch, head, time1, d_k)\n",
    "\n",
    "        # [batch, n_head, d_model]\n",
    "        x = x.transpose(1, 2).reshape(n_batch, -1, self.n_head * self.d_k)  \n",
    "        \n",
    "        # [batch, n_head, d_model]\n",
    "        return self.linear_out(x)\n",
    "\n",
    "    def forward(self, x, pos_emb, mask):\n",
    "        \"\"\"Compute 'Scaled Dot Product Attention' with rel. positional encoding.\n",
    "        Args:\n",
    "            x (torch.Tensor): [batch, num_inputs, d_model] - input tensor\n",
    "            pos_emb (torch.Tensor): [1, 2 * num_inputs - 1, size] - relative positional embeddings\n",
    "                for distances [num_inputs - 1, ..., 0, ..., -(num_inputs - 1)]\n",
    "            mask (torch.Tensor): [batch, num_inputs, num_inputs] - attention mask.\n",
    "                True means value should be masked.\n",
    "        Returns:\n",
    "            (torch.Tensor): [batch, num_queries, d_model] - output\n",
    "        \"\"\"\n",
    "\n",
    "        q, k, v = self.forward_qkv(x)\n",
    "\n",
    "        # [1, 2 * num_inputs - 1, n_head, d_k]\n",
    "        p = self.linear_pos(pos_emb).view(1, -1, self.n_head, self.d_k)\n",
    "\n",
    "        # [1, n_head, 2 * num_inputs - 1, d_k]\n",
    "        p = p.transpose(1, 2)\n",
    "\n",
    "        # [batch, head, num_inputs, d_k]\n",
    "        q_with_bias_u = (q + self.pos_bias_u.view(1, self.n_head, 1, self.d_k))\n",
    "\n",
    "        # [batch, head, num_inputs, d_k]\n",
    "        q_with_bias_v = (q + self.pos_bias_v.view(1, self.n_head, 1, self.d_k))\n",
    "\n",
    "        # compute attention score\n",
    "        # first compute matrix a and matrix c\n",
    "        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n",
    "        # [batch, head, num_inputs, num_inputs]\n",
    "        matrix_ac = torch.matmul(q_with_bias_u, k.transpose(-2, -1))\n",
    "\n",
    "        # compute matrix bd\n",
    "        # [batch, n_head, num_inputs, 2 * num_inputs - 1]\n",
    "        matrix_bd_prime = torch.matmul(q_with_bias_v, p.transpose(-2, -1))\n",
    "        \n",
    "        # [batch, n_head, num_inputs, num_inputs]\n",
    "        matrix_bd = extract_bd_from_bd_prime(matrix_bd_prime)\n",
    "\n",
    "        # [batch, n_head, num_inputs, num_inputs]\n",
    "        scores = (matrix_ac + matrix_bd) / self.s_d_k\n",
    "\n",
    "        out = self.forward_attention(v, scores, mask)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            state (dict): {\n",
    "                'keys': (torch.Tensor) - previous chunk_size * left_chunks_num keys\n",
    "                'values': (torch.Tensor) - previous chunk_size * left_chunks_num values\n",
    "            } - initial state\n",
    "        \"\"\"\n",
    "        # Do not forget to set correct device/dtype for your tensors\n",
    "        device = self.linear_q.weight.device\n",
    "        dtype = self.linear_q.weight.dtype\n",
    "\n",
    "        # Note: you may choose to store keys and values in a state as\n",
    "        # [batch, n_head, chunk_size * left_chunks_num, d_k] tensors or\n",
    "        # maybe some other transposed way for efficiency reasons - your choice!\n",
    "\n",
    "        # Your code goes here...\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def streaming_forward(self, x, pos_emb, mask, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): [1, num_queries, d_model] - new inputs\n",
    "            pos_emb (torch.Tensor): [1, num_queries + num_keys - 1, d_pos_emb] - relative positional embeddings for distances\n",
    "                num_keys - 1, ..., 0, ..., -(num_queries - 1)\n",
    "            mask (torch.Tensor): [1, num_queries, num_keys] - attention mask.\n",
    "                True means value should be masked.\n",
    "            state (dict): {\n",
    "                'keys': (torch.Tensor) - previous chunk_size * left_chunks_num keys\n",
    "                'values': (torch.Tensor) - previous chunk_size * left_chunks_num values\n",
    "            } - current state\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, dict]: output and new_state\n",
    "                output: (torch.Tensor(: [1, num_queries, d_model] - new outputs\n",
    "                state: dict: {\n",
    "                    'keys': (torch.Tensor) - previous chunk_size * left_chunks_num keys\n",
    "                    'values': (torch.Tensor) - previous chunk_size * left_chunks_num values\n",
    "                } - new state\n",
    "        \"\"\"\n",
    "        # Your code goes here...\n",
    "        raise NotImplementedError()\n",
    "        # Feel free to reuse forward_qkv and forward_attention methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3a38f-7e83-4d6d-923c-95764b81a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mha_layer(\n",
    "    n_head: int,\n",
    "    n_feat: int,\n",
    "    chunk_size: int,\n",
    "    left_chunks_num: int,\n",
    "    input_size: int,\n",
    "    chunks_per_step: int,\n",
    "    tolerance: float = 1e-5,\n",
    "    layer_constructor: Callable = RelPositionMultiHeadAttention\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        assert input_size % (chunk_size * chunks_per_step) == 0\n",
    "        device = torch.device('cpu')\n",
    "        pos_enc = RelPositionalEncoding(d_model=n_feat, device=device)\n",
    "\n",
    "        layer = layer_constructor(\n",
    "            n_head=n_head,\n",
    "            n_feat=n_feat,\n",
    "            chunk_size=chunk_size,\n",
    "            left_chunks_num=left_chunks_num,\n",
    "            pos_bias_nonzero_init=True\n",
    "        )\n",
    "\n",
    "        mask = create_attn_mask(chunk_size=chunk_size, left_chunks_num=left_chunks_num, input_size=input_size, device=device)\n",
    "        \n",
    "        test_input = torch.tensor(np.random.randn(1, input_size, n_feat).astype(np.float32)).to(device)\n",
    "        regular_output = layer(\n",
    "            x=test_input,\n",
    "            pos_emb=pos_enc(end_idx=(input_size - 1), start_idx=-(input_size - 1)),\n",
    "            mask=mask\n",
    "        )\n",
    "\n",
    "        state = layer.get_initial_state()\n",
    "        streaming_output = []\n",
    "        streaming_pe = pos_enc(end_idx=(left_chunks_num + chunks_per_step) * chunk_size - 1, start_idx=-(chunk_size * chunks_per_step - 1))\n",
    "        for start_idx in range(0, input_size, chunk_size * chunks_per_step):\n",
    "            chunk_input = test_input[:, start_idx:start_idx + chunk_size * chunks_per_step, :]\n",
    "            step_mask = create_streaming_attn_mask(\n",
    "                chunk_size=chunk_size, \n",
    "                left_chunks_num=left_chunks_num,\n",
    "                new_inputs_size=chunk_size * chunks_per_step,\n",
    "                processed_inputs=start_idx,\n",
    "                device=device\n",
    "            )\n",
    "            step_output, state = layer.streaming_forward(\n",
    "                x=chunk_input,\n",
    "                pos_emb=streaming_pe,\n",
    "                mask=step_mask,\n",
    "                state=state\n",
    "            )\n",
    "            streaming_output.append(step_output)\n",
    "        streaming_output = torch.cat(streaming_output, axis=1)\n",
    "\n",
    "        assert torch.abs(regular_output - streaming_output).max() < tolerance\n",
    "    print('Test ok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a788cd-7745-4440-874a-029e18809aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mha_layer(n_head=2, n_feat=4, chunk_size=2, left_chunks_num=1, input_size=32, chunks_per_step=1)\n",
    "test_mha_layer(n_head=2, n_feat=4, chunk_size=2, left_chunks_num=1, input_size=32, chunks_per_step=2)\n",
    "\n",
    "test_mha_layer(n_head=2, n_feat=4, chunk_size=2, left_chunks_num=5, input_size=32, chunks_per_step=1)\n",
    "test_mha_layer(n_head=2, n_feat=4, chunk_size=2, left_chunks_num=5, input_size=32, chunks_per_step=2)\n",
    "\n",
    "test_mha_layer(n_head=4, n_feat=32, chunk_size=3, left_chunks_num=5, input_size=60, chunks_per_step=1)\n",
    "test_mha_layer(n_head=4, n_feat=32, chunk_size=3, left_chunks_num=5, input_size=60, chunks_per_step=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c379fd9f-6506-4d87-af28-d66a27a735c5",
   "metadata": {},
   "source": [
    "Great! We have implemented all basic streaming layers.\n",
    "\n",
    "### 4. Conformer Feed Forward layer.\n",
    "\n",
    "This is a section to rest a bit. We will define ConformerFeedForward layer, which requires no streaming support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f8b13c-c4bc-483a-9fff-c644a5f8b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerFeedForward(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        activation: torch.nn.Module = torch.nn.SiLU()\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._d_model = d_model\n",
    "        self._d_ff = d_ff\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(self._d_model, self._d_ff)\n",
    "        self.activation = activation\n",
    "        self.linear2 = torch.nn.Linear(self._d_ff, self._d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013fefd8-ae89-4f90-b702-3c1b226b4275",
   "metadata": {},
   "source": [
    "### 5. Composite layers and putting it all together.\n",
    "\n",
    "Now let's implement streaming for \"composite\" layers &mdash; layers, which are basically applying several inner layers, some of which are streaming.\n",
    "\n",
    "\n",
    "Basic framework of making streaming version of \"composite\" layers is simple: state is just combination of states of inner streaming layers (e.g, list of states or dict of inner layer name -> state).\n",
    "\n",
    "`streaming_forward` method looks almost identical to `forward`, except for streaming inner layers we call `streaming_forward` instead of `forward` (and \"update\" their state along the way).\n",
    "\n",
    "### 5.1 ConformerConvolution\n",
    "Let's look at an example:\n",
    "\n",
    "`ConformerConvolution` layer consists of several pointwise operations (activations, 1x1 convolutions, layernorm) as well as one `CausalConv1D`.\n",
    "\n",
    "Since there is only one streaming inner layer, we can make `ConformerConvolution` state just `CausalConv1D`'s state.\n",
    "\n",
    "`streaming_forward` looks almost exactly like `forward`, except we call `streaming_forward` for causal convolution\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d13c320-25d6-4425-b342-2e7a0824e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerConvolution(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        kernel_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert (kernel_size - 1) % 2 == 0\n",
    "        self._d_model = d_model\n",
    "        self._kernel_size = kernel_size\n",
    "        self.pointwise_activation = lambda x: torch.nn.functional.glu(x, dim=1)\n",
    "\n",
    "        self.pointwise_conv1 = torch.nn.Conv1d(\n",
    "            in_channels=self._d_model, out_channels=self._d_model * 2, kernel_size=1, stride=1, padding=0, bias=True\n",
    "        )\n",
    "        self.depthwise_conv = CausalConv1D(\n",
    "            in_channels=self._d_model,\n",
    "            out_channels=self._d_model,\n",
    "            kernel_size=self._kernel_size,\n",
    "            stride=1,\n",
    "            groups=self._d_model,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        # yep, batch_norm here is layer norm - for weight compatibility reason.\n",
    "        self.batch_norm = torch.nn.LayerNorm(self._d_model)\n",
    "        self.activation = torch.nn.SiLU()\n",
    "        self.pointwise_conv2 = torch.nn.Conv1d(\n",
    "            in_channels=self._d_model, out_channels=d_model, kernel_size=1, stride=1, padding=0, bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): [batch, time, feats] - input tensor\n",
    "        Returns:\n",
    "            (torch.Tensor) - layer output\n",
    "        \"\"\"\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.pointwise_conv1(x)\n",
    "        x = self.pointwise_activation(x)\n",
    "\n",
    "        x = self.depthwise_conv(x)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.batch_norm(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = self.activation(x)\n",
    "        x = self.pointwise_conv2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            torch.Tensor: initial state\n",
    "        \"\"\"\n",
    "        # Your code goes here...\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def streaming_forward(self, x, state):\n",
    "        \"\"\"Args:\n",
    "            x (torch.Tensor): [1, time, feats] - input tensor\n",
    "            state (torch.Tensor): state\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]: output tensor and new state.\n",
    "        \"\"\"\n",
    "\n",
    "        # Your code goes here...\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bc3a1-f69e-4795-a5f4-e024444f1aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_streaming(\n",
    "    layer=ConformerConvolution(d_model=5, kernel_size=9),\n",
    "    input_shape=(1, 16, 5),\n",
    "    time_dimention=1,\n",
    "    chunk_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23135806-3beb-442d-9da4-645b376f9a16",
   "metadata": {},
   "source": [
    "### 5.2 ConvSubsampling\n",
    "\n",
    "Next \"composite\" layer is `ConvSubsampling`. It consists of several `CausalConv2D` layers, as well as some 1x1 convolutions and activations.\n",
    "\n",
    "Making a streaming version of it is, again, pretty straightforward &mdash; state is just a list of all inner `CausalConv2D` states and instead of `forward` calls to `CausalConv2D` we are making `streaming_forward` call.\n",
    "\n",
    "The only new thing is this: `forward` method takes `lengths` argument &mdash; length of each input element in batch\n",
    "and returns not only output tensor, but also `lengths` tensor &mdash; length of each output element in a batch.\n",
    "\n",
    "Since we are currently implementing streaming version only for the case of `batch_size=1`, we do not need to worry about length input argument and return value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c754f-daa4-4452-b7be-a8e40b24bedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, a little helper function\n",
    "\n",
    "def calc_length(lengths: torch.Tensor, paddings: int, kernel_size: int, stride: int, repeat_num: int):\n",
    "    \"\"\"Calculates the output length of a Tensor passed through series of convolution or max pooling layer\"\"\"\n",
    "    add_pad: int = paddings - kernel_size\n",
    "    one: float = 1.0\n",
    "    for i in range(repeat_num):\n",
    "        lengths = torch.div(lengths.to(dtype=torch.float) + add_pad, stride) + one\n",
    "        lengths = torch.floor(lengths)\n",
    "    return lengths.to(dtype=torch.int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1010e943-852a-4e45-8f14-deeaa1f8a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvSubsampling(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        subsampling_factor: int,\n",
    "        feat_in: int,\n",
    "        feat_out: int,\n",
    "        conv_channels: int,\n",
    "        activation: torch.nn.Module,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._conv_channels = conv_channels\n",
    "        self._feat_in = feat_in\n",
    "        self._feat_out = feat_out\n",
    "\n",
    "        # checking that subsampling_factor is a power of 2\n",
    "        assert subsampling_factor & (subsampling_factor - 1) == 0\n",
    "\n",
    "        self._sampling_num = int(math.log(subsampling_factor, 2))\n",
    "        self._subsampling_factor = subsampling_factor\n",
    "\n",
    "        in_channels = 1\n",
    "        layers = []\n",
    "\n",
    "        self._stride = 2\n",
    "        self._kernel_size = 3\n",
    "\n",
    "        self._left_padding = self._kernel_size - 1\n",
    "        self._right_padding = 0\n",
    "        self._top_padding = self._kernel_size - 1\n",
    "        self._bottom_padding = self._stride - 1\n",
    "\n",
    "        layers.append(\n",
    "            CausalConv2D(\n",
    "                in_feats=self._feat_in,\n",
    "                in_channels=in_channels,\n",
    "                out_channels=conv_channels,\n",
    "                kernel_size=self._kernel_size,\n",
    "                stride=self._stride,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        in_channels = conv_channels\n",
    "        out_length = int(\n",
    "            calc_length(\n",
    "                torch.tensor(self._feat_in, dtype=torch.float),\n",
    "                paddings=self._top_padding + self._bottom_padding,\n",
    "                kernel_size=self._kernel_size,\n",
    "                stride=self._stride,\n",
    "                repeat_num=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        layers.append(activation)\n",
    "\n",
    "        for i in range(self._sampling_num - 1):\n",
    "            layers.append(\n",
    "                CausalConv2D(\n",
    "                    in_feats=out_length,\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    kernel_size=self._kernel_size,\n",
    "                    stride=self._stride,\n",
    "                    groups=in_channels,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            layers.append(\n",
    "                torch.nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=conv_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=1,\n",
    "                )\n",
    "            )\n",
    "            layers.append(activation)\n",
    "            in_channels = conv_channels\n",
    "            out_length = int(\n",
    "                calc_length(\n",
    "                    torch.tensor(out_length, dtype=torch.float),\n",
    "                    paddings=self._top_padding + self._bottom_padding,\n",
    "                    kernel_size=self._kernel_size,\n",
    "                    stride=self._stride,\n",
    "                    repeat_num=1\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.out = torch.nn.Linear(conv_channels * out_length, self._feat_out)\n",
    "        self.conv = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): [batch, max_time, features] - input tensor\n",
    "            lengths (torch.Tensor): [batch] -  lengths of inputs\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]: output tensor and output lengths\n",
    "        \"\"\"\n",
    "        lengths = calc_length(\n",
    "            lengths,\n",
    "            paddings=self._left_padding + self._right_padding,\n",
    "            kernel_size=self._kernel_size,\n",
    "            stride=self._stride,\n",
    "            repeat_num=self._sampling_num,\n",
    "        )\n",
    "\n",
    "        # [batch, 1, time, features]\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        batch, channels, time, features = x.size()\n",
    "\n",
    "        # [batch, time channels * features]\n",
    "        x = x.transpose(1, 2).reshape(batch, time, -1)\n",
    "        x = self.out(x)\n",
    "        return x, lengths\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list[torch.Tensor]: initial layer state - list of initial states of\n",
    "                inner layers\n",
    "        \"\"\"\n",
    "        # Your code goes here...\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # hint: you can distinguish CausalConv2D layers using isinstance(layer, CausalConv2D)\n",
    "\n",
    "\n",
    "    def streaming_forward(self, x, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): [1, time, features] - input.\n",
    "            state (list[torch.Tensor]): state.\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, list[torch.Tensor]]: output and new state.\n",
    "        \"\"\"\n",
    "        # Your code goes here\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f984f894-f060-4c90-b29e-1c801c7fff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_streaming(\n",
    "    layer=ConvSubsampling(subsampling_factor=8, feat_in=5, feat_out=3, conv_channels=3, activation=torch.nn.ReLU()),\n",
    "    input_shape=(1, 40, 5),\n",
    "    time_dimention=1,\n",
    "    chunk_size=8,\n",
    "    additional_inputs_fn = lambda x: [torch.tensor([x.size(1)])]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45835609-a5d8-4e92-9b3b-b8cc004e7435",
   "metadata": {},
   "source": [
    "### 5.2 ConformerLayer\n",
    "\n",
    "Next \"composite\" layer is `ConformerLayer`. It consists of several `layer_norm` layers, several `ConformerFeedForward` layers, a `ConformerConvolution` and a `RelPositionMultiHeadAttention` layer.\n",
    "\n",
    "Although it may look intimidating, there is actually nothing new here. Just combine states and pass them along!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc19f7a5-5d52-4c1b-bb58-f05bd417dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerLayer(torch.nn.Module):\n",
    "    \"\"\"A single block of the Conformer encoder.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): input dimension of RelPositionMultiHeadAttention and ConformerFeedForward\n",
    "        d_ff (int): hidden dimension of ConformerFeedForward\n",
    "        n_heads (int): number of heads for multi-head attention\n",
    "        conv_kernel_size (int): kernel size for depthwise convolution in convolution module\n",
    "        chunk_size (int): chunk_size\n",
    "        left_chunks_num (int): number of chunks to attend to on the left\n",
    "        pos_bias_nonzero_init (bool, optional): initialize pos_bias vectors in RelPositionMultiHeadAttention module\n",
    "            with nonzero values - useful for testing\n",
    "    \n",
    "    Note: in forward method there is a mask argument - it should already account for which elements to attend.\n",
    "    chunk_size and left_chunks_num parameters are just passed to RelPositionMultiHeadAttention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        n_heads: int,\n",
    "        conv_kernel_size: int,\n",
    "        chunk_size: int,\n",
    "        left_chunks_num: int,\n",
    "        pos_bias_nonzero_init: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._d_model = d_model\n",
    "        self._d_ff = d_ff\n",
    "        self._n_heads = n_heads\n",
    "        self._conv_kernel_size = conv_kernel_size\n",
    "\n",
    "        self._fc_factor = 0.5\n",
    "\n",
    "        self.norm_feed_forward1 = torch.nn.LayerNorm(self._d_model)\n",
    "        self.feed_forward1 = ConformerFeedForward(d_model=self._d_model, d_ff=self._d_ff)\n",
    "\n",
    "        self.norm_conv = torch.nn.LayerNorm(self._d_model)\n",
    "        self.conv = ConformerConvolution(\n",
    "            d_model=self._d_model,\n",
    "            kernel_size=self._conv_kernel_size,\n",
    "        )\n",
    "\n",
    "        self.norm_self_att = torch.nn.LayerNorm(self._d_model)\n",
    "\n",
    "        self.self_attn = RelPositionMultiHeadAttention(\n",
    "            n_head=self._n_heads,\n",
    "            n_feat=self._d_model,\n",
    "            chunk_size=chunk_size,\n",
    "            left_chunks_num=left_chunks_num,\n",
    "            pos_bias_nonzero_init=pos_bias_nonzero_init,\n",
    "        )\n",
    "\n",
    "        self.norm_feed_forward2 = torch.nn.LayerNorm(self._d_model)\n",
    "        self.feed_forward2 = ConformerFeedForward(d_model=self._d_model, d_ff=self._d_ff)\n",
    "\n",
    "        self.norm_out = torch.nn.LayerNorm(self._d_model)\n",
    "\n",
    "    def forward(self, x, pos_emb, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): [batch, num_inputs, d_model] - input\n",
    "            pos_emb (torch.Tensor): [batch, 2 * num_inputs - 1, size] - relative positional embeddings\n",
    "                for distances num_inputs - 1, ..., -(num_inputs - 1)\n",
    "            mask (torch.Tensor): [batch, num_inputs, num_inputs] - attention mask\n",
    "                True means value should be masked.\n",
    "        Returns:\n",
    "            torch.Tensor: [batch, num_inputs, d_model] - output\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = self.norm_feed_forward1(x)\n",
    "        x = self.feed_forward1(x)\n",
    "        residual = residual + x * self._fc_factor\n",
    "\n",
    "        x = self.norm_self_att(residual)\n",
    "        x = self.self_attn(x, pos_emb=pos_emb, mask=mask)\n",
    "\n",
    "        residual = residual + x\n",
    "\n",
    "        x = self.norm_conv(residual)\n",
    "        x = self.conv(x)\n",
    "        residual = residual + x\n",
    "\n",
    "        x = self.norm_feed_forward2(residual)\n",
    "        x = self.feed_forward2(x)\n",
    "        residual = residual + x * self._fc_factor\n",
    "\n",
    "        x = self.norm_out(residual)\n",
    "        return x\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        \"\"\"Returns:\n",
    "            ???: initial state.\n",
    "        \"\"\"\n",
    "        # Your code goes here\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def streaming_forward(self, x, pos_emb, mask, state):\n",
    "        \"\"\"Args:\n",
    "            x (torch.Tensor): [1, num_queries, d_model] - new inputs\n",
    "            pos_emb (torch.Tensor): [1, 2 * num_queries + chunk_size * left_chunks_num - 1, size] -\n",
    "                relative positional embeddings for distances \n",
    "                    chunk_size * left_chunks_num + num_queries - 1, ..., num_queries - 1\n",
    "            mask (torch.Tensor): [1, num_queries, chunk_size * left_chunks_num + num_queries] - attention mask.\n",
    "                True means value should be masked.\n",
    "            state (???): state.\n",
    "        \"\"\"\n",
    "        # Your code goes here\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b365ce40-44dd-4b75-881c-0a57af673d15",
   "metadata": {},
   "source": [
    "Signature of ConformerLayer call is the same as signature of RelPositionMultiHeadAttention call, so we will reuse `test_mha_layer` function for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb6605-379f-4ab6-b9f9-796b4e6e5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conformer_layer_constructor(d_ff: int = 4, conv_kernel_size: int = 9):\n",
    "    \"\"\"Adapter to pass to test_mha_layer function\"\"\"\n",
    "\n",
    "    def constructor(n_head: int, n_feat: int, chunk_size: int, left_chunks_num: int, pos_bias_nonzero_init: bool):\n",
    "        return ConformerLayer(\n",
    "            d_model=n_feat,\n",
    "            d_ff=d_ff,\n",
    "            n_heads=n_head,\n",
    "            conv_kernel_size=conv_kernel_size,\n",
    "            chunk_size=chunk_size,\n",
    "            left_chunks_num=left_chunks_num,\n",
    "            pos_bias_nonzero_init=pos_bias_nonzero_init\n",
    "        )\n",
    "    return constructor\n",
    "\n",
    "test_mha_layer(\n",
    "    n_head=2,\n",
    "    n_feat=4,\n",
    "    chunk_size=2,\n",
    "    left_chunks_num=1,\n",
    "    input_size=32,\n",
    "    chunks_per_step=1,\n",
    "    layer_constructor=make_conformer_layer_constructor()\n",
    ")\n",
    "test_mha_layer(\n",
    "    n_head=2,\n",
    "    n_feat=4,\n",
    "    chunk_size=2,\n",
    "    left_chunks_num=1,\n",
    "    input_size=32,\n",
    "    chunks_per_step=2,\n",
    "    layer_constructor=make_conformer_layer_constructor()\n",
    ")\n",
    "\n",
    "test_mha_layer(\n",
    "    n_head=2,\n",
    "    n_feat=4,\n",
    "    chunk_size=2,\n",
    "    left_chunks_num=5,\n",
    "    input_size=32,\n",
    "    chunks_per_step=1,\n",
    "    layer_constructor=make_conformer_layer_constructor()\n",
    ")\n",
    "test_mha_layer(\n",
    "    n_head=2,\n",
    "    n_feat=4,\n",
    "    chunk_size=2,\n",
    "    left_chunks_num=5,\n",
    "    input_size=32,\n",
    "    chunks_per_step=2,\n",
    "    layer_constructor=make_conformer_layer_constructor()\n",
    ")\n",
    "\n",
    "test_mha_layer(\n",
    "    n_head=4,\n",
    "    n_feat=32,\n",
    "    chunk_size=3,\n",
    "    left_chunks_num=5,\n",
    "    input_size=60,\n",
    "    chunks_per_step=1,\n",
    "    layer_constructor=make_conformer_layer_constructor()\n",
    ")\n",
    "test_mha_layer(\n",
    "    n_head=4,\n",
    "    n_feat=32,\n",
    "    chunk_size=3,\n",
    "    left_chunks_num=5,\n",
    "    input_size=60,\n",
    "    chunks_per_step=2,\n",
    "    layer_constructor=make_conformer_layer_constructor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da40e89-2f0a-42c3-ba9b-2711128619b2",
   "metadata": {},
   "source": [
    "### 5.3 ConformerEncoder &mdash; puting it all together\n",
    "\n",
    "Finally, we are going to implement streaming Conformer Encoder.\n",
    "\n",
    "There are several differences from other \"composite\" layers:\n",
    "* We need to call `RelPositionalEncoding` layer (arguments will be different for streaming and non-streaming version)\n",
    "* We need to create masks &mdash; they are different for streaming and non-streaming version\n",
    "* Also in order to create masks we need to keep track of number of processed inputs. So in the state there will be not only inner layer states, but also an integer &mdash; number of processed inputs\n",
    "\n",
    "Otherwise, implementation is pretty straightforward.\n",
    "\n",
    "Note: when using number of processed inputs, keep track of what inputs are you counting &mdash; are these inputs to ConformerEncoder, or are these inputs to ConformerLayer? They are different because of subsampling!\n",
    "\n",
    "\n",
    "<details> \n",
    "  <summary>Hint</summary>\n",
    "   You may look at <span>test_mha_layer</span> function if you struggle with creation of relative positional embeddings or mask\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e25e4f-7fd5-4b90-9189-862687e9a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_in: int,\n",
    "        n_layers: int,\n",
    "        d_model: int,\n",
    "        ff_expansion_factor: int,\n",
    "        n_heads: int,\n",
    "        subsampling_factor: int,\n",
    "        subsampling_conv_channels: int,\n",
    "        chunk_size: int,\n",
    "        left_chunks_num: int,\n",
    "        conv_kernel_size: int,\n",
    "        pos_emb_max_len: int = 5000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self._feat_in = feat_in\n",
    "        self._n_layers = n_layers\n",
    "        self._d_model = d_model\n",
    "        self._ff_expansion_factor = ff_expansion_factor\n",
    "        self._n_heads = n_heads\n",
    "        self._subsampling_factor = subsampling_factor\n",
    "        self._subsampling_conv_channels = subsampling_conv_channels\n",
    "        self._x_scale = math.sqrt(self._d_model)\n",
    "\n",
    "        self._chunk_size = chunk_size\n",
    "        self._left_chunks_num = left_chunks_num\n",
    "        self._conv_kernel_size = conv_kernel_size\n",
    "        self._pos_emb_max_len = pos_emb_max_len\n",
    "\n",
    "        self.pre_encode = ConvSubsampling(\n",
    "            subsampling_factor=self._subsampling_factor,\n",
    "            feat_in=self._feat_in,\n",
    "            feat_out=self._d_model,\n",
    "            conv_channels=self._subsampling_conv_channels,\n",
    "            activation=torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self._feat_out = d_model\n",
    "\n",
    "        self.pos_enc = RelPositionalEncoding(\n",
    "            d_model=d_model,\n",
    "            max_len=pos_emb_max_len,\n",
    "            device=next(iter(self.pre_encode.parameters())).device\n",
    "        )\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            layer = ConformerLayer(\n",
    "                d_model=self._d_model,\n",
    "                d_ff=self._d_model * self._ff_expansion_factor,\n",
    "                n_heads=self._n_heads,\n",
    "                conv_kernel_size=self._conv_kernel_size,\n",
    "                chunk_size=self._chunk_size,\n",
    "                left_chunks_num=self._left_chunks_num,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, features, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (torch.Tensor): [batch, input_size, features] - input features.\n",
    "            lengths (torch.Tensor): [batch] - input lengths.\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor] - output features and lengths.\n",
    "        \"\"\"\n",
    "        features, lengths = self.pre_encode(x=features, lengths=lengths)\n",
    "        lengths = lengths.to(torch.int64)\n",
    "\n",
    "        features = features * self._x_scale\n",
    "\n",
    "        # this is different form input_size, because of subsampling!\n",
    "        layers_input_size = features.size(1)\n",
    "\n",
    "        pos_emb = self.pos_enc(end_idx=layers_input_size - 1, start_idx=-(layers_input_size - 1))\n",
    "\n",
    "        # [1, input_size, input_size]\n",
    "        chunked_mask = create_attn_mask(\n",
    "            chunk_size=self._chunk_size,\n",
    "            left_chunks_num=self._left_chunks_num,\n",
    "            input_size=layers_input_size,\n",
    "            device=features.device\n",
    "        )\n",
    "\n",
    "        # [batch, input_size, 1]\n",
    "        # padding_mask[i, j, 0] = ~(i < lengths[j])\n",
    "        padding_mask = ~(\n",
    "            torch.arange(0, layers_input_size, device=features.device).unsqueeze(0) < lengths.unsqueeze(-1)\n",
    "        ).unsqueeze(-1)\n",
    "        mask = torch.logical_or(chunked_mask, padding_mask)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            features = layer(\n",
    "                x=features,\n",
    "                mask=mask,\n",
    "                pos_emb=pos_emb\n",
    "            )\n",
    "\n",
    "        return features, lengths\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        \"\"\"Returns:\n",
    "            ???: initial state\n",
    "        \"\"\"\n",
    "        # Your code goes here...\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def streaming_forward(self, features, state):\n",
    "        \"\"\"Args:\n",
    "            features (torch.Tensor): [1, new_input_size, features] - new inputs\n",
    "            state (???): input state\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, ???] - new outputs and new state.\n",
    "        \"\"\"\n",
    "\n",
    "        # Your code goes here...\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec59ef0d-5f52-4b28-b486-64ecaff36c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_streaming(\n",
    "    layer=ConformerEncoder(\n",
    "        feat_in=3,\n",
    "        n_layers=2,\n",
    "        d_model=8,\n",
    "        ff_expansion_factor=2,\n",
    "        n_heads=2,\n",
    "        subsampling_factor=4,\n",
    "        subsampling_conv_channels=3,\n",
    "        chunk_size=2,\n",
    "        left_chunks_num=3,\n",
    "        conv_kernel_size=9,\n",
    "    ),\n",
    "    input_shape=(1, 80, 3),\n",
    "    time_dimention=1,\n",
    "    chunk_size=2 * 4,\n",
    "    additional_inputs_fn = lambda x: [torch.tensor([x.size(1)])]\n",
    ")\n",
    "\n",
    "test_streaming(\n",
    "    layer=ConformerEncoder(\n",
    "        feat_in=3,\n",
    "        n_layers=2,\n",
    "        d_model=8,\n",
    "        ff_expansion_factor=2,\n",
    "        n_heads=2,\n",
    "        subsampling_factor=4,\n",
    "        subsampling_conv_channels=3,\n",
    "        chunk_size=2,\n",
    "        left_chunks_num=3,\n",
    "        conv_kernel_size=9,\n",
    "    ),\n",
    "    input_shape=(1, 80, 3),\n",
    "    time_dimention=1,\n",
    "    chunk_size=4 * 4,\n",
    "    additional_inputs_fn = lambda x: [torch.tensor([x.size(1)])]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156dc896-d74e-430e-b5af-119db60546be",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully implemented streaming ConformerEncoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84653c85-8b54-4a52-9590-a6ee904fe2b9",
   "metadata": {},
   "source": [
    "## 6. Testing with real-life data.\n",
    "\n",
    "Now, let's relax, load some real weights and put our hard work in action!\n",
    "\n",
    "### 6.1. Helper classes.\n",
    "\n",
    "But first, we need to define several helper classes &mdash; filterbank feature calculator and Greedy CTC Decoder.\n",
    "\n",
    "Note: if you don't want to, you don't need to understand implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560003d8-0300-4aaf-a3d0-9bfe62a6e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_singal_to_tensor(int_signal: np.ndarray, device: torch.device) -> torch.Tensor:\n",
    "    return torch.Tensor(\n",
    "        int_signal.astype(np.float32) / np.float32(2. ** 15)\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "class FilterbankFeatures(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_rate=16000,\n",
    "        n_window_size=400,\n",
    "        n_window_stride=160,\n",
    "        preemph=0.97,\n",
    "        nfilt=80,\n",
    "        lowfreq=0,\n",
    "        highfreq=None,\n",
    "        log=True,\n",
    "        log_zero_guard_value=2 ** -24,\n",
    "        pad_value=0,\n",
    "        nb_max_freq=4000,\n",
    "        mel_norm=\"slaney\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.log_zero_guard_value = log_zero_guard_value\n",
    "\n",
    "        self.win_length = n_window_size\n",
    "        self.hop_length = n_window_stride\n",
    "        self.n_fft = n_window_size\n",
    "\n",
    "        window_fn = torch.hann_window\n",
    "        window_tensor = window_fn(self.win_length, periodic=False)\n",
    "        self.register_buffer(\"window\", window_tensor)\n",
    "        self.stft = lambda x: torch.stft(\n",
    "            x,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            center=False,\n",
    "            window=self.window.to(dtype=torch.float),\n",
    "            return_complex=True,\n",
    "        )\n",
    "\n",
    "        self.nfilt = nfilt\n",
    "        self.preemph = preemph\n",
    "        highfreq = highfreq or sample_rate / 2\n",
    "\n",
    "        filterbanks = torch.tensor(\n",
    "            librosa.filters.mel(\n",
    "                sr=sample_rate, n_fft=self.n_fft, n_mels=nfilt, fmin=lowfreq, fmax=highfreq, norm=mel_norm\n",
    "            ),\n",
    "            dtype=torch.float,\n",
    "        ).unsqueeze(0)\n",
    "        self.register_buffer(\"fb\", filterbanks)\n",
    "\n",
    "        self.pad_value = pad_value\n",
    "\n",
    "        self.forward = torch.no_grad()(self.forward)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Args:\n",
    "            x (torch.Tensor): [num_samples] - input float32 waveform with values from -1 to 1\n",
    "        Returns:\n",
    "            torch.Tensor: [num_features, nfilt] fbank features.\n",
    "                num_features = (num_samples - n_window_size) / n_window_stride + 1\n",
    "        \"\"\"\n",
    "        if x.shape[0] < self.win_length:\n",
    "            raise ValueError('Not enough data')\n",
    "\n",
    "        x = torch.cat((x[0].unsqueeze(0), x[1:] - self.preemph * x[:-1]), dim=0)\n",
    "        # disable autocast to get full range of stft values\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            x = self.stft(x)\n",
    "        x = torch.view_as_real(x)\n",
    "        x = x.pow(2).sum(-1)\n",
    "        # dot with filterbank energies\n",
    "        x = torch.matmul(self.fb.to(x.dtype), x).squeeze(0)\n",
    "        x = torch.log(x + self.log_zero_guard_value)\n",
    "        return x.transpose(0, 1)\n",
    "\n",
    "\n",
    "class ChunkedStreamingFbank:\n",
    "    \"\"\"Streaming adapter for FilterbankFeatures.\n",
    "    Consumes waveform chunks and output fixed-size feature chunks.\n",
    "\n",
    "    Args:\n",
    "        chunk_size_feats (int): fixed output chunk_size\n",
    "        featurizer (FilterbankFeatures): feature calculator to wrap.\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_size_feats: int, featurizer: FilterbankFeatures):\n",
    "        self.featurizer = featurizer\n",
    "        \n",
    "        self.win_length_samples = self.featurizer.win_length\n",
    "        self.hop_length_samples = self.featurizer.hop_length\n",
    "        \n",
    "        self.chunk_size_feats = chunk_size_feats\n",
    "        \n",
    "        self.buffer_signal = None\n",
    "        self.buffer_feature_chunks = queue.Queue()\n",
    "        self.last_feature_chunk_prefix = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = None\n",
    "        self.buffer_feature_chunks = queue.Queue()\n",
    "        self.last_feature_chunk_prefix = None\n",
    "\n",
    "    def _get_valid_samples_and_feats(self, signal_length: int) -> tuple[int, int]:\n",
    "        if signal_length < self.win_length_samples:\n",
    "            return 0, 0\n",
    "        valid_feats = (signal_length - self.win_length_samples) // self.hop_length_samples + 1\n",
    "        return (valid_feats - 1) * self.hop_length_samples + self.win_length_samples, valid_feats\n",
    "    \n",
    "    def add(self, signal_chunk):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            signal (torch.Tensor): input signal chunk\n",
    "        \"\"\"\n",
    "        if self.buffer_signal is not None:\n",
    "            signal_chunk = torch.cat([self.buffer_signal, signal_chunk], axis=0)\n",
    "            self.buffer_signal = None\n",
    "\n",
    "        valid_samples, valid_feats = self._get_valid_samples_and_feats(signal_chunk.shape[0])\n",
    "        self.buffer_signal = signal_chunk[valid_feats * self.hop_length_samples:]\n",
    "\n",
    "        if valid_samples == 0:\n",
    "            return None\n",
    "    \n",
    "        signal_chunk = signal_chunk[:valid_samples]\n",
    "\n",
    "        feats = self.featurizer(signal_chunk)\n",
    "        if self.last_feature_chunk_prefix is not None:\n",
    "            feats = torch.cat((self.last_feature_chunk_prefix, feats), axis=0)\n",
    "            self.last_feature_chunk_prefix = None\n",
    "\n",
    "        idx = 0\n",
    "        while (idx + 1) * self.chunk_size_feats <= feats.shape[0]:\n",
    "            self.buffer_feature_chunks.put(feats[idx * self.chunk_size_feats:(idx + 1) * self.chunk_size_feats])\n",
    "            idx += 1\n",
    "        if idx * self.chunk_size_feats != feats.shape[0]:\n",
    "            self.last_feature_chunk_prefix = feats[idx * self.chunk_size_feats:]\n",
    "\n",
    "    def get_next_feature_chunk(self) -> torch.Tensor | None:\n",
    "        if self.buffer_feature_chunks.empty():\n",
    "            return None\n",
    "        return self.buffer_feature_chunks.get()\n",
    "\n",
    "\n",
    "class GreedyCtcDecoder(torch.nn.Module):\n",
    "    def __init__(self, enc_output_size, tokenizer_settings):\n",
    "        super().__init__()\n",
    "        self._tokenizer_settings = tokenizer_settings\n",
    "        self.decoder_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(\n",
    "                in_channels=enc_output_size,\n",
    "                out_channels=len(tokenizer_settings['token_to_piece']) + 1,\n",
    "                kernel_size=1,\n",
    "                stride=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, enc_output, enc_lengths):\n",
    "        \"\"\"Args:\n",
    "            enc_output (torch.Tensor): [batch, time, features]\n",
    "            enc_lengths (torch.Tensor): [batch]\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]: logits [batch, time, num_logits] and logits lengths [batch]\n",
    "        \"\"\"\n",
    "        enc_output = enc_output.transpose(1, 2)\n",
    "        logits = self.decoder_layers(enc_output).transpose(1, 2)\n",
    "        return logits, enc_lengths\n",
    "\n",
    "    def decode(self, logits, logits_lengths):\n",
    "        \"\"\"Args:\n",
    "            logits (torch.Tensor): [batch, time, num_logits]\n",
    "            logits_lengths (torch.Tensor): [batch]\n",
    "        Returns:\n",
    "            list[str]: [batch] of greedy hypos.\n",
    "        \"\"\"\n",
    "\n",
    "        result = []\n",
    "        logits_lengths = logits_lengths.detach().cpu().numpy()\n",
    "        for idx in range(len(logits)):\n",
    "            tokens = list(map(int, logits[idx, :logits_lengths[idx]].max(dim=-1)[1].detach().cpu().numpy()))\n",
    "            prediction = []\n",
    "            prev_token = None\n",
    "            for token in tokens:\n",
    "                if token != prev_token and token != self._tokenizer_settings['blank_idx']:\n",
    "                    prediction.append(self._tokenizer_settings['token_to_piece'][str(token)])\n",
    "                prev_token = token\n",
    "            result.append(''.join(prediction).replace(self._tokenizer_settings['special_symbol'], ' ').strip())\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d35938-594d-45c8-9fef-8ab9cd854adb",
   "metadata": {},
   "source": [
    "### 6.2 Real-life data\n",
    "\n",
    "Let's download some real model weights, tokenizer settings and audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059f208d-4568-4bae-a2cf-4a03761756fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(public_link, filename='archieve.tgz'):\n",
    "    base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
    "    final_url = base_url + urlencode(dict(public_key=public_link))\n",
    "    response = requests.get(final_url)\n",
    "    parse_href = response.json()['href']\n",
    "\n",
    "    url = parse_href\n",
    "    download_url = requests.get(url)\n",
    "    final_link = os.path.join(os.getcwd(), filename)\n",
    "    print(final_link)\n",
    "    with open(final_link, 'wb') as ff:\n",
    "        ff.write(download_url.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9bd14-d965-4b43-996b-769b073e51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_to_archive = \"https://disk.yandex.ru/d/Omgg4HryF5AWLQ\"\n",
    "download_file(link_to_archive, filename='archieve.tgz')\n",
    "!mkdir -p ../data\n",
    "!mv archieve.tgz ../data/\n",
    "!tar xzvf ../data/archieve.tgz -C ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e26513-914a-40f3-8c44-40dc1336fcca",
   "metadata": {},
   "source": [
    "### 6.3 Testing on real-life data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11ad6a-6965-473b-a56b-f79844dd843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ConformerEncoder(\n",
    "    feat_in=80,\n",
    "    n_layers=17,\n",
    "    d_model=512,\n",
    "    ff_expansion_factor=4,\n",
    "    n_heads=8,\n",
    "    subsampling_factor=8,\n",
    "    subsampling_conv_channels=256,\n",
    "    chunk_size=2,\n",
    "    left_chunks_num=70,\n",
    "    conv_kernel_size=9,\n",
    ")\n",
    "# chunk_size * subsampling_factor\n",
    "encoder_step = 2 * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447e93e-4905-413f-b349-3abfb49ddfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/week12_data/encoder_state.pkl', 'rb') as fp:\n",
    "    encoder.load_state_dict(pickle.load(fp), strict=False)\n",
    "encoder = encoder.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aab62b-dff8-464f-a048-176a47f7acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/week12_data/token.json') as fp:\n",
    "    tokenizer_settings = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c70bf4-c35e-4338-9a60-4f5c92f116e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = GreedyCtcDecoder(512, tokenizer_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f772fa-4c12-4a3b-8a8b-7dac7884aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/week12_data/decoder_state.pkl', 'rb') as fp:\n",
    "    decoder.load_state_dict(pickle.load(fp))\n",
    "decoder = decoder.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108c9002-e08b-4476-83ee-c83e2b1b1cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/week12_data/audio.wav', 'rb') as fp:\n",
    "    with wave.open(fp, 'r') as wfp:\n",
    "        pcm_data = wfp.readframes(wfp.getnframes())\n",
    "\n",
    "signal = np.frombuffer(pcm_data, dtype=np.int16)\n",
    "signal = int_singal_to_tensor(signal, device=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99a488-a7b0-4192-87b6-5468cdd648b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio('../data/week12_data/audio.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22958d-dac9-46f0-84aa-22d8139989bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = FilterbankFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de59bca9-353f-4a0a-9d03-84c967b79d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features = featurizer(signal)\n",
    "\n",
    "    # make features multiple of encoder_step\n",
    "    features = features[:(features.shape[0] // encoder_step) * encoder_step, :]\n",
    "    encoded, encoded_len = encoder(features.unsqueeze(0), torch.tensor([features.size(0)]))\n",
    "    logits, logits_len = decoder(encoded, encoded_len)\n",
    "    print(decoder.decode(logits, logits_len)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c62dfa-3c97-4199-9411-6126f6cae3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_chunker = ChunkedStreamingFbank(chunk_size_feats=encoder_step, featurizer=featurizer)\n",
    "\n",
    "# 200ms (16 = 16000 / 1000 - samples per ms)\n",
    "signal_chunk_size_samples = 200 * 16\n",
    "with torch.no_grad():\n",
    "    whole_logits = None\n",
    "    state = encoder.get_initial_state()\n",
    "    for start_idx in range(0, signal.shape[0], signal_chunk_size_samples):\n",
    "        total_processed_samples = start_idx + signal_chunk_size_samples\n",
    "        signal_chunk = signal[start_idx:total_processed_samples]\n",
    "        features_chunker.add(signal_chunk)\n",
    "        while (features_chunk := features_chunker.get_next_feature_chunk()) is not None:\n",
    "            encoder_step_output, state = encoder.streaming_forward(features_chunk.unsqueeze(0), state)\n",
    "            step_logits, _ = decoder(encoder_step_output, torch.tensor([encoder_step_output.shape[1]]))\n",
    "            if whole_logits is not None:\n",
    "                whole_logits = torch.cat([whole_logits, step_logits], axis=1)\n",
    "            else:\n",
    "                whole_logits = step_logits\n",
    "            hypo = decoder.decode(whole_logits, torch.tensor([whole_logits.shape[1]]))[0]\n",
    "            print(f\"time: {total_processed_samples / 16000:.2f}s: '{hypo}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c92b71e-c7c5-4985-8416-4ad5d78c7844",
   "metadata": {},
   "source": [
    "Let's compare naive approach (rerun model on prefix each time new chunk arrives) and streaming approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5781c1-4e23-4a9b-9d8b-540ed7a6188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "features = featurizer(signal)\n",
    "with torch.no_grad():\n",
    "    for end_idx in range(encoder_step, features.shape[0] + 1, encoder_step):\n",
    "        prefix_features = features[:end_idx]\n",
    "        encoded, encoded_len = encoder(prefix_features.unsqueeze(0), torch.tensor([prefix_features.size(0)]))\n",
    "        logits, logits_len = decoder(encoded, encoded_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba773c0-2571-4829-b707-a453e9052af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "features_chunker = ChunkedStreamingFbank(chunk_size_feats=encoder_step, featurizer=featurizer)\n",
    "\n",
    "signal_chunk_size_samples = 200 * 16\n",
    "with torch.no_grad():\n",
    "    whole_logits = None\n",
    "    state = encoder.get_initial_state()\n",
    "    for start_idx in range(0, signal.shape[0], signal_chunk_size_samples):\n",
    "        total_processed_samples = start_idx + signal_chunk_size_samples\n",
    "        signal_chunk = signal[start_idx:total_processed_samples]\n",
    "        features_chunker.add(signal_chunk)\n",
    "        while (features_chunk := features_chunker.get_next_feature_chunk()) is not None:\n",
    "            encoder_step_output, state = encoder.streaming_forward(features_chunk.unsqueeze(0), state)\n",
    "            step_logits, _ = decoder(encoder_step_output, torch.tensor([encoder_step_output.shape[1]]))\n",
    "            if whole_logits is not None:\n",
    "                whole_logits = torch.cat([whole_logits, step_logits], axis=1)\n",
    "            else:\n",
    "                whole_logits = step_logits\n",
    "            hypo = decoder.decode(whole_logits, torch.tensor([whole_logits.shape[1]]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1dfa52-10c7-4141-acd7-be56a9096ccb",
   "metadata": {},
   "source": [
    "## 7. The task\n",
    "\n",
    "1) If you've successfully filled in the gaps and now have working streaming conformer encoder &mdash; congratulations, you can collect your **5 points**\n",
    "2) If you want *bonus* **5 points** &mdash; you can modify the code to be able to apply streaming to work with batch_size > 1. Things to consider:\n",
    "* You may need to modify signatures in order to accomodate additional \"length\" argument.\n",
    "* You may need to define `combine_states` methods, in order to merge states from different examples to one batch.\n",
    "* You only need to work with input lengths divisible by encoder step.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b53242-7c3c-4aa5-b8d4-a121ef0de096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
