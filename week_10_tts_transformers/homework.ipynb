{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Homework 09_codec_models [15 points]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First, let's download the required files and packages"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# !wget https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_09_tts_tranformer/data.py\n", "# !wget https://raw.githubusercontent.com/yandexdataschool/speech_course/main/week_09_tts_tranformer/model.py"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# !pip install deep-phonemizer librosa matplotlib numpy pyannote.audio pyloudnorm torch torchaudio tqdm"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "from urllib.parse import urlencode\n", "\n", "import requests\n", "\n", "def download_file(public_link):\n", "    base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n", "    final_url = base_url + urlencode(dict(public_key=public_link))\n", "    response = requests.get(final_url)\n", "    parse_href = response.json()['href']\n", "\n", "    url = parse_href\n", "    start_filename = url.find('filename=')\n", "    end_filename = url[start_filename:].find('&')\n", "    end_name = start_filename + end_filename\n", "    filename = url[start_filename:end_name][9:]\n", "    download_url = requests.get(url)\n", "    final_link = os.path.join(os.getcwd(), filename)\n", "    with open(final_link, 'wb') as ff:\n", "        ff.write(download_url.content)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["### To download the file uncomment the following line\n", "\n", "# link_to_archive = \"https://disk.yandex.ru/d/XvDaWCWch6hWTw\"\n", "# download_file(link_to_archive)\n", "# !unzip lingware.zip\n", "# !rm lingware.zip\n", "# !mkdir -p ../data/09_tts_transformers\n", "# !mv lingware ../data/09_tts_transformers"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lingware - is a folder with all of the stuff, needed for synthesis: tokenizer weights, g2p dictionaries and so on..."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 0. Transformer\n", "\n", "In this homework we will download a pretrained transformer and write inference for the model.\n", "\n", "First let's take a look on the required tools:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pathlib import Path\n", "\n", "import torch\n", "import torch.nn.functional as F\n", "import torchaudio\n", "from IPython.display import Audio, display"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%load_ext autoreload\n", "%autoreload 2\n", "\n", "from model import *\n", "from data import *"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["device = torch.device(\"cuda:7\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["### Paths\n", "lingware_folder = Path(\"../data/09_tts_transformers/lingware\")\n", "\n", "ckpt_path = lingware_folder / \"ckpt\"\n", "\n", "codec_model_path = lingware_folder / \"codec\" / \"ckpt\"\n", "codec_config_path = lingware_folder / \"codec\" / \"config.json\"\n", "\n", "phonemizer_path = lingware_folder / \"phonemizer_en_us.pt\"\n", "\n", "dataset_url = \"dev-clean\"\n", "data_path = Path(\"../data/09_tts_transformers\")\n", "data_path.mkdir(exist_ok=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's download data for playing with our model. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tts_dataset = torchaudio.datasets.LIBRITTS(root=data_path, url=dataset_url, download=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's create a **phonemizer**. It will be a simple phonemizer that will use the `deep-phonemizer` library. It has 2 methods:\n", "- `phonemize` - that will take a text and return a phonemized version of it, as a sequence of phonemes.\n", "- `tokenize` - that will take a text, phonemize it and return a list of indices, assigned to each phoneme."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["phonemizer = Phonemizer(phonemizer_path)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's create a **bioembedding model**. We will condition our model on its outputs, to mimic the speaker in synthesis.\n", "\n", "It has method `__call__` that takes a waveform and returns the embedding of the speaker from this waveform."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["bioemb_model = BioembModel(device=device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's create a codec model. It can convert wav to codecs and back: \n", "- `encode` - transforms waveform of size `[Time]` to a sequence of codecs of size `[short_time, 4]`\n", "- `decode` - transforms sequence of codecs of size `[short_time, 4]` back to waveform.\n", "\n", "Note, that:\n", "- `160` - index of end_token\n", "- `161` - index of start_token\n", "- `162` - index of pad_token"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["codec_model = CodecApplier(\n", "    config_path=codec_config_path,\n", "    ckpt_path=codec_model_path,\n", "    sample_rate=16000,\n", "    device=device,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's assemble everything in one dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["infer_dataset = CodecsDataset(\n", "    dataset=tts_dataset,\n", "    phonemizer=phonemizer,\n", "    bioemb_model=bioemb_model,\n", "    codec_model=codec_model,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Model\n", "\n", "We will work with model, which mimics model from [mqtts paper](https://arxiv.org/abs/2302.04215). It consist of encoder, decoder and sub-decoder. \n", "- `Encoder` consists of several Self-Attention layers and Feed-Forward layers. It takes a sequence of embeddings of phonemes and return the encoded representation of the sequence.\n", "- `Decoder` consists of several Self-Attention layers and Feed-Forward layers. It uses cross-attention to watch on embeddings from encoder. It takes a sequence of codecs, for each layer creates an embeddings, concatenates them and uses as an input. Then for each codec it predicts an embedding, which is further used by sub-decoder to predict next codec.\n", "- `SubDecoder` - decoder-only transformer, which gets an embedding from decoder and predicts 4 tokens. It makes 4 steps of autoregression to predict 4 tokens of the codec.\n", "\n", "This model was discussed on the lecture, you can refer the recording for better understanding of what is happening.\n", "\n", "This model was trained on LibriTTS dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ckpt = torch.load(ckpt_path, map_location=torch.device(\"cpu\"))\n", "\n", "model = TTSTransformer(\n", "    n_phonemes=49,\n", "    n_codes=163,\n", "    n_codebooks=4,\n", ")\n", "\n", "model.load_state_dict(ckpt)\n", "model = model.eval().to(device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Inference function\n", "\n", "This function iterates over the input dataset `n_samples` times. For each sample predicts tokens in a teacher-forcing regime. Then decodes it with a codec_model back to waveform and plays it.\n", "\n", "Here we use teacher forcing, which means we use ground-truth codecs to predict the next token. This is not the best way to generate audio, but it is the simplest one for sanity check."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def infer_teacher_forcing(model, dataset, codec_model, n_samples=1, sampling_fn=lambda x: x.argmax(dim=-1)):\n", "    \"\"\"\n", "    model: TTSTranformer model, which has `forward` method. It gets phoneme_ids, speaker_embedding and codecs sequence and predicts logits for the next codec.\n", "    dataset: Iterator over the CodecsDataset. On each iteration it shounld return tuple with (phonemes, phoneme_ids, codecs, bioemb)\n", "    codec_model: Codec model, needed to decode codecs sequence back to the waveform\n", "    n_samples: number of samples from the dataset which will be inferred\n", "    sampling_fn: function which takes logits and returns the predicted labels. By default it returns the argmax of the logits\n", "    \"\"\"\n", "    device = model.parameters().__next__().device\n", "\n", "    for idx, (phonemes, phoneme_ids, codecs, bioemb) in zip(range(n_samples), dataset):\n", "        phoneme_ids = torch.tensor([phoneme_ids], device=device)\n", "        codecs = torch.tensor([codecs], device=device)\n", "        bioemb = torch.tensor([bioemb], device=device)\n", "\n", "        phones_mask = torch.ones_like(phoneme_ids, dtype=torch.bool)\n", "        codes_mask = torch.ones(codecs.shape[:2], dtype=torch.bool, device=device)\n", "\n", "        prediction = model(\n", "            phones=phoneme_ids,\n", "            phones_mask=phones_mask, # [B, l]\n", "            codes=codecs, # [B, L, N]\n", "            codes_mask=codes_mask, # [B, L]\n", "            speaker_embs=bioemb, # [B, d]\n", "        )\n", "        pred_labels = sampling_fn(prediction)\n", "\n", "        # [:, 1:, :] is needed to remove the start tokens\n", "        gt_wav = codec_model.decode(codecs[:, 1:, :], bioemb)\n", "\n", "        # Clamp is needed to remove the eos, bos or padding token if they emerge in the prediction\n", "        pred_labels = pred_labels.clamp(min=0, max=159)\n", "        synthesized_wav = codec_model.decode(pred_labels, bioemb)\n", "\n", "        print(f\"Phonemes: {'_'.join(phonemes)}\")\n", "        print(f\"Ground truth\")\n", "        display(Audio(gt_wav, rate=16000))\n", "        print(f\"Synthesized\")\n", "        display(Audio(synthesized_wav, rate=16000))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["infer_teacher_forcing(model, dataset=infer_dataset, codec_model=codec_model, n_samples=5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1. Sampling functions [3 points]\n", "During inference our model predict logits, and we need to sample from these logits to get the next token. We will use several functions to do that.\n", "- `ArgmaxSampling` - dedicated for greedy decoding, it returns the token with the highest logit (aka probability).\n", "- `MultinomialSampling` - samples indices of codecs from multinomial distribution with probabilities `softmax (logits / temperature)`.\n", "- `TopKSampling` - takes only tok-k logits with highest probabilities and samples from them, using multinomial sampling. \n", "\n", "Each function gets FloatTensor of size [\\*, logits], and returns LongTensor of size [\\*], where \\* - is the arbitrary number of dimensions.\n", "\n", "These function from torch can be useful:\n", "- [torch.multinomial](https://pytorch.org/docs/stable/generated/torch.multinomial.html)\n", "- [torch.topk](https://pytorch.org/docs/stable/generated/torch.topk.html)\n", "- [torch.gather](https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# TODO: implement the following functions\n", "\n", "class ArgmaxSampling:\n", "    def __init__(self):\n", "        pass\n", "\n", "    def __call__(self, logits):\n", "        return torch.argmax(logits, dim=-1)\n", "\n", "\n", "class MultinomialSampling:\n", "    def __init__(self, temperature=1.0):\n", "        self.temperature = temperature\n", "\n", "    def __call__(self, logits):\n", "                # Your code here\n", "        raise NotImplementedError(\"TODO: assignment\")\n", "        # ^^^^^^^^^^^^^^\n", "\n", "\n", "\n", "class TopKSampling:\n", "    def __init__(self, k, temperature=1.0):\n", "        self.k = k\n", "        self.temperature = temperature\n", "\n", "    def __call__(self, logits):\n", "                # Your code here\n", "        raise NotImplementedError(\"TODO: assignment\")\n", "        # ^^^^^^^^^^^^^^\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["infer_teacher_forcing(\n", "    model,\n", "    dataset=infer_dataset,\n", "    codec_model=codec_model,\n", "    n_samples=1,\n", "    sampling_fn=MultinomialSampling(temperature=1.0),\n", ")\n", "\n", "infer_teacher_forcing(\n", "    model,\n", "    dataset=infer_dataset,\n", "    codec_model=codec_model,\n", "    n_samples=1,\n", "    sampling_fn=TopKSampling(k=3, temperature=1.0),\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's listen what we've got and how hyperparameters influence the sampling. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sampling_functions_to_test = [\n", "    (MultinomialSampling, {\"temperature\": 1.}),\n", "    (MultinomialSampling, {\"temperature\": 3.}),\n", "    (MultinomialSampling, {\"temperature\": 0.5}),\n", "    (TopKSampling, {\"k\": 7, \"temperature\": 1.}),\n", "    (TopKSampling, {\"k\": 20, \"temperature\": 1.}),\n", "    (TopKSampling, {\"k\": 3, \"temperature\": 1.}),\n", "]\n", "\n", "for sampling_class, sampling_kwargs in sampling_functions_to_test:\n", "    sampling_fn = sampling_class(**sampling_kwargs)\n", "    print(f\"======== Sampling function: {sampling_class.__name__} with kwargs {sampling_kwargs} ========\")\n", "    infer_teacher_forcing(\n", "        model,\n", "        dataset=infer_dataset,\n", "        codec_model=codec_model,\n", "        n_samples=2,\n", "        sampling_fn=sampling_fn,\n", "    )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Assignment:\n", "\n", "What are your notions about these different sampling methods ? What is the difference between them ? What are the advantages and disadvantages of each ? Which is the preferable one ?\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["TODO"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2. Autoregressive inference [12 points]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Autoregressive sampling function. It creates, exactly the same, as `infer_teacher_forcing`, but uses `model.autoregressive_sampling` instead of `model.forward`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def infer_autoregressive(model, dataset, codec_model, n_samples=5, sampling_fn=lambda x: x.argmax(dim=-1)):\n", "    device = model.parameters().__next__().device\n", "\n", "    for idx, (phonemes, phoneme_ids, codecs, bioemb) in zip(range(n_samples), dataset):\n", "        phoneme_ids = torch.tensor([phoneme_ids], device=device)\n", "        codecs = torch.tensor([codecs], device=device)\n", "        bioemb = torch.tensor([bioemb], device=device)\n", "\n", "\n", "        # This function is not supposed to use codecs for prediction\n", "        pred_labels = model.autoregressive_sampling(\n", "            phones=phoneme_ids,\n", "            speaker_embs=bioemb,\n", "            sampling_fn=sampling_fn,\n", "        )\n", "\n", "        print(f\"{codecs.shape=}\")\n", "        # [:, 1:, :] is needed to remove the start tokens\n", "        gt_wav = codec_model.decode(codecs[:, 1:, :], bioemb)\n", "\n", "        # Clamp is needed to remove the eos, bos or padding token if they emerge in the prediction\n", "        pred_labels = pred_labels.clamp(min=0, max=159)\n", "        synthesized_wav = codec_model.decode(pred_labels, bioemb)\n", "\n", "        print(f\"Phonemes: {'_'.join(phonemes)}\")\n", "        print(f\"Ground truth\")\n", "        display(Audio(gt_wav, rate=16000))\n", "        print(f\"Synthesized\")\n", "        display(Audio(synthesized_wav, rate=16000))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Assignment:\n", "\n", "Go to the file model.py and implement SubDecoder.autoregressive_sampling and TTSTranformer.autoregressive_sampling methods.\n", "\n", "Notes:\n", "- The model is allmost exact copy of MQTTS model from the lecture. Except that it doesn't use trick with a window in encoder-decoder attention during inference. \n", "- You better not modify the `__init__` and `forward` methods of each model. Because the behaviour of the model can change.\n", "- During autoregressive sampling, the model should not use the ground truth codec sequence. Instead, the model should generate the target sequence one token at a time. Starting with a vector of 4 start_tokens.\n", "- You will need to figure out how the model works, so don't hesitate to print the shapes of the tensors you are working with.\n", "- You will need to use SubDecoder.forward and TTSTransformer.forward methods multiple times. But do not modify them.\n", "- The synthesis has two conditions that end the generation of the target sequence:\n", "    - The target sequence is longer than the maximum length.\n", "    - The target sequence contains at least one end token."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["infer_autoregressive(model, dataset=infer_dataset, codec_model=codec_model, sampling_fn=MultinomialSampling())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's say, that you have implemented those methods successefully if the model \n", "geneates comprehensible speech."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now you can play with different types and hyperparameters of sampling in autoregressive synthesis."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Write down:\n", "- What hyperparameters you have played with ?\n", "- Compare them.\n", "- How do they influece autoregressive synthesis ?\n", "- What is their effect on audio-quality, intonation and speaker-similarity ? \n", "- What are the optimal hyperparameters ?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["TODO:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Rules for commiting homework\n", "\n", "- Clear your code from debugging `print`-s\n", "- You need to commit 2 files: `model.py` and `homework.ipynb` and some samples with your synthesis\n", "- The resulted notebook includes some audios and weights a lot of memory. YOu can use this tutorial to share the notebook: https://gist.github.com/yashika51/58d2b6d8d1048a1d0a9ea5949a8aa7f6 . Or you can also try using collab."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.16"}}, "nbformat": 4, "nbformat_minor": 4}