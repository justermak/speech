{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccbb917e",
   "metadata": {},
   "source": [
    "# Homework description\n",
    "\n",
    "In this assignment we are going to train a neural network for noise reduction.\n",
    "\n",
    "The total cost of this assignment is 15 pts.\n",
    "\n",
    "**Get ready, this assignment is huge!**\n",
    "\n",
    "## Plan:\n",
    "0. Datasets\n",
    "1. Volume normalization, gain, RMS: everything we need to mix signal with noise [2 points]\n",
    "2. Room impulse response (RIR): what we need to simulat acoustics and perform partial dereverberation [1 point]\n",
    "3. On-the-fly data generation [5 points]\n",
    "4. Neural network architecture [3 points]\n",
    "5. Loss function [2 point]\n",
    "6. Train Loop [2 points]\n",
    "\n",
    "## A homework submission should include:\n",
    "1. filled notebook\n",
    "2. tensorboard logs\n",
    "3. 5 examples of input-output files from the trained model in .wav format\n",
    "\n",
    "## Note!\n",
    "If submission requirements are not satisfied we keep the right not to accept the work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45cf343",
   "metadata": {},
   "source": [
    "# 0. Datasets\n",
    "\n",
    "We are going to use clean speech and room impluse responses from DNS Challenge dataset.\n",
    "For speech it is random subsample, for RIR we shall use the full smallroom partition to avoid extreme reverberation levels.\n",
    "\n",
    "Originally DNS Challenge data comes in 48 kHz sample rate. We [down-sampled](https://librosa.org/doc/0.11.0/generated/librosa.resample.html#librosa-resample) it to 16 kHz in advance.\n",
    "\n",
    "For noise we are going to use Musan dataset. Why not DNS Challenge data? Training progress will be seen faster with Musan.\n",
    "\n",
    "Let's download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3530fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "from zipfile import ZipFile\n",
    "\n",
    "base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
    "public_key = 'https://disk.yandex.ru/d/ECHrgBGJrrGQqw'\n",
    "\n",
    "final_url = base_url + urlencode(dict(public_key=public_key))\n",
    "response = requests.get(final_url)\n",
    "download_url = response.json()['href']\n",
    "response = requests.get(download_url)\n",
    "\n",
    "path_to_dataset = 'data'    # Choose any appropriate local path\n",
    "\n",
    "zipfile = ZipFile(BytesIO(response.content))\n",
    "zipfile.extractall(path=path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3550a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ROOT_DATA = os.path.join(path_to_dataset, \"homework_1_16kHz\")\n",
    "\n",
    "DATA_PATHS = {\n",
    "    \"speech\": os.path.join(ROOT_DATA, \"clean_train\"),\n",
    "    \"noise\": os.path.join(ROOT_DATA, \"musan/noise\"),\n",
    "    \"rir\": os.path.join(ROOT_DATA, \"impulse_responses_all/SLR26/simulated_rirs_48k/smallroom\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772796e5",
   "metadata": {},
   "source": [
    "How many audio files do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda20ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "\n",
    "def list_wavs_in_folder_recursively(path: str) -> list[str]:\n",
    "    return sorted(glob(os.path.join(path, \"**\", \"*.wav\"), recursive=True))\n",
    "\n",
    "for key, folder in DATA_PATHS.items():\n",
    "    paths = list_wavs_in_folder_recursively(folder)\n",
    "    print(f\"{key}: {len(paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f5887",
   "metadata": {},
   "source": [
    "The full dataset contains about 1M speech utterances, 64k noise samples and 63k impulse responses. The dataset is available in 48 kHz samplerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f853c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import librosa  # to plot mel-spectrograms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import scipy.signal as sig\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "SR = 16_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4410818c",
   "metadata": {},
   "source": [
    "This is a utility function which we shall use to view spectrograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_spec_for_plot(waveform):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=waveform, sr=SR)\n",
    "    min_val = 1e-10\n",
    "    mel_spec = np.clip(mel_spec, min_val, None)\n",
    "    mel_spec[-1, -1] = min_val\n",
    "    mel_spec_db = 10 * np.log10(mel_spec)\n",
    "    return np.flip(mel_spec_db, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83186739",
   "metadata": {},
   "source": [
    "**Let's select sample files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c68e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_path = \"read_speech/book_00007_chp_0008_reader_01326_64_seg_2.wav\"\n",
    "SAMPLE_SIGNAL, _sr = sf.read(os.path.join(DATA_PATHS[\"speech\"], rel_path))\n",
    "\n",
    "rel_path = \"free-sound/noise-free-sound-0001.wav\"\n",
    "SAMPLE_NOISE, _sr = sf.read(os.path.join(DATA_PATHS[\"noise\"], rel_path))\n",
    "\n",
    "SAMPLE_RIR, _sr = sf.read(os.path.join(\n",
    "    ROOT_DATA,\n",
    "    \"impulse_responses_all/SLR28/RIRS_NOISES/real_rirs_isotropic_noises/air_type1_air_binaural_office_0_1_1ch.wav\",\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce9093",
   "metadata": {},
   "source": [
    "# 1. Volume normalization, gain, RMS: everything we need to mix signal with noise [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd65e0",
   "metadata": {},
   "source": [
    "## RMS-full-scale\n",
    "\n",
    "Let's suppose we have an audio signal $x=(x[0], x[1], ..., x[T-1])$.\n",
    "\n",
    "A basic measure of its loundness would be its L2-norm. It is also referred to as RMS (root-mean-square):\n",
    "$$\\text{rms}_{\\text{raw}}(x) = ||x|| = ||x||_2 = \\sqrt{\\frac{1}{T}\\sum_{t=0}^{T-1} x[t]^2}$$\n",
    "\n",
    "It is convenient to express RMS in decibels. Decibels involve logarithm computation and are only applicable to dimensionless physical quantities, typically to ratios.\n",
    "\n",
    "So we need to define a reference value to normalize by. As signals are represented with floating-point values ranged between -1 and 1, 2 options are typically adopted:\n",
    "\n",
    "$$\\text{rms}_{\\text{ref}, \\sin} = \\int_{0}^{2\\pi} sin^2(t)dt = 0.5$$\n",
    "which corresponds to rms of a sine wave , or\n",
    "$$\\text{rms}_{\\text{ref, square}} = 1$$\n",
    "the latter corresponds to rms of a [square wave](https://en.wikipedia.org/wiki/Square_wave).\n",
    "\n",
    "Both options are used, leading to confusion in the industry.\n",
    "\n",
    "In this assignment **let's use:**\n",
    "$$\\text{rms}_{\\text{ref}} = \\text{rms}_{\\text{ref, square}} = 1$$\n",
    "\n",
    "Thus we get:\n",
    "\n",
    "$$\\text{rms}_\\text{dB}(x) = 20\\log_{10}\\frac{||x||}{\\text{rms}_{\\text{ref, square}}} = 20\\log_{10}||x|| = 10\\log_{10}\\frac{1}{T}\\sum_{t=0}^{T-1}x[t]^2$$\n",
    "\n",
    "**The latter is called RMS-full-scale (or RMS-fs)**, i.e. RMS relative to full scale of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e752657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mean_square(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes mean-square of x\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Your code\")\n",
    "\n",
    "\n",
    "def power_to_db(x: float) -> float:\n",
    "    \"\"\"\n",
    "    Computes 10log10(x)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Your code\")\n",
    "\n",
    "\n",
    "def eval_rms_db(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes rms-fs of x\n",
    "    \"\"\"\n",
    "    rms_square_raw = # your code\n",
    "    rms_db = # your code\n",
    "    return rms_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5375400",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-12\n",
    "\n",
    "assert abs(power_to_db(0.01) + 20) < eps\n",
    "assert abs(power_to_db(0.1) + 10) < eps\n",
    "assert abs(power_to_db(1) - 0) < eps\n",
    "assert abs(power_to_db(10) - 10) < eps\n",
    "assert abs(power_to_db(100) - 20) < eps\n",
    "\n",
    "assert abs(eval_rms_db(np.ones(999) * 0.1) + 20) < eps\n",
    "assert abs(eval_rms_db(np.ones(999) * 1) - 0) < eps\n",
    "assert abs(eval_rms_db(np.ones(531) * 10) - 20) < eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f2916b",
   "metadata": {},
   "source": [
    "## Gain, normalization\n",
    "\n",
    "When a signal $x$ is multiplied by a scalar factor of $\\alpha \\geq 0$, it corresponds to addition in the world of decibels:\n",
    "$$\\text{rms}_\\text{dB}(\\alpha x) = 20 \\log_{10} ||\\alpha x|| = 20\\log_{10}\\alpha + 20\\log_{10} ||x|| = 20\\log_{10}\\alpha + \\text{rms}_\\text{dB}(x)$$\n",
    "\n",
    "The multiplication of $x$ by $\\alpha \\geq 0$ is often referred to as **gain by $\\boldsymbol{G}$ dB**, where $G = 20\\log_{10}\\alpha$, which can be both positive or negative (or even infinite negative if $\\alpha=0$).\n",
    "\n",
    "The inverse relationship can be inferred to express the scalar factor from the gain in decibels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79065005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_to_mult(gain_db: float) -> float:\n",
    "    \"\"\"\n",
    "    Finds the positive scalar factor which corresponds to given gain_db\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Your code\")\n",
    "    \n",
    "    \n",
    "def mult_to_gain(mult: float) -> float:\n",
    "    \"\"\"\n",
    "    Finds the gain in dB from positive scalar factor\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c973ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-12\n",
    "\n",
    "assert abs(gain_to_mult(-20) - 0.1) < eps\n",
    "assert abs(gain_to_mult(0) - 1) < eps\n",
    "assert abs(gain_to_mult(20) - 10) < eps\n",
    "\n",
    "for mult in [1., 0.265, 10.5]:\n",
    "    gain_db = mult_to_gain(mult)\n",
    "    mult_power = mult ** 2  # if x is multiplied by mult, x ** 2 is multiplied by mult ** 2\n",
    "    gain_db_from_power = power_to_db(mult_power)\n",
    "    assert abs(gain_db_from_power - gain_db) < eps\n",
    "    mult_restored = gain_to_mult(gain_db)\n",
    "    assert abs(mult - mult_restored) < eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234f5a4",
   "metadata": {},
   "source": [
    "Now we know how to apply gain in decibels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f4e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gain(x: np.ndarray, gain_db: float) -> np.ndarray:\n",
    "    mult = # your code\n",
    "    result = # your code\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones(10)\n",
    "\n",
    "assert np.allclose(apply_gain(x, -20), x / 10)\n",
    "assert np.allclose(apply_gain(x, 0), x)\n",
    "assert np.allclose(apply_gain(x, 20), x * 10)\n",
    "\n",
    "\n",
    "for _ in range(100):\n",
    "    x = np.random.uniform(-1, 1, 16_000)\n",
    "    gain = np.random.uniform(-20, 20)\n",
    "    rms_before = eval_rms_db(x)\n",
    "    rms_expected = rms_before + gain\n",
    "    \n",
    "    gained = apply_gain(x, gain)\n",
    "    rms_after = eval_rms_db(gained)\n",
    "    \n",
    "    assert abs(rms_after - rms_expected) < 1e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dcc8dc",
   "metadata": {},
   "source": [
    "Let's implement a function which will normalize an input signal to a desired level of $\\text{rms}_\\text{dB}$.\n",
    "\n",
    "How should it work?\n",
    "\n",
    "1. Calculate $\\text{rms}_\\text{dB}(\\text{signal})$\n",
    "2. Calculate gain in decibels: $g = \\text{rms}_\\text{dB, target} - \\text{rms}_\\text{dB}(\\text{signal})$\n",
    "3. Translate the gain to the linear scale (`db_to_linear` could be a better name for the `gain_to_mult` function)\n",
    "4. Multiply input signal by the linear-scale gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246907b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_mult(x: np.ndarray, target_rms_db: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculates a mult such that rms_db(x * mult) == target_rms_db\n",
    "    \"\"\"\n",
    "    rms_db_cur = # your code\n",
    "    gain_db = # your code\n",
    "    mult = # your code\n",
    "    return mult\n",
    "\n",
    "\n",
    "def normalize_to_rms(x: np.ndarray, target_rms_db: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes signal x to target_rms_db\n",
    "    \"\"\"\n",
    "\n",
    "    mult = # your code\n",
    "    return # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc1ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    x = np.random.uniform(-1, 1, 16_000)\n",
    "    target_rms_db = np.random.uniform(-20, 20)\n",
    "    normalized = normalize_to_rms(x, target_rms_db)\n",
    "    rms_after_normalization = eval_rms_db(normalized)\n",
    "    assert abs(target_rms_db - rms_after_normalization) < 1e-12\n",
    "    \n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea09b1",
   "metadata": {},
   "source": [
    "**Let's play with it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ebb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.copy(SAMPLE_SIGNAL)\n",
    "\n",
    "gain_db = -10\n",
    "x_gained = apply_gain(x, gain_db)\n",
    "x_normalized = normalize_to_rms(x, -20)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(x_normalized, label=\"rms: -20 dB\")\n",
    "ax.plot(x, label=\"raw\")\n",
    "ax.plot(x_gained, label=f\"gained by {gain_db} dB\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e5e60",
   "metadata": {},
   "source": [
    "### SNR\n",
    "\n",
    "SNR (signal-to-noise ratio) is expressed in decibels and is defined as:\n",
    "\n",
    "$$\\text{SNR} = 10\\log_{10}\\frac{||\\text{signal}||^2}{||\\text{noise}||^2} = 10\\log_{10}||\\text{signal}||^2 - 10\\log_{10}||\\text{noise}||^2 = \\text{rms}_{\\text{dB}}(\\text{signal}) - \\text{rms}_{\\text{dB}}(\\text{noise})$$\n",
    "\n",
    "Also, **SNR can be used as a quality metrics** or even a loss function for gradient descent.\n",
    "\n",
    "Given a ground truth signal $y$ and its estimate $\\hat y$, we define noise as $\\hat y - y$. Slightly abusing notation we get:\n",
    "\n",
    "$$\\text{SNR}(\\hat y, y) = 10 \\log_{10} \\frac{||y||^2}{||\\hat y - y||^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_snr(estimate: np.ndarray, signal: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    evaluates SNR as a quality metrics\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Your code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ad597",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "\n",
    "    signal = np.random.uniform(-1, 1, 16_000)\n",
    "    noise = np.random.uniform(-1, 1, 16_000) * np.random.uniform(0.3, 2)\n",
    "    mixture = signal + noise\n",
    "    snr = eval_rms_db(signal) - eval_rms_db(noise)\n",
    "    snr_est = eval_snr(mixture, signal)\n",
    "    assert abs(snr - snr_est) < 1e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffebcc7",
   "metadata": {},
   "source": [
    "Now we have everything to generate a mixture of speech and noise with defined signal loudness (RMS) and SNR:\n",
    "\n",
    "1. Normalize signal to $\\text{rms}_\\text{target, signal}$\n",
    "2. Noramlize noise to $\\text{rms}_\\text{target, noise} = \\text{rms}_\\text{target, signal} - \\text{SNR}$\n",
    "3. Add noise to signal. What if shapes don't match? Let's just assume they match and enforce it outside the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfd7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_speech_with_noise(signal, noise, rms_signal, snr):\n",
    "    signal_normalized = # your code\n",
    "    rms_noise_target = # your code\n",
    "    noise_normalized = # your code\n",
    "    mixture = signal + noise\n",
    "    return mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822dae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    signal = np.random.uniform(-1, 1, 16_000)\n",
    "    noise = np.random.uniform(-1, 1, 16_000) * np.random.uniform(0.3, 2)\n",
    "    rms_signal = np.random.uniform(-20, 20)\n",
    "    snr = np.random.uniform(-20, 20)\n",
    "    mixture = mix_speech_with_noise(signal, noise, rms_signal, snr)\n",
    "    \n",
    "    signal_gained = normalize_to_rms(signal, rms_signal)\n",
    "    snr_est = eval_snr(mixture, signal_gained)\n",
    "\n",
    "    assert abs(snr - snr_est) < 1e-12\n",
    "    \n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33401638",
   "metadata": {},
   "source": [
    "Let's mix speech signal with noise on different SNR's and look at the spectrograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb751a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_vals = [-10, 10]\n",
    "_, axes = plt.subplots(ncols=2, figsize=(9, 4), sharex=True, sharey=True)\n",
    "for snr_idx, snr in enumerate(snr_vals):\n",
    "    min_len = min(len(SAMPLE_SIGNAL), len(SAMPLE_NOISE))\n",
    "    signal = SAMPLE_SIGNAL[:min_len]\n",
    "    noise = SAMPLE_NOISE[:min_len]\n",
    "\n",
    "    mixture = mix_speech_with_noise(signal, noise, -30, snr)\n",
    "\n",
    "    spec = build_spec_for_plot(mixture)\n",
    "    ax = axes[snr_idx]\n",
    "    ax.set_title(f\"SNR: {snr} dB\")\n",
    "    ax.imshow(spec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a6d8f",
   "metadata": {},
   "source": [
    "# 2. Room impulse response (RIR): what we need to simulat acoustics and perform partial dereverberation [1 point]\n",
    "\n",
    "The common approach to simulate acoustics is convolving signal with room impulse response (RIR).\n",
    "\n",
    "It follows from the linear acoustic model (which is accurate enough to be used in practice) and the assumption of time-invariance (i.e. that room acoustics does not change over time or it changes slowly).\n",
    "\n",
    "For input signal $x$ and RIR $r$:\n",
    "\n",
    "$$x_{\\text{reverberated}} = x * r$$\n",
    "\n",
    "A RIR is defined as the reverberated version of the unit impulse, i.e. the (1, 0, 0, 0, ...) signal.\n",
    "\n",
    "An RIR can be listened to and it sounds like a click."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390a90c",
   "metadata": {},
   "source": [
    "**Let's take a look at an impulse response.**\n",
    "\n",
    "This will be a real impulse response from a relatively highly reverberant environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d4d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rir = np.copy(SAMPLE_RIR)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(rir)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37716d76",
   "metadata": {},
   "source": [
    "**This is how a RIR is convolved with a signal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4000d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved = sig.convolve(SAMPLE_SIGNAL, SAMPLE_RIR, mode=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=2, figsize=(16, 7), sharex=True, sharey=True)\n",
    "for ax, (name, data) in zip(axes, [\n",
    "    (\"raw\", SAMPLE_SIGNAL),\n",
    "    (\"convolved\", convolved),\n",
    "]):\n",
    "    ax.set_title(name)\n",
    "    ax.plot(data)\n",
    "    ax.grid()\n",
    "plt.show()\n",
    "\n",
    "_, axes = plt.subplots(nrows=2, figsize=(16, 7), sharex=True, sharey=True)\n",
    "for ax, (name, data) in zip(axes, [\n",
    "    (\"raw\", SAMPLE_SIGNAL),\n",
    "    (\"convolved\", convolved),\n",
    "]):\n",
    "    spec = build_spec_for_plot(data)\n",
    "    ax.set_title(name)\n",
    "    ax.imshow(spec)\n",
    "    ax.set_aspect(\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855cd8d0",
   "metadata": {},
   "source": [
    "**Note the shapes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(convolved) == len(SAMPLE_SIGNAL) + len(SAMPLE_RIR) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293346d0",
   "metadata": {},
   "source": [
    "The output file is longer, and as it can be seen from the spectrum, the appended length mainly consists of the reverberation tail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb22c8",
   "metadata": {},
   "source": [
    "**Let's take a closer look what a RIR looks like**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a6ecf",
   "metadata": {},
   "source": [
    "**These are helper functinons** which evaluate windowed power of a signal (RIR) and plot it in the dB scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1002791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_power(rir, win_size=160):\n",
    "    win = np.hanning(win_size)\n",
    "    win /= win.sum()\n",
    "    rir_sq = np.square(rir)\n",
    "    win_power = sig.convolve(rir_sq, win, mode=\"valid\")\n",
    "    return win_power\n",
    "\n",
    "\n",
    "def plot_win_power_db(win_power_db, ax):\n",
    "    lines = ax.plot(win_power_db)\n",
    "    return lines[0]\n",
    "\n",
    "\n",
    "def plot_rir_for_rt60(rir, ax):\n",
    "    win_power = get_win_power(rir)\n",
    "    win_power_db = 10 * np.log10(win_power)\n",
    "    line = plot_win_power_db(win_power_db, ax)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cfeb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "win_power = get_win_power(SAMPLE_RIR)\n",
    "win_power_db = 10 * np.log10(win_power)\n",
    "plot_win_power_db(win_power_db, ax)\n",
    "ax.set_title(\"RIR power decay\")\n",
    "ax.set_ylabel(\"Power (db-fs)\")\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2e880",
   "metadata": {},
   "source": [
    "**We can oberve the following pattern:**\n",
    "\n",
    "First the power drops abruptly and then it decays by a linear pattern in the log scale.\n",
    "\n",
    "The slope of the linear fit defines the $\\boldsymbol{rt_{60}}$ property of a RIR (and even a room).\n",
    "\n",
    "$\\boldsymbol{rt_{60}}$ (reverb time 60) is the time in which the linear fit decays by 60 dB. Measured in seconds.\n",
    "\n",
    "Why 60 dB? It is the difference between the loudest and the quietest volumes in a symphonic orchestra.\n",
    "\n",
    "**For the curious:** [More about rt60](https://svantek.com/academy/rt60-reverberation-time/), they measure it directly, not from an RIR.\n",
    "\n",
    "**Let's fit a linear regression estimator:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1824302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "\n",
    "linear_trend_start = 500\n",
    "linear_trend_end = 8000\n",
    "\n",
    "x = np.arange(linear_trend_end)[linear_trend_start: linear_trend_end]\n",
    "y = win_power_db[linear_trend_start: linear_trend_end]\n",
    "\n",
    "linear_regression.fit(x[:, None], y);\n",
    "linear_fit = linear_regression.predict(x[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39dd586",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "\n",
    "ax.set_title(\"RIR power decay\")\n",
    "ax.set_ylabel(\"Power (db-fs)\")\n",
    "plot_rir_for_rt60(SAMPLE_RIR, ax)\n",
    "ax.plot(x, linear_fit)\n",
    "ax.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e517be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = linear_regression.coef_.item()\n",
    "# intercept = linear_regression.intercept_\n",
    "rt_60_sec = # your code: use coef to evaluate rt60\n",
    "\n",
    "print(f\"RT60 of our RIR is {rt_60_sec} seconds\")\n",
    "assert abs(rt_60_sec - 0.7) < 0.02, rt_60_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1cc738",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rt_60_sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3798dfe",
   "metadata": {},
   "source": [
    "**RIR decay**\n",
    "\n",
    "For targets of partial dereverberation RIR is decayed.\n",
    "\n",
    "How we will do it:\n",
    "\n",
    "1. Find the argmax of a RIR. Keep the part of RIR before argmax unchanged: it corresponds to the direct sound pass.\n",
    "\n",
    "2. The rest part should be decayed exponentially, -60 dB per 0.3 sec, i.e. $T_{60}=0.3 \\text{ sec}$.\n",
    "\n",
    "This procedure is defined in the [Cruse](https://arxiv.org/pdf/2101.09249.pdf) paper, a similar approach was used in [PoCoNet](https://arxiv.org/pdf/2008.04470.pdf).\n",
    "\n",
    "![title](assets/pictures/CRUSE_RIR_Shaping.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb79b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_rir(rir: np.ndarray, decay_rt_60_sec = 0.3, sr=SR) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Decays a RIR as described above\n",
    "    \"\"\"\n",
    "    main_tap = np.argmax(rir).item()\n",
    "\n",
    "    part_to_decay = rir[main_tap:]\n",
    "    T_60_max_sec = 0.3\n",
    "    T_60_max_frames = # your code: convert seconds to audio sample scale\n",
    "    gain_db = # your code: -60 per T_60_max_frames, same len as part_to_decay\n",
    "    gain_linear = # convert gain_db into a linear mult\n",
    "    part_decayed = part_to_decay * gain_linear\n",
    "    rir_decayed = np.concatenate([rir[:main_tap], part_decayed])\n",
    "    return rir_decayed\n",
    "\n",
    "\n",
    "rir_decayed = decay_rir(rir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df91be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "\n",
    "ax.set_title(\"RIR power decay\")\n",
    "ax.set_ylabel(\"Power (db-fs)\")\n",
    "plot_rir_for_rt60(rir, ax)\n",
    "plot_rir_for_rt60(rir_decayed, ax)\n",
    "\n",
    "ax.set_ylim(-120, -23)\n",
    "x_ticks = ax.get_xticks()\n",
    "x_tick_labels = x_ticks / SR\n",
    "ax.set_xticks(x_ticks, x_tick_labels)\n",
    "ax.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f0fa52",
   "metadata": {},
   "source": [
    "We don't provide and assertion test here, but this is what it should look like.\n",
    "\n",
    "Pay attention to your decay rate, it should not deviate too much.\n",
    "\n",
    "![title](assets/pictures/rir_shaping_ours.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ddce28",
   "metadata": {},
   "source": [
    "# 3. On-the-fly data generation [5 points]\n",
    "\n",
    "**Why?**\n",
    "\n",
    "We are going to train a model on synthetic mixtures of signals, noises with acoustics simulation via RIR convolution.\n",
    "\n",
    "Why do we train a model on synthetic data? It is the most straight-forward way to obtain corresponding (mixture, signal) pairs.\n",
    "\n",
    "It can seem natural to simulate all the data in advance and train on it.\n",
    "\n",
    "But we shall take another approach: we will generate training mixtures on-the-fly. Data will be generated in parallel with forward-backward passes on GPU -- thanks to PyTorch's DataLoader class.\n",
    "\n",
    "Generating data on the fly we can both increate training data diversity and save disk storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041490c",
   "metadata": {},
   "source": [
    "**Efficent audio chunk reading:**\n",
    "\n",
    "We are going to train on fixed-length chunks of audio.\n",
    "\n",
    "A naive approach to read a chunk of audio file would be to read the full file and then crop it.\n",
    "\n",
    "But we can do it better.\n",
    "\n",
    "`sf.read` function provides `start` and `stop` arguments. When provided, the `audio[start: stop]` segment is read directly without reading the whole file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32e65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_path = \"read_speech/book_00000_chp_0009_reader_06709_11_seg_0.wav\"\n",
    "path = os.path.join(DATA_PATHS[\"speech\"], rel_path)\n",
    "\n",
    "x, _sr = sf.read(path)\n",
    "print(f\"total audio file duration: {len(x)} frames or {len(x) / SR} seconds\")\n",
    "\n",
    "crop_size_sec = 1\n",
    "crop_size_frames = int(crop_size_sec * SR)\n",
    "\n",
    "start = 16_000\n",
    "stop = start + crop_size_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "x = sf.read(path)[0][start: stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "x, _sr = sf.read(path, start=start, stop=stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24013e",
   "metadata": {},
   "source": [
    "**Let's implement a class that will read raw signal and noise data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19887d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalSampler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        paths: list[str],\n",
    "        crop_size_sec: float = 5.0,\n",
    "        min_rms_db: float | None = -38,\n",
    "        sr: int = SR,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        paths: list of absolute paths to the files we are going to sample from\n",
    "        crop_size_sec: the size of generated chunks, in seconds\n",
    "        min_rms_db: chunks with RMS lower than this should be discarded\n",
    "        sr: samplerate\n",
    "        \"\"\"\n",
    "        self.paths = paths\n",
    "        self.crop_size_frames = int(crop_size_sec * sr)\n",
    "        self.min_rms_db = min_rms_db\n",
    "        self.sr = sr\n",
    "\n",
    "    def _sample_from_single_file(\n",
    "        self, path: str, crop_size_frames: int | None = None\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reads a random crop of size crop_size_frames from path.\n",
    "        If the file is shorter, reads the full file.\n",
    "           \n",
    "        Use sf.read(..., start=start_index, stop=end_index) to read\n",
    "        chunks efficiently.\n",
    "        \"\"\"\n",
    "        if crop_size_frames is None:\n",
    "            crop_size_frames = self.crop_size_frames\n",
    "        with sf.SoundFile(path) as f:\n",
    "            if f.samplerate != self.sr:\n",
    "                assert False, (path, f.samplerate, self.sr)\n",
    "            file_duration_frames: int = f.frames\n",
    "        if file_duration_frames < crop_size_frames:\n",
    "            # your code\n",
    "            return # your code\n",
    "        # to avoid the +-1 mistake, keep in mind the corner case when file_duration_frames == crop_size_frames\n",
    "        start = # your code: sample a random segment start;\n",
    "        stop = # your code: the end of the sigment;\n",
    "        x, _sr = sf.read(path, start=start, stop=stop)\n",
    "        return x\n",
    "\n",
    "    def __call__(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generates a chunk of audio data of length self.crop_size_frames.\n",
    "        \n",
    "        1. Samples a random file from self.paths\n",
    "        \n",
    "        2. Reads its random crop of the target size (initialized as self.crop_size_frames).\n",
    "           If the file is shorter, reads the full file.\n",
    "           <this should be done in self._sample_from_single_file>\n",
    "           \n",
    "        3. Checks RMS of the crop.\n",
    "           The crop is discarded if its rms is lower than self.min_rms_db.\n",
    "           Otherwise it is accumulated\n",
    "           \n",
    "        4. Returns the concatenation of accumulated crops\n",
    "           if their total length reaches self.crop_size_frames.\n",
    "           Otherwise sets target size (for 2) to n_frames_ramaining and repeats 1-4\n",
    "        \"\"\"\n",
    "        chunks: list[np.ndarray] = []\n",
    "        duration_frames_remaining = self.crop_size_frames\n",
    "        while duration_frames_remaining > 0:\n",
    "            path = # your code: sample a random path\n",
    "            chunk = self._sample_from_single_file(path, duration_frames_remaining)\n",
    "            if self.min_rms_db is not None:\n",
    "                chunk_rms_db = # your code: use the eval_rms_db function we defined high above\n",
    "                if chunk_rms_db < self.min_rms_db:\n",
    "                    continue\n",
    "            <your code>\n",
    "            # save the chunk and update duration_frames_remaining\n",
    "\n",
    "        result = np.concatenate(chunks)\n",
    "\n",
    "        assert result.ndim == 1, result.shape\n",
    "        assert len(result) == self.crop_size_frames\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d2d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "for crop_size_sec in [1, 3]:\n",
    "    for key in [\"speech\", \"noise\"]:\n",
    "        print(f\"crop_size_sec: {crop_size_sec}, data: {key}\")\n",
    "        sampler = SignalSampler(\n",
    "            list_wavs_in_folder_recursively(DATA_PATHS[key]),\n",
    "            crop_size_sec=crop_size_sec\n",
    "        )\n",
    "        n_samples = 1000\n",
    "        for idx in enumerate(tqdm(range(n_samples))):\n",
    "            chunk = sampler()\n",
    "            assert chunk.ndim == 1, chunk.shape\n",
    "            assert len(chunk) == crop_size_sec * SR, chunk.shape\n",
    "        print(\"Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfea266",
   "metadata": {},
   "source": [
    "**Now let's define a similar sampler for RIRs.**\n",
    "\n",
    "This guy is simpler, because it does not read chunks: it should read full RIRs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RirSampler:\n",
    "    def __init__(self, paths: list[str], sr: int = SR) -> None:\n",
    "        \"\"\"\n",
    "        paths: list of absolute paths to the files we are going to sample from\n",
    "        sr: samplerate\n",
    "        \"\"\"\n",
    "        self.paths = paths\n",
    "        self.sr = sr\n",
    "\n",
    "    def __call__(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Samples a random path and reads the full audio file from it\n",
    "        \"\"\"\n",
    "        path = self.paths[np.random.randint(len(self.paths))]\n",
    "        rir, sr = sf.read(path)\n",
    "        assert sr == self.sr, (path, sr, self.sr)\n",
    "        return rir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011353ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RirSampler(list_wavs_in_folder_recursively(DATA_PATHS[\"rir\"]))\n",
    "n_samples = 1000\n",
    "for idx in enumerate(tqdm(range(n_samples))):\n",
    "    rir = sampler()\n",
    "    assert rir.ndim == 1, rir.shape\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2635a2",
   "metadata": {},
   "source": [
    "**We have learnt to sample RIRs and chunks of signal/noise.**\n",
    "\n",
    "Before we start generating synthetic mixtures, let's define **energy-based Voice Activity Detector (VAD)** and VAD-based RMS calculation.\n",
    "\n",
    "Energy-based VAD calculates frame-level energies of a waveform and selects as active the frames where rms is higher than a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65536ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rms_active(wave: np.ndarray, window: np.ndarray, dynamic_range_db=30) -> tuple[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    wave: input waveform\n",
    "    window: used for frame-level energy calculation\n",
    "    dynamic_range_db: frames with rms > max_rms - dynanic_range_db are treated as active\n",
    "    \"\"\"\n",
    "    wave_sq = np.square(wave)\n",
    "    assert len(window) % 2 == 1, len(window)\n",
    "    # frame-level energies:\n",
    "    win_rms_sq = sig.convolve(wave_sq, window, mode=\"same\")\n",
    "    win_rms_db = # convert to db (power_to_db)\n",
    "    max_win_rms_db = # your code\n",
    "    if not np.isfinite(max_win_rms_db):\n",
    "        return float(\"-inf\", np.zeros(len(wave)), dtype=bool)\n",
    "    activity_mask = # True for frames with higher rms, False for frames with lower rms\n",
    "    \n",
    "    <your code>\n",
    "    # calculate rms_linear (i.e. root-mean-square, without decibels)\n",
    "    # for the masked wave (you may mask wave_sq)\n",
    "    \n",
    "    return rms_linear, activity_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98275c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = SAMPLE_SIGNAL\n",
    "window = np.ones(int(0.030 * SR) + 1)\n",
    "window = window / window.sum()\n",
    "\n",
    "rms_linear, activity_mask = calc_rms_active(x, window)\n",
    "\n",
    "# we have pre-computed the value for the test\n",
    "assert abs(rms_linear - 0.03491389538314626) < 1e-5\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(x, label=\"waveform\")\n",
    "ax.plot(activity_mask, label=\"activity mask\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df60421",
   "metadata": {},
   "source": [
    "Again, check your VAD with our reference:\n",
    "\n",
    "![title](assets/pictures/vad_reference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524356a",
   "metadata": {},
   "source": [
    "**Finally we are ready to generate training mixtures. Let's do it!**\n",
    "\n",
    "We suggest you to start implementation with the `__call__` method and implement the other methods when you see them in the `__call__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_same_length(x: np.ndarray, rir: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convoves signal with rir and crops the result to have the original shape\n",
    "    \"\"\"\n",
    "    convolved = sig.convolve(x, rir, mode=\"full\")\n",
    "    result = convolved[: len(x)]  # we crop out the reverb-only semgent\n",
    "    return result\n",
    "\n",
    "\n",
    "class RandomMixtureSampler:\n",
    "    \"\"\"\n",
    "    Inspired by PoCoNet: https://arxiv.org/pdf/2008.04470.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sig_sampler: tp.Callable[[], np.ndarray],  # SignalSampler\n",
    "        noise_sampler: tp.Callable[[], np.ndarray],  # SignalSampler\n",
    "        rir_sampler: tp.Callable[[], np.ndarray],  # RirSampler\n",
    "        prob_rir_sig: float = 0.5,  # prob to convolve signal with a RIR\n",
    "        prob_rir_noise: float = 0.5,  # prob to convolve noise with a RIR\n",
    "        normalization_rms_db: float = -20,  # normalization level used for signal and noise before gain\n",
    "        noise_gain_range_db: tuple[float, float] = (-5, 5),  # gain applied to noise\n",
    "        mixture_gain_range_db: tuple[float, float] = (-25, 5),  # gain applied to final mixture\n",
    "        partial_dereverb: bool = True,  # whether to do partial dereverberation\n",
    "        *,\n",
    "        sr: int = 16_000,  # samplerate\n",
    "    ) -> None:\n",
    "        self.sig_sampler = sig_sampler\n",
    "        self.noise_sampler = noise_sampler\n",
    "        self.rir_sampler = rir_sampler\n",
    "        self.prob_rir_sig = prob_rir_sig\n",
    "        self.prob_rir_noise = prob_rir_noise\n",
    "        self.sr = sr\n",
    "\n",
    "        self.normalization_rms_db = normalization_rms_db\n",
    "        self.noise_gain_range_db = noise_gain_range_db\n",
    "        self.mixture_gain_range_db = mixture_gain_range_db\n",
    "\n",
    "        self.partial_dereverb = partial_dereverb\n",
    "        \n",
    "        rms_calc_window = np.ones(int(0.030 * SR) + 1)\n",
    "        rms_calc_window = rms_calc_window / rms_calc_window.sum()\n",
    "        self.rms_calc_window = rms_calc_window\n",
    "\n",
    "    def sample_noise_rms_db(self) -> float:\n",
    "        \"\"\"\n",
    "        Samples the rms_db for noise which is relevant before mixing with signal.\n",
    "\n",
    "        Noise is first normlized to normalization_rms_db\n",
    "        and the gain by Uniform(self.noise_gain_range_db).\n",
    "\n",
    "        These 2 operations can be implemented as a single normalize_to_rms operation\n",
    "        with the final rms.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"your code\")\n",
    "\n",
    "    def sample_mixture_gain(self) -> float:\n",
    "        \"\"\"\n",
    "        Uniform(*self.mixture_gain_range_db)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"your code\")\n",
    "\n",
    "    def __call__(self) -> tuple[np.ndarray, np.ndarray, float]:\n",
    "        signal = self.sig_sampler()\n",
    "        noise = self.noise_sampler()\n",
    "        if np.random.binomial(1, self.prob_rir_sig):\n",
    "            rir_signal = # your code: sample a rir\n",
    "            signal_input = # your code: convolve. This signal will be part of the input, not the target\n",
    "            if self.partial_dereverb:\n",
    "                rir_signal_decayed = # your code\n",
    "                signal_target = # your code: convolve. This signal will be target, with partial dereverberation\n",
    "            else:\n",
    "                signal_target = np.copy(\n",
    "                    signal_input\n",
    "                )  # np.copy is crucial to avoid double scaling\n",
    "        else:\n",
    "            signal_input = signal\n",
    "            signal_target = np.copy(\n",
    "                signal\n",
    "            )  # np.copy is crucial to avoid double scaling\n",
    "        del signal\n",
    "        if np.random.binomial(1, self.prob_rir_noise):\n",
    "            rir_noise = self.rir_sampler()\n",
    "            noise = # your code: convolve noise with its rir\n",
    "\n",
    "        # input_signal and mic_signal should be multiplied by the same factor to match each other\n",
    "        mult_signal = get_normalization_mult(\n",
    "            signal_target, self.normalization_rms_db\n",
    "        )\n",
    "        signal_input *= # your code\n",
    "        signal_target *= # your code\n",
    "\n",
    "        noise_rms_db = self.sample_noise_rms_db()\n",
    "        noise = # your code: normalize_to_rms\n",
    "\n",
    "        mixture = signal_input + noise\n",
    "\n",
    "        mixture_gain_db = self.sample_mixture_gain()\n",
    "        mixture_mult = # your code\n",
    "\n",
    "        mixture *= mixture_mult\n",
    "        signal_target *= mixture_mult\n",
    "\n",
    "        mixture = mixture.astype(np.float32)\n",
    "        signal_target = signal_target.astype(np.float32)\n",
    "\n",
    "        rms_signal_active, _ = # your code: calcuate rms of signal_target with energy VAD\n",
    "        # Why do we need it? We will use it for level-invariant training (you will find it somewhere below)\n",
    "\n",
    "        return mixture, signal_target, rms_signal_active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b430237",
   "metadata": {},
   "outputs": [],
   "source": [
    "for crop_size_sec in [1, 5]:\n",
    "    print(f\"crop_size_sec: {crop_size_sec}\")\n",
    "    sampler = RandomMixtureSampler(\n",
    "        sig_sampler=SignalSampler(\n",
    "            list_wavs_in_folder_recursively(DATA_PATHS[\"speech\"]),\n",
    "            crop_size_sec=crop_size_sec\n",
    "        ),\n",
    "        noise_sampler=SignalSampler(\n",
    "            list_wavs_in_folder_recursively(DATA_PATHS[\"noise\"]),\n",
    "            crop_size_sec=crop_size_sec\n",
    "        ),\n",
    "        rir_sampler=RirSampler(\n",
    "            list_wavs_in_folder_recursively(DATA_PATHS[\"rir\"])\n",
    "        ),\n",
    "    )\n",
    "    n_samples = 1000\n",
    "    for idx in enumerate(tqdm(range(n_samples))):\n",
    "        mixture, signal, rms_signal = sampler()\n",
    "        assert len(mixture) == len(signal) == crop_size_sec * SR, chunk.shape\n",
    "    print(\"Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4171e",
   "metadata": {},
   "source": [
    "**Sanity check**\n",
    "\n",
    "It is easy to leave bugs with gains. Here is a simple way to check it.\n",
    "\n",
    "We should listen to: mixture, signal and the difference: (mixture - signal).\n",
    "\n",
    "The difference is the sum of noise and late reverberation. No distinct signal should stay there.\n",
    "\n",
    "An even simpler sanity check: turn partial dereverberation off in the sampler. Then difference should be the noise and it should not contain any trace of the speech signal.\n",
    "\n",
    "Let's generate some tracks in 2 modes:\n",
    "1. No-dereverb\n",
    "2. Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f8e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(sampler, n_samples=4):\n",
    "    _, axes = plt.subplots(ncols=3, nrows=n_samples, figsize=(16, 10), sharex=True, sharey=True)\n",
    "    axes[0, 0].set_title(\"Mixture\")\n",
    "    axes[0, 1].set_title(\"Target\")\n",
    "    axes[0, 2].set_title(\"Interference\")\n",
    "    for sample_idx in range(n_samples):\n",
    "        mixture, target, speech_rms_linear = sampler()\n",
    "        interference = mixture - target\n",
    "        \n",
    "        speech_rms_db = mult_to_gain(speech_rms_linear)\n",
    "\n",
    "        spec_mixture = build_spec_for_plot(mixture)\n",
    "        spec_target = build_spec_for_plot(target)\n",
    "        spec_interf = build_spec_for_plot(interference)\n",
    "\n",
    "        ax = axes[sample_idx][0]\n",
    "        ax.imshow(spec_mixture)\n",
    "        ax.set_aspect(\"auto\")\n",
    "\n",
    "        ax = axes[sample_idx][1]\n",
    "        ax.imshow(build_spec_for_plot(target))\n",
    "        ax.set_xlabel(f\"RMS dBFS: {speech_rms_db:.1f}\")\n",
    "        ax.set_aspect(\"auto\")\n",
    "\n",
    "        ax = axes[sample_idx][2]\n",
    "        ax.imshow(build_spec_for_plot(interference))\n",
    "        ax.set_aspect(\"auto\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b048aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size_sec = 5\n",
    "\n",
    "sig_sampler = SignalSampler(\n",
    "    list_wavs_in_folder_recursively(DATA_PATHS[\"speech\"]),\n",
    "    crop_size_sec=crop_size_sec\n",
    ")\n",
    "noise_sampler = SignalSampler(\n",
    "    list_wavs_in_folder_recursively(DATA_PATHS[\"noise\"]),\n",
    "    crop_size_sec=crop_size_sec\n",
    ")\n",
    "rir_sampler = RirSampler(list_wavs_in_folder_recursively(DATA_PATHS[\"rir\"]))\n",
    "\n",
    "print(\"dereverb off:\")\n",
    "sampler = RandomMixtureSampler(\n",
    "    sig_sampler=sig_sampler,\n",
    "    noise_sampler=noise_sampler,\n",
    "    rir_sampler=rir_sampler,\n",
    "    partial_dereverb=False,\n",
    ")\n",
    "show_samples(sampler)\n",
    "print(\"*\" * 50)\n",
    "\n",
    "print(\"Full\")\n",
    "sampler = RandomMixtureSampler(\n",
    "    sig_sampler=sig_sampler,\n",
    "    noise_sampler=noise_sampler,\n",
    "    rir_sampler=rir_sampler,\n",
    ")\n",
    "show_samples(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7659a37",
   "metadata": {},
   "source": [
    "**Again, our reference images.**\n",
    "\n",
    "Withour dereverb (note the interferences are speech-free):\n",
    "![title](assets/pictures/ref_gen_samples_no_dereverb.png)\n",
    "\n",
    "With dereverb (note residual speech in some of the interferences)\n",
    "![title](assets/pictures/ref_gen_samples_with_dereverb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c632f95",
   "metadata": {},
   "source": [
    "**Wrapping our sampler to PyTorch Dataset**\n",
    "\n",
    "On `__getitem__` it will ignore the input and return a sampler from the sampler.\n",
    "We also define `dummy_duration` variable which will simulate the size of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfacfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "\n",
    "\n",
    "class Dataset(Data.Dataset):\n",
    "    def __init__(self, sampler: RandomMixtureSampler, dummy_duration: int):\n",
    "        self.sampler = sampler\n",
    "        self.dummy_duration = dummy_duration\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.dummy_duration\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Ignores index and a sample from self.sampler, converted to float32\n",
    "        \"\"\"\n",
    "        mixture, target, rms_signal = self.sampler()\n",
    "        mixture = mixture.astype(np.float32)\n",
    "        target = target.astype(np.float32)\n",
    "        return mixture, target, rms_signal\n",
    "\n",
    "    \n",
    "sampler = RandomMixtureSampler(\n",
    "    sig_sampler=sig_sampler,\n",
    "    noise_sampler=noise_sampler,\n",
    "    rir_sampler=rir_sampler,\n",
    ")\n",
    "dataset = Dataset(sampler, 100_000)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1532314c",
   "metadata": {},
   "source": [
    "Now we can use PyTorch DataLoader with our sampler, which should be really fast. If the throughput is higher that 10 batches per second (note that it outputs batches, not single samples), it is more than enough.\n",
    "\n",
    "Pay attention to the `num_workers` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95733386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"\n",
    "    setting different numpy seeds for different workers\n",
    "    \n",
    "    Without it all the DataLoader's workers will produce identical result.\n",
    "    We had a pitty bug here in spring 2024.\n",
    "    \"\"\"\n",
    "    seed = torch.randint(2 ** 32, (1,)).item()\n",
    "    print(f\"worker_id: {worker_id} setting seed to: {seed}\")\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    \n",
    "loader = Data.DataLoader(\n",
    "    dataset, batch_size=10, num_workers=8,\n",
    "    worker_init_fn=worker_init_fn\n",
    ")\n",
    "\n",
    "for idx, batch in enumerate(tqdm(loader)):\n",
    "    if idx == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8629c9d",
   "metadata": {},
   "source": [
    "**Data is ready**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a0a086",
   "metadata": {},
   "source": [
    "# 4. Neural Network Architecture [3 points]\n",
    "\n",
    "**Whoa, we've reached this point.** Good job, but it's still quite a way to go though.\n",
    "\n",
    "Our model will be a 2-dimensional UNet with Dual-path RNN inside. It will perform complex spectral masking and its input will be a power-law compressed complex spectrogram.\n",
    "\n",
    "Our UNet will do down-sampling for both time and requency axes and the convolutions in encoder-decoder will not be causal. Why? Down-sampling saves a lot of computations and we don't want this task to be overly demanding for GPU.\n",
    "\n",
    "Feature maps will be of shape (b, c, t, f): (batch, channels, time, frequencies) for the convolutional layers. For DPRNN part we will use the (b, t, f, c) layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b83b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio as tha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dd5d98",
   "metadata": {},
   "source": [
    "**Here we define the UNet engine.**\n",
    "\n",
    "STFT, Power-law compression and masking logic will be done later.\n",
    "\n",
    "UNet encoder-decoder is custom and pretty typical.\n",
    "\n",
    "Please, refer to the lecture or [DPCRN](https://arxiv.org/pdf/2101.09249.pdf) paper for Band-split RNN definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4cd348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Sequential):\n",
    "    def __init__(self, c_in: int, c_out: int):\n",
    "        super().__init__(\n",
    "            # Conv-BatchNorm-PReLU\n",
    "            # kernel: 3x3\n",
    "            # stride: 2\n",
    "            # padding: 1\n",
    "        )\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet decoder block.\n",
    "    On forward takes input feature_map and a skip feature_map.\n",
    "    The skip feature_map is transformed with a small convolutional layer and added to the input feature_map.\n",
    "    Upsampling is done with PixelShuffle\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in: int, c_out: int, is_last=False):\n",
    "        super().__init__()\n",
    "        self.is_last = is_last\n",
    "        self.skip = # Conv: c_in -> c_in, kernel=1. It will process the skip connection input\n",
    "        self.conv = # Conv: c_in -> c_out * 4, kernel=3, stride=1, same-padding. Why c_out*4? PixelShuffle next.\n",
    "        self.upsample = nn.PixelShuffle(2)\n",
    "        if not is_last:\n",
    "            self.norm_act = nn.Sequential(\n",
    "                # BatchNorm-PReLU\n",
    "            )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        <your code>  # see the class docstring\n",
    "        if not self.is_last:\n",
    "            x = self.norm_act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DPRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual-path RNN block without normalization layers.\n",
    "    \n",
    "    Input-output layout: (b, t, f, c)\n",
    "    \"\"\"\n",
    "    def __init__(self, f_dim, h_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.intra_seq = nn.GRU(f_dim, h_dim // 2, bidirectional=True, batch_first=True)\n",
    "        self.intra_fc = nn.Linear(h_dim, f_dim)\n",
    "        \n",
    "        self.inter_seq = nn.GRU(f_dim, h_dim, bidirectional=False, batch_first=True)\n",
    "        self.inter_fc = nn.Linear(h_dim, f_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: btfc\n",
    "        <your code>\n",
    "\n",
    "\n",
    "class UNetEngine(nn.Module):\n",
    "    def __init__(self, c_in=2, c_out=2, n_enc_dec=4, c_start=16, c_max=128) -> None:\n",
    "        super().__init__()\n",
    "        assert n_enc_dec >= 1, n_enc_dec\n",
    "        self.n_enc_dec = n_enc_dec\n",
    "        enc_layers = [EncoderBlock(c_in, c_start)]\n",
    "        dec_layers = [DecoderBlock(c_start, c_out, is_last=True)]\n",
    "        c_out = c_start\n",
    "        for _ in range(n_enc_dec - 1):\n",
    "            <your code>\n",
    "            # add encoder-decoder layers.\n",
    "            # n_channels should double with each layer pair\n",
    "            # but it should be clipped by c_max\n",
    "\n",
    "            # put the last encoder layer's n_output_channels to c_out\n",
    "            # in the end of the cycle\n",
    "            \n",
    "        # nn.ModuleList allows UNetEngine to see the list of layers\n",
    "        self.enc_layers = nn.ModuleList(enc_layers)\n",
    "        self.bottle = DPRNN(c_out, c_out)\n",
    "        self.dec_layers = nn.ModuleList(reversed(dec_layers))\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        \n",
    "        # we will use these to crop decoder features maps\n",
    "        enc_input_shapes = []\n",
    "\n",
    "        for layer_idx, layer in enumerate(self.enc_layers):\n",
    "            enc_input_shapes.append(x.shape)\n",
    "            if x.shape[2] % 2 != 1:\n",
    "                # order: left-right-top-bottom -> padding time dimension with one zero in the end\n",
    "                x = nn.functional.pad(x, [0, 0, 0, 1])\n",
    "            assert x.shape[3] % 2 == 1, x.shape  # we assume this in crops and paddings\n",
    "            # we cannot control the time dimension: it depends on the inputs\n",
    "            # but the frequency dimension is defined by us.\n",
    "            \n",
    "            # run encoder layer on x and assign x to the output\n",
    "            # store the layer's output for skip connection\n",
    "\n",
    "        x_bctf = x\n",
    "        x_btfc = # permute x_bctf to (b, t, f, c) layout\n",
    "        x_btfc = self.bottle(x_btfc)\n",
    "        x_bctf = # permute x_btfc to (b, c, t, f) layout\n",
    "        x = x_bctf\n",
    "\n",
    "        for dec_layer_idx, layer in enumerate(self.dec_layers):\n",
    "            enc_layer_idx = # the corresponding encoder layer index\n",
    "            skip = skips[enc_layer_idx]\n",
    "            x = # run layer on x and skip, not the order :)\n",
    "            enc_input_shape = enc_input_shapes[enc_layer_idx]\n",
    "            x = # crop according to enc_input_shape\n",
    "        return x\n",
    "\n",
    "\n",
    "model = UNetEngine()\n",
    "x = torch.rand(3, 2, 188, 257)\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137746a9",
   "metadata": {},
   "source": [
    "Here we define complex spectrum masking.\n",
    "\n",
    "Power-law compressed complex spectrogram will be passed to the model in the complex-as-channels fashion.\n",
    "\n",
    "Model's output will be converted to a mask the way it is done in DCUnet (the Tanh-scheme). Please refer to the lecture or the [DCUNet](https://openreview.net/pdf?id=SkeRTsAcYm) paper for mask representation.\n",
    "\n",
    "Such scheme is used in [CRUSE-v1](https://arxiv.org/pdf/2111.11606)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexSpectumMaskingModel(nn.Module):\n",
    "    def __init__(self, engine):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_freqs = 257  # n_fft // 2 + 1, we need this for an assersion\n",
    "        \n",
    "        # win: 32 ms, hop: 16 ms. That should be a little easier than 20 ms and 10 ms.\n",
    "        self.stft = tha.transforms.Spectrogram(\n",
    "            n_fft=512,\n",
    "            win_length=512,\n",
    "            hop_length=256,\n",
    "            power=None,\n",
    "            window_fn=lambda size: torch.sqrt(torch.hann_window(size)),\n",
    "        )\n",
    "\n",
    "        self.istft = tha.transforms.InverseSpectrogram(\n",
    "            n_fft=512,\n",
    "            win_length=512,\n",
    "            hop_length=256,\n",
    "            window_fn=lambda size: torch.sqrt(torch.hann_window(size)),\n",
    "        )\n",
    "        self.engine = engine\n",
    "        \n",
    "    def forward(self, waveform):\n",
    "        assert waveform.ndim == 2, waveform.shape\n",
    "        spec = self.stft(waveform)  # complex\n",
    "        \n",
    "        # power-law compression, still complex\n",
    "        spec_compressed = spec / (torch.abs(spec) + 1e-8) ** 0.7\n",
    "\n",
    "        spec_ri = torch.view_as_real(spec_compressed)  # (b, f, t, 2)\n",
    "        \n",
    "        model_input = # permute spec_ri to match the (b, 2, t, f) layout\n",
    "        \n",
    "        assert model_input.shape[0] == waveform.shape[0], (model_input.shape, waveform.shape)  # batch\n",
    "        assert model_input.shape[1] == 2, model_input.shape  # complex\n",
    "        assert model_input.shape[3] == self.n_freqs, model_input.shape\n",
    "\n",
    "        model_output = # run self.engine\n",
    "        # (b, 2, t, f) -> (b, f, t, 2)\n",
    "        mask_linear = torch.permute(model_output, [0, 3, 2, 1]).contiguous()\n",
    "        mask_linear = torch.view_as_complex(mask_linear)\n",
    "        \n",
    "        # your code\n",
    "        # 1. use torch.abs to for the absolute of complex value\n",
    "        # 2. use torch.angle for its angle\n",
    "        # 3. apply tanh to the absolute of the complex values (compress the absolute)\n",
    "        # 4. use torch.polar to combine the compressed absolute with the original angle\n",
    "        # not that as the absolute values are non-negative, their tanh is non-negative, too\n",
    "\n",
    "        # complex multilication\n",
    "        spec_enhanced = spec * mask_tanh\n",
    "\n",
    "        wave_enhanced = self.istft(spec_enhanced, length=waveform.shape[-1])\n",
    "        return wave_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ce525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    engine = UNetEngine()\n",
    "    model = ComplexSpectumMaskingModel(engine)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "x = torch.rand(3, 48_000)\n",
    "out = model(x)\n",
    "\n",
    "assert x.shape == out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c2eee",
   "metadata": {},
   "source": [
    "# Loss function [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e57f675",
   "metadata": {},
   "source": [
    "**First we will implement multi-resolution power-law compressed spectral loss:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiResLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        configs=(\n",
    "            dict(n_fft=2048, win_length=2048, hop_length=512),\n",
    "            dict(n_fft=1024, win_length=1024, hop_length=256),\n",
    "            dict(n_fft=512, win_length=512, hop_length=128),\n",
    "        )\n",
    "    ):\n",
    "        super().__init__()\n",
    "        stft_variants = []\n",
    "        for entry in configs:\n",
    "            stft_variants.append()<your code>  # build tha.transforms.Spectrogram with power=None and entry as **kwargs\n",
    "        self.stft_variants = nn.ModuleList(stft_variants)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_compressed_spec_mag(wave: torch.Tensor, stft: tha.transforms.Spectrogram):\n",
    "        spec = stft(wave)\n",
    "        # 1. calculate magnitude\n",
    "        # 2. divide both magnitude and complex spectrum by (magnutude + 1e-8) ** 0.7\n",
    "\n",
    "        # note that for the magnitude we actually calcuate the x ** 0.3 function\n",
    "        # which has an infinite derivative in x=0. That's why we need 1e-8.\n",
    "\n",
    "        return spec_compressed, mag_compressed\n",
    "\n",
    "    def forward(self, wave_est: torch.Tensor, wave_target: torch.Tensor):\n",
    "        loss_components = []\n",
    "        for stft in self.stft_variants:\n",
    "            assert isinstance(stft, tha.transforms.Spectrogram)\n",
    "            spec_est, mag_est = self.get_compressed_spec_mag(wave_est, stft)\n",
    "            spec_target, mag_target = self.get_compressed_spec_mag(wave_target, stft)\n",
    "            loss_mag = torch.abs(mag_est - mag_target).square().mean()\n",
    "            loss_cplx = torch.abs(spec_est - spec_target).square().mean()\n",
    "            loss_components.append(0.3 * loss_mag + 0.7 * loss_cplx)\n",
    "        loss = sum(loss_components) / len(loss_components)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b7a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MultiResLoss()\n",
    "\n",
    "est = torch.rand(3, 48_000)\n",
    "target = torch.rand(3, 48_000)\n",
    "\n",
    "criterion(est, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ffc0a",
   "metadata": {},
   "source": [
    "**Another part of the final loss will be negative SNR**\n",
    "\n",
    "Once again,\n",
    "\n",
    "$$\\text{SNR}(\\hat y, y) = 10 \\log_{10} \\frac{||y||^2}{||\\hat y - y||^2} = 10\\log_{10}(||y||^2) - 10\\log_{10}(||\\hat y - y||^2)$$\n",
    "\n",
    "Note that $10\\log_{10}(||\\hat y - y||^2)$ is log-mse and $10\\log_{10}(||y||^2)$ does not depend on the model output $\\hat y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706445b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_neg_snr(wave_est: torch.Tensor, wave_target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    returns a batch of -SNR(wave_est, wave_target) for batched wave_est and wave_target\n",
    "    \"\"\"\n",
    "    \n",
    "    # Note: add 1e-8 to torch.log10 inputs!\n",
    "    <your code>\n",
    "\n",
    "est = torch.rand(3, 48_000)\n",
    "target = torch.rand(3, 48_000)\n",
    "\n",
    "neg_snr = calc_neg_snr(est, target)\n",
    "assert neg_snr.shape[0] == est.shape[0]\n",
    "snr_numpy = eval_snr(est[0].numpy(), target[0].numpy())\n",
    "diff = snr_numpy.item() + neg_snr[0].item()\n",
    "assert abs(diff) < 1e-3, (neg_snr, snr_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69585dbd",
   "metadata": {},
   "source": [
    "# 6. Train loop [2 points]\n",
    "\n",
    "Now everything is ready. Let's train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16347c6",
   "metadata": {},
   "source": [
    "**General parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d19b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 2  # parallel data generation\n",
    "\n",
    "LR = 2e-4\n",
    "BATCH_SIZE = 12  # the bigger the better\n",
    "MAX_GRAD_NORM = 4  # for clipping\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    PIN_MEMORY = True\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    PIN_MEMORY = False\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b37d9",
   "metadata": {},
   "source": [
    "**metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab22419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.audio import SignalNoiseRatio, ScaleInvariantSignalNoiseRatio\n",
    "\n",
    "\n",
    "metrics = torch.nn.ModuleDict(\n",
    "    {\n",
    "        \"SNR\": SignalNoiseRatio(),\n",
    "        \"SI-SNR\": ScaleInvariantSignalNoiseRatio(),\n",
    "    }\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f74ee3",
   "metadata": {},
   "source": [
    "**data, train-dev split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0675a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "SNR_RANGE = (-5, 5)  # range for SNR in data generation\n",
    "CROP_SIZE_SEC = 3\n",
    "\n",
    "\n",
    "test_size = 0.1\n",
    "split_speech = train_test_split(\n",
    "    list_wavs_in_folder_recursively(DATA_PATHS[\"speech\"]),\n",
    "    random_state=2967,\n",
    "    test_size=test_size,\n",
    ")\n",
    "split_noise = train_test_split(\n",
    "    list_wavs_in_folder_recursively(DATA_PATHS[\"noise\"]),\n",
    "    random_state=8701,\n",
    "    test_size=test_size,\n",
    ")\n",
    "split_rir = train_test_split(\n",
    "    list_wavs_in_folder_recursively(DATA_PATHS[\"rir\"]),\n",
    "    random_state=9807,\n",
    "    test_size=test_size,\n",
    ")\n",
    "\n",
    "\n",
    "loaders = {}\n",
    "for split_idx, mode in enumerate([\"train\", \"val\"]):\n",
    "    sig_sampler = SignalSampler(split_speech[split_idx], crop_size_sec=CROP_SIZE_SEC)\n",
    "    noise_sampler = SignalSampler(split_noise[split_idx], crop_size_sec=CROP_SIZE_SEC)\n",
    "    rir_sampler = RirSampler(split_rir[split_idx])\n",
    "    mixture_sampler = RandomMixtureSampler(\n",
    "        sig_sampler=sig_sampler,\n",
    "        noise_sampler=noise_sampler,\n",
    "        rir_sampler=rir_sampler,\n",
    "        prob_rir_noise=0.5,\n",
    "        prob_rir_sig=0.5,\n",
    "        normalization_rms_db=-20,\n",
    "        noise_gain_range_db=(-SNR_RANGE[1], -SNR_RANGE[0]),\n",
    "        mixture_gain_range_db=(-25, 5),\n",
    "        sr=SR,\n",
    "        partial_dereverb=True,\n",
    "    )\n",
    "    batches_per_epoch = 1024 if mode == \"train\" else 256\n",
    "    dataset = Dataset(mixture_sampler, dummy_duration=BATCH_SIZE * batches_per_epoch)\n",
    "    loader = Data.DataLoader(\n",
    "        dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "    loaders[mode] = loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fe7640",
   "metadata": {},
   "source": [
    "**model, optimizer, criterion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e8202",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = MultiResLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d9173",
   "metadata": {},
   "source": [
    "**Tensorboard logger:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfcb209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "SAVE_SOUND_FREQ = 512\n",
    "LOG_FREQ = 20\n",
    "SAVE_SNAPSHOT_FREQ = 512\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a092da1d",
   "metadata": {},
   "source": [
    "**For level invariant training.** We will scale target to -25 dBFS when calculating loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884fb241",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMS_DB_TARGET_FOR_LOSS = -25\n",
    "RMS_LINEAR_TARGET_FOR_LOSS = gain_to_mult(-25)\n",
    "RMS_LINEAR_TARGET_FOR_LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, training: bool, global_step_idx: int = 0):\n",
    "    mode_name = \"train\" if training else \"val\"\n",
    "    model.train(training)\n",
    "\n",
    "    for evaluator in metrics.values():\n",
    "        evaluator.reset()\n",
    "    loss_storage = [[] for _ in range(1)]\n",
    "    for step_idx, (mixture, target, rms_signal) in enumerate(tqdm(loader)):\n",
    "        mixture = mixture.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        rms_signal = rms_signal.to(DEVICE)\n",
    "        est = model(mixture)\n",
    "        \n",
    "        scaling_mult = RMS_LINEAR_TARGET_FOR_LOSS / rms_signal[:, None]\n",
    "        target_scaled =  # your code: scale target\n",
    "        est_scaled =  # your code: scale est\n",
    "        mixture_scaled =  # your code: scale mixture\n",
    "\n",
    "        loss = criterion(est_scaled, target_scaled)\n",
    "        neg_snr_loss =  # your code: calc_neg_snr\n",
    "        loss = loss + 0.1 * neg_snr_loss\n",
    "    \n",
    "        if training and (global_step_idx % 512 == 0):\n",
    "            print(\"Loss:\", loss.item())\n",
    "\n",
    "        loss_storage[0].append(loss.item())\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for name, evaluator in metrics.items():\n",
    "                value_out = evaluator(est_scaled, target_scaled).mean().item()\n",
    "            \n",
    "        if training:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), MAX_GRAD_NORM\n",
    "            )\n",
    "            optimizer.step()\n",
    "\n",
    "        if (step_idx % LOG_FREQ == 0) and training:\n",
    "            loss_logs = [sum(x) / len(x) for x in loss_storage]\n",
    "            writer.add_scalar(f\"{mode_name}/loss/final\", loss_logs[0], global_step_idx)\n",
    "            loss_storage = [[] for _ in range(1)]\n",
    "\n",
    "            writer.add_scalar(f\"{mode_name}/grad_norm\", grad_norm.item(), global_step_idx)\n",
    "\n",
    "            for name, evaluator in metrics.items():\n",
    "                value_out = evaluator.compute().item()\n",
    "                writer.add_scalar(f\"{mode_name}/metrics/{name}\", value_out, global_step_idx,)\n",
    "                evaluator.reset()\n",
    "\n",
    "        if step_idx % SAVE_SOUND_FREQ == 0:\n",
    "            with torch.no_grad():\n",
    "                path_samples = f\"samples/{mode_name}\"\n",
    "                os.makedirs(path_samples, exist_ok=True)\n",
    "                sf.write(\n",
    "                    f\"{path_samples}/{global_step_idx:05d}_mixture.wav\",\n",
    "                    mixture[0].cpu().numpy(),\n",
    "                    SR,\n",
    "                )\n",
    "                sf.write(\n",
    "                    f\"{path_samples}/{global_step_idx:05d}_clean.wav\",\n",
    "                    target[0].cpu().numpy(),\n",
    "                    SR,\n",
    "                )\n",
    "                sf.write(\n",
    "                    f\"{path_samples}/{global_step_idx:05d}_noise.wav\",\n",
    "                    mixture[0].cpu().numpy() - target[0].cpu().numpy(),\n",
    "                    SR,\n",
    "                )\n",
    "                sf.write(\n",
    "                    f\"{path_samples}/{global_step_idx:05d}_est.wav\",\n",
    "                    est[0].cpu().numpy(),\n",
    "                    SR,\n",
    "                )\n",
    "        if step_idx % SAVE_SNAPSHOT_FREQ == 0 and training:\n",
    "            torch.save(model.state_dict(), \"state_dict_latest.pt\")\n",
    "        if training:\n",
    "            global_step_idx += 1\n",
    "            \n",
    "    if not training:\n",
    "        for name, evaluator in metrics.items():\n",
    "            value_out = evaluator.compute().item()\n",
    "            print(f\"{mode_name}/metrics/{name}\", value_out)\n",
    "            writer.add_scalar(f\"{mode_name}/metrics/{name}\", value_out, global_step_idx,)\n",
    "        \n",
    "    return global_step_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c9943",
   "metadata": {},
   "source": [
    "### At last!!! Running the train loop!\n",
    "\n",
    "**Your goal is to beat 6 dB SI-SNR on validation.** Actually it should take only about 2-3 epochs.\n",
    "\n",
    "You may stop training once 6 dB SI-SNR is reached or you may continue it as you wish. If trained longer, 10 dB SI-SNR should reachable with this setup.\n",
    "\n",
    "Note that the training dataset is very small in this notebook. You can use this code directly with a larger dataset.\n",
    "\n",
    "See our training curve below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ec7e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_step_idx = 0\n",
    "while True:\n",
    "    global_step_idx = run_epoch(loaders[\"train\"], True, global_step_idx)\n",
    "    with torch.no_grad():\n",
    "        run_epoch(loaders[\"val\"], False, global_step_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af02d70c",
   "metadata": {},
   "source": [
    "**Thiese are our training curves.** They were obtained on a laptop with GPU, nvidia-smi shows 625 MiB GPU memory usage. Threshold was beaten after the second train epoch and it took less than 2 minutes.\n",
    "\n",
    "If you hold the training longer, you may get a much better quality. As the train dataset is small, you may listen to outputs on the train set to check the potential of the model. Once again, this code is applicable for full-sized datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210584a",
   "metadata": {},
   "source": [
    "![title](assets/pictures/training_curvers.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af2584-e0cd-4c0e-9cfa-f2e1400c5493",
   "metadata": {},
   "source": [
    "Thank you for solving this task. It was hard, but hope, you liked it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d855547-88b9-4f62-b1bb-58ebc3af0b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
