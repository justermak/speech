{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework (Week 3) -- Biometrics (20 points)\n",
    "\n",
    "In this homework we train Biometrics Verification model and use some features to increase quality:\n",
    "1) Train ECAPA-TDNN (10 points)\n",
    "2) Any contrastive loss (10 points)\n",
    "\n",
    "Link to download dataset: https://disk.yandex.ru/d/lyhtieYbxQOYqw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data\n",
    "import torchaudio\n",
    "import tqdm.notebook as tqdm\n",
    "import urllib\n",
    "\n",
    "import dataset\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-17 19:10:49--  https://downloader.disk.yandex.ru/disk/9a36c09c6aa94ea4c283d6e7e1a3c08847bca72bbce7027d7929aa41b9ad769d/65d112c9/gtj3WQiuHGabqHv6W0pVHNJIUgtSWTZDwbzsVe5DMplbs5asv2nBcD3Cqjp-J8909zgU-GpPeRE_lnRWKdcTVw%3D%3D?uid=0&filename=voxceleb.tar.gz&disposition=attachment&hash=UbepA8Q9HDxeFKWCa8Y0tO7K9asCmTAjTZtG1dex6z7oEbCbmS7yu53hGphcr8rhq/J6bpmRyOJonT3VoXnDag%3D%3D%3A&limit=0&content_type=application%2Fx-gzip&owner_uid=163052607&fsize=10417995782&hid=f1790bd28373ce0f598cd14b8c76d6b8&media_type=compressed&tknv=v2\n",
      "Resolving downloader.disk.yandex.ru (downloader.disk.yandex.ru)... 77.88.21.127\n",
      "Connecting to downloader.disk.yandex.ru (downloader.disk.yandex.ru)|77.88.21.127|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://s159vla.storage.yandex.net/rdisk/9a36c09c6aa94ea4c283d6e7e1a3c08847bca72bbce7027d7929aa41b9ad769d/65d112c9/gtj3WQiuHGabqHv6W0pVHNJIUgtSWTZDwbzsVe5DMplbs5asv2nBcD3Cqjp-J8909zgU-GpPeRE_lnRWKdcTVw==?uid=0&filename=voxceleb.tar.gz&disposition=attachment&hash=UbepA8Q9HDxeFKWCa8Y0tO7K9asCmTAjTZtG1dex6z7oEbCbmS7yu53hGphcr8rhq/J6bpmRyOJonT3VoXnDag%3D%3D%3A&limit=0&content_type=application%2Fx-gzip&owner_uid=163052607&fsize=10417995782&hid=f1790bd28373ce0f598cd14b8c76d6b8&media_type=compressed&tknv=v2&rtoken=TDjWY3UVPdeY&force_default=no&ycrid=na-f5f79ce631bd7c4964c75dcc1528d501-downloader3f&ts=6119974e38440&s=fc10cba13911500135faffd780880438f0b465c6f1f107778e611199ac979bbb&pb=U2FsdGVkX19DreIm8jsBnaongtsGS6u74NXhf3DZNayIxMR_-buQ0EE5ey39_vQP5CJerOlkv3sTXaz_6XoaCQYGJ1q_5n_rEYBoUqwCI6c [following]\n",
      "--2024-02-17 19:10:50--  https://s159vla.storage.yandex.net/rdisk/9a36c09c6aa94ea4c283d6e7e1a3c08847bca72bbce7027d7929aa41b9ad769d/65d112c9/gtj3WQiuHGabqHv6W0pVHNJIUgtSWTZDwbzsVe5DMplbs5asv2nBcD3Cqjp-J8909zgU-GpPeRE_lnRWKdcTVw==?uid=0&filename=voxceleb.tar.gz&disposition=attachment&hash=UbepA8Q9HDxeFKWCa8Y0tO7K9asCmTAjTZtG1dex6z7oEbCbmS7yu53hGphcr8rhq/J6bpmRyOJonT3VoXnDag%3D%3D%3A&limit=0&content_type=application%2Fx-gzip&owner_uid=163052607&fsize=10417995782&hid=f1790bd28373ce0f598cd14b8c76d6b8&media_type=compressed&tknv=v2&rtoken=TDjWY3UVPdeY&force_default=no&ycrid=na-f5f79ce631bd7c4964c75dcc1528d501-downloader3f&ts=6119974e38440&s=fc10cba13911500135faffd780880438f0b465c6f1f107778e611199ac979bbb&pb=U2FsdGVkX19DreIm8jsBnaongtsGS6u74NXhf3DZNayIxMR_-buQ0EE5ey39_vQP5CJerOlkv3sTXaz_6XoaCQYGJ1q_5n_rEYBoUqwCI6c\n",
      "Resolving s159vla.storage.yandex.net (s159vla.storage.yandex.net)... 77.88.39.49\n",
      "Connecting to s159vla.storage.yandex.net (s159vla.storage.yandex.net)|77.88.39.49|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10417995782 (9.7G) [application/x-gzip]\n",
      "Saving to: ‘data.tar.gz’\n",
      "\n",
      "data.tar.gz         100%[===================>]   9.70G  5.35MB/s    in 33m 43s \n",
      "\n",
      "2024-02-17 19:44:33 (4.91 MB/s) - ‘data.tar.gz’ saved [10417995782/10417995782]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
    "# public_key = 'https://disk.yandex.ru/d/lyhtieYbxQOYqw'\n",
    "# final_url = base_url + urllib.parse.urlencode(dict(public_key=public_key))\n",
    "# response = requests.get(final_url)\n",
    "# download_url = response.json()['href']\n",
    "# !wget -O voxceleb.tar.gz \"{download_url}\"\n",
    "# !tar -xf voxceleb.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some model train example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' # \"cpu\" for cpu, also you can use \"cuda\" for gpu and \"mps\" for apple silicon\n",
    "DATADIR = '../data'\n",
    "FEATS = 80\n",
    "LOADER_WORKERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchaudio.transforms.MFCC(n_mfcc=FEATS) # You can try some other transformations here\n",
    "trainset = dataset.Dataset(os.path.join(DATADIR, 'voxceleb_train'), transform)\n",
    "testset = dataset.Dataset(os.path.join(DATADIR, 'voxceleb_test'), transform)\n",
    "test_targets = pd.read_csv(os.path.join(DATADIR, 'target.csv')).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_shape: int, output_shape: int, hidden: int, kernel: int = 7, sride: int = 2):\n",
    "        super().__init__()\n",
    "        self._emb = nn.Sequential(\n",
    "            nn.Conv1d(input_shape, hidden, kernel, stride=sride),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden, hidden, kernel, stride=sride),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden, hidden, kernel, stride=sride),\n",
    "            nn.AdaptiveMaxPool1d(1),\n",
    "        )\n",
    "        self._final = nn.Sequential(\n",
    "            nn.Linear(hidden, output_shape),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        emb = self._emb(X).squeeze(2)\n",
    "        return self._final(emb), emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Cosine similarity:\n",
    "$CS(a, b) = \\frac{<a, b>}{\\|a\\| \\|b\\|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    a = a.reshape(-1)\n",
    "    b = b.reshape(-1)\n",
    "    return np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is O(N log N) algorithm for find best_eer:\n",
    "1) Sort prediction by probability\n",
    "2) Going through items and recalculating far and frr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_eer(data):\n",
    "    full = sorted(data, key=lambda x: (x[0], -x[1]))\n",
    "    pos = len([item for item in full if item[1] == 1])\n",
    "    neg = len(full) - pos\n",
    "    cur_pos = pos\n",
    "    cur_neg = 0\n",
    "    best_eer = 1\n",
    "    for _, label in full:\n",
    "        if label == 1:\n",
    "            cur_pos -= 1\n",
    "        else:\n",
    "            cur_neg += 1\n",
    "        cur_eer = max((pos - cur_pos) / pos, (neg - cur_neg) / neg)\n",
    "        best_eer = min(best_eer, cur_eer)\n",
    "    return best_eer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage(model, opt, batch_size: int = 256):\n",
    "    loader = torch_data.DataLoader(\n",
    "        trainset,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "        num_workers=LOADER_WORKERS,\n",
    "    )\n",
    "    loss_sum = 0.0\n",
    "    batches = 0\n",
    "    for X, Y, _ in tqdm.tqdm(loader):\n",
    "        logits, _ = model.forward(X.to(DEVICE))\n",
    "        loss = F.nll_loss(logits, Y.to(DEVICE))\n",
    "        loss_sum += loss.item()\n",
    "        batches += 1\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return loss_sum / batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_eval_score(model: nn.Module, batch_size: int = 256):\n",
    "    loader = torch_data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "        num_workers=LOADER_WORKERS,\n",
    "    )\n",
    "    items = {}\n",
    "    target_scores = []\n",
    "    with torch.no_grad():\n",
    "        for X, _, pathes in tqdm.tqdm(loader):\n",
    "            _, embds = model.forward(X.to(DEVICE))\n",
    "            embds = embds.cpu().data.numpy().reshape(X.shape[0], -1)\n",
    "            for embd, path in zip(embds, pathes):\n",
    "                items[path] = embd\n",
    "    for item1, item2, target in test_targets:\n",
    "        target_scores.append((cosine_similarity(items[item1], items[item2]), target))\n",
    "    return best_eer(target_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    opt,\n",
    "    batch_size: int = 256,\n",
    "    epochs: int = 10,\n",
    "    train_fun = train_stage,\n",
    "    train_kwargs = {},\n",
    "):\n",
    "    train_losses = []\n",
    "    eval_scores = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses.append(train_fun(model, opt, batch_size=batch_size))\n",
    "        model.eval()\n",
    "        eval_scores.append(calc_eval_score(model, batch_size=batch_size))\n",
    "        clear_output()\n",
    "        fig, axis = plt.subplots(1, 2, figsize=(15, 7))\n",
    "        axis[0].plot(np.arange(1, epoch + 2), train_losses, label='train CE loss')\n",
    "        axis[1].plot(np.arange(1, epoch + 2), eval_scores, label='eval')\n",
    "        axis[0].set(xlabel='epoch', ylabel='CE Loss')\n",
    "        axis[1].set(xlabel='epoch', ylabel='EER')\n",
    "        fig.legend()\n",
    "        plt.show()\n",
    "        print(f'Epoch {epoch + 1}. Train loss {train_losses[-1]}. Eval score {eval_scores[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46628e0871742509e7a150217831184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17bdf1d040f4901854bfcfd6360fe36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'id10292/16cc3b2548290b3be6a59b9cb45b75d5.flac'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(FEATS, trainset\u001b[38;5;241m.\u001b[39mspeakers(), \u001b[38;5;241m128\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m      2\u001b[0m opt \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, opt, batch_size, epochs, train_fun, train_kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_fun(model, opt, batch_size\u001b[38;5;241m=\u001b[39mbatch_size))\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 15\u001b[0m eval_scores\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcalc_eval_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     16\u001b[0m clear_output()\n\u001b[0;32m     17\u001b[0m fig, axis \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m, in \u001b[0;36mcalc_eval_score\u001b[1;34m(model, batch_size)\u001b[0m\n\u001b[0;32m     15\u001b[0m             items[path] \u001b[38;5;241m=\u001b[39m embd\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item1, item2, target \u001b[38;5;129;01min\u001b[39;00m test_targets:\n\u001b[1;32m---> 17\u001b[0m     target_scores\u001b[38;5;241m.\u001b[39mappend((cosine_similarity(\u001b[43mitems\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem1\u001b[49m\u001b[43m]\u001b[49m, items[item2]), target))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_eer(target_scores)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id10292/16cc3b2548290b3be6a59b9cb45b75d5.flac'"
     ]
    }
   ],
   "source": [
    "model = Model(FEATS, trainset.speakers(), 128).to(DEVICE)\n",
    "opt = optim.Adam(model.parameters())\n",
    "train(model, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECAPA TDNN (10 points)\n",
    "\n",
    "Paper: https://arxiv.org/pdf/2005.07143.pdf\n",
    "\n",
    "Papers for ECAPA parts:\n",
    "- SE-Blocks - https://arxiv.org/pdf/1709.01507.pdf\n",
    "- Res2Net - https://arxiv.org/pdf/1904.01169.pdf\n",
    "- Attentive Stats Pooling - https://arxiv.org/pdf/1803.10963.pdf\n",
    "- AAM Softmax - https://arxiv.org/pdf/1906.07317.pdf\n",
    "\n",
    "Also you can optionally add other settings for paper:\n",
    "- SpecAug\n",
    "- Weight decay for optimizer\n",
    "- LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, input_shape: int, reduction: int):\n",
    "        super().__init__()\n",
    "        # <YOUR CODE IS HERE>\n",
    "\n",
    "    def __call__(self, X):\n",
    "        pass\n",
    "        # <YOUR CODE IS HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res2Net(nn.Module):\n",
    "    def __init__(self, hidden: int, dilation: int, scale: int):\n",
    "        super().__init__()\n",
    "        assert hidden % scale == 0\n",
    "        # <YOUR CODE IS HERE>\n",
    "\n",
    "    def __call__(self, X):\n",
    "        pass\n",
    "        # <YOUR CODE IS HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcapaBlock(nn.Module):\n",
    "    def __init__(self, hidden: int, dilation: int, scale: int):\n",
    "        super().__init__()\n",
    "        # <YOUR CODE IS HERE>\n",
    "\n",
    "    def __call__(self, X):\n",
    "        pass\n",
    "        # <YOUR CODE IS HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveStatsPooling(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden: int):\n",
    "        super().__init__()\n",
    "        # <YOUR CODE IS HERE>\n",
    "\n",
    "    def __call__(self, X):\n",
    "        # X shape = [time, feats]\n",
    "        # calc mean and std for X over time dimension\n",
    "        # concatenate mean and std to X over feats dimension to make shape [time, feats * 3]\n",
    "        # attention\n",
    "        # weighted mean and std with weights from attention for original X\n",
    "        pass\n",
    "        # <YOUR CODE IS HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAMSoftmax(nn.Module):\n",
    "    def __init__(self, input_shape, n_class, margin, scale):\n",
    "        super().__init__()\n",
    "        # <YOUR CODE IS HERE>\n",
    "\n",
    "    def forward(self, X):\n",
    "        # calc cosine similarity between X and weights\n",
    "        # theta = angle from cosine similarity\n",
    "        # return matrix S, where S_ij = \n",
    "        #     \\log \\frac{\n",
    "        #         \\exp{scale \\cos{theta_ij + margin}}\n",
    "        #     }{\n",
    "        #         \\exp{scale \\cos{theta_ij + margin}} + \\sum_{k != j} \\exp{scale \\cos{theta_ij}}\n",
    "        #     }\n",
    "        pass\n",
    "        # <YOUR CODE IS HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcapaTDNN(nn.Module):\n",
    "    def __init__(self, input_shape: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        # <YOUR CODE IS HERE>\n",
    "\n",
    "    def forward(self, X):\n",
    "        # <YOUR CODE IS HERE>\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ECAPA model, at this point you can archive stable score (for several consecutive epochs) near 0.08 EER.\n",
    "\n",
    "You can train ECAPA with hidden size 256 to increase speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EcapaTDNN(FEATS, trainset.speakers(), 128).to(DEVICE)\n",
    "opt = optim.Adam(model.parameters())\n",
    "train(model, opt, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to preserve results before uptraining experiments\n",
    "torch.save(model, 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive losses (10 points)\n",
    "\n",
    "You can use anyone constrative loss.\n",
    "Good article with contrastive losses https://lilianweng.github.io/posts/2021-05-31-contrastive/\n",
    "\n",
    "Base losses:\n",
    "- contrastive\n",
    "- triplet -- it gives a better quality usually\n",
    "- lifted structured loss -- better batch data utilization\n",
    "\n",
    "The main problem with contrastive loss is the positive pairs sampler.\n",
    "This is because a large number of classes provided only once per batch\n",
    "in case of large number of classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositivePairsSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, speakers: list[int], batch_size: int):\n",
    "        pass\n",
    "        # <YOUR CODE IS HERE>\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "        # <YOUR CODE IS HERE>\n",
    "\n",
    "    def __iter__(self):\n",
    "        # yield __len__ batches as list of indexes of samples from dataset\n",
    "        # <YOUR CODE IS HERE>\n",
    "        for _ in range(len(self)):\n",
    "            indexes = []\n",
    "            # <YOUR CODE IS HERE>\n",
    "            yield indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_train_stage(model, opt, batch_size: int = 256):\n",
    "    # You can use any contrastive loss here to improve training\n",
    "    # You can combine contrastive loss with the NLL loss after AAM softmax to improve stability\n",
    "    loader = torch_data.DataLoader(\n",
    "        trainset,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "        num_workers=LOADER_WORKERS,\n",
    "        batch_sampler=PositivePairsSampler(trainset._speakers, batch_size)\n",
    "    )\n",
    "    # <YOUR CODE IS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model with contrastive loss here. At this point you can archive EER near 0.06-0.07 (it should be at least on 0.005 to 0.01 better than before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pt').to(DEVICE)\n",
    "opt = optim.Adam(model.parameters())\n",
    "train(model, opt, batch_size=128, train_fun=contrastive_train_stage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
