{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework (Week 3) -- Biometrics (20 points)\n",
    "\n",
    "In this homework we train Biometrics Verification model and use some features to increase quality:\n",
    "1) Train ECAPA-TDNN (10 points)\n",
    "2) Any contrastive loss (10 points)\n",
    "\n",
    "Link to download dataset: https://disk.yandex.ru/d/lyhtieYbxQOYqw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data\n",
    "import torchaudio\n",
    "import tqdm.notebook as tqdm\n",
    "import urllib\n",
    "\n",
    "import dataset\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
    "# public_key = 'https://disk.yandex.ru/d/lyhtieYbxQOYqw'\n",
    "# final_url = base_url + urllib.parse.urlencode(dict(public_key=public_key))\n",
    "# response = requests.get(final_url)\n",
    "# download_url = response.json()['href']\n",
    "# !wget -O voxceleb.tar.gz \"{download_url}\"\n",
    "# !tar -xf voxceleb.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some model train example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' # \"cpu\" for cpu, also you can use \"cuda\" for gpu and \"mps\" for apple silicon\n",
    "DATADIR = '../data'\n",
    "FEATS = 80\n",
    "LOADER_WORKERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\AppData\\Local\\Python\\Python310\\lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_transform = nn.Sequential(\n",
    "    torchaudio.transforms.MFCC(n_mfcc=FEATS),\n",
    "    # torchaudio.transforms.TimeMasking(10),\n",
    "    # torchaudio.transforms.FrequencyMasking(10),\n",
    ")\n",
    "test_transform = torchaudio.transforms.MFCC(n_mfcc=FEATS)\n",
    "trainset = dataset.Dataset(os.path.join(DATADIR, 'voxceleb_train'), train_transform)\n",
    "testset = dataset.Dataset(os.path.join(DATADIR, 'voxceleb_test'), test_transform)\n",
    "test_targets = pd.read_csv(os.path.join(DATADIR, 'target.csv')).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Model(nn.Module):\n",
    "#     def __init__(self, input_shape: int, output_shape: int, hidden: int, kernel: int = 7, sride: int = 2):\n",
    "#         super().__init__()\n",
    "#         self._emb = nn.Sequential(\n",
    "#             nn.Conv1d(input_shape, hidden, kernel, stride=sride),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(hidden, hidden, kernel, stride=sride),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(hidden, hidden, kernel, stride=sride),\n",
    "#             nn.AdaptiveMaxPool1d(1),\n",
    "#         )\n",
    "#         self._final = nn.Sequential(\n",
    "#             nn.Linear(hidden, output_shape),\n",
    "#             nn.LogSoftmax(dim=-1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, X) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "#         emb = self._emb(X).squeeze(2)\n",
    "#         return self._final(emb), emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Cosine similarity:\n",
    "$CS(a, b) = \\frac{<a, b>}{\\|a\\| \\|b\\|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    a = a.reshape(-1)\n",
    "    b = b.reshape(-1)\n",
    "    return np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is O(N log N) algorithm for find best_eer:\n",
    "1) Sort prediction by probability\n",
    "2) Going through items and recalculating far and frr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_eer(data):\n",
    "    full = sorted(data, key=lambda x: (x[0], -x[1]))\n",
    "    pos = len([item for item in full if item[1] == 1])\n",
    "    neg = len(full) - pos\n",
    "    cur_pos = pos\n",
    "    cur_neg = 0\n",
    "    best_eer = 1\n",
    "    for _, label in full:\n",
    "        if label == 1:\n",
    "            cur_pos -= 1\n",
    "        else:\n",
    "            cur_neg += 1\n",
    "        cur_eer = max((pos - cur_pos) / pos, (neg - cur_neg) / neg)\n",
    "        best_eer = min(best_eer, cur_eer)\n",
    "    return best_eer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage(model, opt, scheduler, batch_size: int = 32):\n",
    "    loader = torch_data.DataLoader(\n",
    "        trainset,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "        num_workers=LOADER_WORKERS,\n",
    "    )\n",
    "    loss_sum = 0.0\n",
    "    batches = 0\n",
    "    i = 0\n",
    "    for X, Y, _ in tqdm.tqdm(loader):\n",
    "        logits, _ = model.forward(X.to(DEVICE))\n",
    "        loss = F.nll_loss(logits, Y.to(DEVICE))\n",
    "        loss_sum += loss.item()\n",
    "        batches += 1\n",
    "        loss.backward()\n",
    "        if (i + 1) % 4 == 0:\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "            opt.zero_grad()\n",
    "        i += 1\n",
    "    return loss_sum / batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_eval_score(model: nn.Module, batch_size: int = 256):\n",
    "    loader = torch_data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "        num_workers=LOADER_WORKERS,\n",
    "    )\n",
    "    items = {}\n",
    "    target_scores = []\n",
    "    with torch.no_grad():\n",
    "        for X, _, pathes in tqdm.tqdm(loader):\n",
    "            _, embds = model.forward(X.to(DEVICE))\n",
    "            embds = embds.cpu().data.numpy().reshape(X.shape[0], -1)\n",
    "            for embd, path in zip(embds, pathes):\n",
    "                items[path] = embd\n",
    "    for item1, item2, target in test_targets:\n",
    "        item1 = item1.replace('/', '\\\\') # windows is cringe\n",
    "        item2 = item2.replace('/', '\\\\') # windows is cringe x2\n",
    "        target_scores.append((cosine_similarity(items[item1], items[item2]), target))\n",
    "    return best_eer(target_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    opt,\n",
    "    scheduler,\n",
    "    batch_size: int = 32,\n",
    "    epochs: int = 10,\n",
    "    train_fun = train_stage,\n",
    "):\n",
    "    train_losses = []\n",
    "    eval_scores = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses.append(train_fun(model, opt, scheduler, batch_size=batch_size))\n",
    "        \n",
    "        model.eval()\n",
    "        eval_scores.append(calc_eval_score(model, batch_size=batch_size))\n",
    "        clear_output()\n",
    "        fig, axis = plt.subplots(1, 2, figsize=(15, 7))\n",
    "        axis[0].plot(np.arange(1, epoch + 2), train_losses, label='train CE loss')\n",
    "        axis[0].set(xlabel='epoch', ylabel='CE Loss')\n",
    "    \n",
    "        axis[1].plot(np.arange(1, epoch + 2), eval_scores, label='eval')\n",
    "        axis[1].set(xlabel='epoch', ylabel='EER')\n",
    "        fig.legend()\n",
    "        plt.show()\n",
    "        print(f'Epoch {epoch + 1}. Train loss {train_losses[-1]}. Eval score {eval_scores[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Model(FEATS, trainset.speakers(), 128).to(DEVICE)\n",
    "# opt = optim.Adam(model.parameters())\n",
    "# train(model, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECAPA TDNN (10 points)\n",
    "\n",
    "Paper: https://arxiv.org/pdf/2005.07143.pdf\n",
    "\n",
    "Papers for ECAPA parts:\n",
    "- SE-Blocks - https://arxiv.org/pdf/1709.01507.pdf\n",
    "- Res2Net - https://arxiv.org/pdf/1904.01169.pdf\n",
    "- Attentive Stats Pooling - https://arxiv.org/pdf/1803.10963.pdf\n",
    "- AAM Softmax - https://arxiv.org/pdf/1906.07317.pdf\n",
    "\n",
    "Also you can optionally add other settings for paper:\n",
    "- SpecAug\n",
    "- Weight decay for optimizer\n",
    "- LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, input_shape: int, reduction: int):\n",
    "        super().__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Conv1d(input_shape, input_shape // reduction, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(input_shape // reduction, input_shape, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.se(X) * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = SEBlock(128, 16)\n",
    "X = torch.randn(10, 128, 10)\n",
    "b(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res2Net(nn.Module):\n",
    "    def __init__(self, hidden: int, dilation: int, scale: int=8):\n",
    "        super().__init__()\n",
    "        assert hidden % scale == 0\n",
    "        self.hidden = hidden\n",
    "        self.scale = scale\n",
    "        self.dconv3 = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(hidden // scale, hidden // scale, 3, dilation=dilation, padding='same'),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden // scale)\n",
    "            ) for _ in range(scale)\n",
    "        ])\n",
    "\n",
    "    def __call__(self, X):\n",
    "        split = torch.split(X, self.hidden // self.scale, dim=1)\n",
    "        split = [conv(x) for conv, x in zip(self.dconv3, split)]\n",
    "        out = torch.cat(split, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = Res2Net(128, 2)\n",
    "X = torch.randn(10, 128, 10)\n",
    "b(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcapaBlock(nn.Module):\n",
    "    def __init__(self, hidden: int, dilation: int, scale: int=8):\n",
    "        super().__init__()\n",
    "        assert hidden % scale == 0\n",
    "        self.scale = scale\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(hidden, hidden, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden)\n",
    "        )\n",
    "        self.res2net = Res2Net(hidden, dilation, scale)\n",
    "        self.conv1_2 = nn.Sequential(\n",
    "            nn.Conv1d(hidden, hidden, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden)\n",
    "        )\n",
    "        self.se = SEBlock(hidden, scale)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        out = self.conv1(X)\n",
    "        out = self.res2net(out)\n",
    "        out = self.conv1_2(out)\n",
    "        out = self.se(out)\n",
    "        return out + X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = EcapaBlock(128, 2)\n",
    "x = torch.randn(10, 128, 10)\n",
    "b(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveStatsPooling(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden: int):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv1d(input_shape * 3, hidden, 1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv1d(hidden, 1, 1),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def __call__(self, X):\n",
    "        # X shape = [time, feats]\n",
    "        # calc mean and std for X over time dimension\n",
    "        # concatenate mean and std to X over feats dimension to make shape [time, feats * 3]\n",
    "        # attention\n",
    "        # weighted mean and std with weights from attention for original X\n",
    "        \n",
    "        means = X.mean(dim=-1, keepdim=True) * torch.ones_like(X)\n",
    "        stds = X.std(dim=-1, keepdim=True) * torch.ones_like(X)\n",
    "        features = torch.cat([means, stds, X], dim=-2)\n",
    "        weights = self.attention(features)\n",
    "        mean = (weights * X).sum(dim=-1, keepdim=True)\n",
    "        std = ((weights * X * X).sum(dim=-1, keepdim=True) - mean * mean + 1e-5).sqrt()\n",
    "        return torch.cat([mean, std], dim=-2).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.6536e+00,  5.9152e+00, -3.6226e+00, -2.1261e+00, -7.6340e+00,\n",
       "          2.4891e+00, -6.0967e+00,  3.8034e-01,  3.6055e-01,  3.6353e+00,\n",
       "         -1.4687e+00,  1.0973e+00,  1.0869e+01,  9.6953e+00,  9.4925e+00,\n",
       "          9.5407e+00,  8.6853e+00,  1.0268e+01,  6.2653e+00,  1.0849e+01,\n",
       "          7.5544e+00,  6.7990e+00,  8.6921e+00,  8.7302e+00],\n",
       "        [ 8.3064e-01, -1.3538e+00, -7.5505e+00, -5.5487e+00, -2.6887e-01,\n",
       "         -7.9378e+00, -4.5483e-01, -2.1605e+00,  8.8248e+00, -5.5592e-01,\n",
       "          2.8720e+00, -5.4973e+00,  1.0776e+01,  1.0888e+01,  6.3402e+00,\n",
       "          1.0170e+01,  1.0694e+01,  7.7288e+00,  8.6547e+00,  9.8982e+00,\n",
       "          1.0892e+01,  3.1806e+00,  9.9687e+00,  5.1868e+00],\n",
       "        [-1.5809e+00,  8.8868e+00,  2.4897e+00,  2.2863e+00,  1.1140e+00,\n",
       "          3.5438e+00,  1.6368e+00,  1.2423e+00, -4.9613e+00, -1.5333e-01,\n",
       "         -1.9147e+00,  1.2995e+00,  9.7003e+00,  9.3914e+00,  1.1934e+01,\n",
       "          9.7183e+00,  1.2247e+01,  8.4260e+00,  7.2929e+00,  8.9852e+00,\n",
       "          7.5215e+00,  6.7991e+00,  9.1891e+00,  5.9746e+00],\n",
       "        [ 1.2312e+00,  8.1025e-01,  1.4708e+00,  7.1907e+00, -5.8548e+00,\n",
       "         -3.1438e+00, -3.6381e+00, -1.4825e+00, -3.6905e+00, -1.2690e+00,\n",
       "         -1.5229e+00, -5.4106e+00,  1.1415e+01,  1.0132e+01,  6.3881e+00,\n",
       "          9.9533e+00,  7.8566e+00,  9.4664e+00,  1.2422e+01,  1.0388e+01,\n",
       "          8.9095e+00,  1.4233e+01,  1.0252e+01,  1.2724e+01],\n",
       "        [-4.6113e+00, -1.6033e+00, -1.0305e+00,  5.7300e+00,  4.0340e+00,\n",
       "         -2.3050e-01, -8.6503e+00,  8.7327e-02,  2.6750e+00,  2.7807e+00,\n",
       "          4.3284e+00,  2.3074e+00,  1.2857e+01,  6.8144e+00,  6.0826e+00,\n",
       "          6.6872e+00,  5.8597e+00,  5.8486e+00,  9.4986e+00,  4.6802e+00,\n",
       "          9.1994e+00,  6.7627e+00,  1.0915e+01,  6.5456e+00],\n",
       "        [-5.3347e+00, -4.0703e+00, -6.8338e-01,  4.7949e+00,  3.0834e-01,\n",
       "          9.6631e-01, -2.9944e+00,  3.9859e+00,  3.5292e+00, -1.5556e+00,\n",
       "          1.6223e+00, -1.1837e+00,  5.6601e+00,  7.1926e+00,  1.0238e+01,\n",
       "          8.7329e+00,  7.6113e+00,  6.2795e+00,  8.4986e+00,  7.8910e+00,\n",
       "          9.4901e+00,  1.5040e+01,  9.4981e+00,  9.7034e+00],\n",
       "        [ 2.4737e+00,  1.9909e+00,  1.0826e+00,  2.2585e+00, -2.7388e+00,\n",
       "          7.6491e-01,  6.0323e+00,  1.1137e+00,  1.0596e+01, -2.9769e+00,\n",
       "         -1.4224e+00, -6.5698e-01,  5.7886e+00,  9.2195e+00,  5.2260e+00,\n",
       "          9.0315e+00,  1.2256e+01,  7.8199e+00,  8.8578e+00,  5.4559e+00,\n",
       "          1.1828e+01,  1.1855e+01,  1.1021e+01,  5.4044e+00],\n",
       "        [-2.1292e+00, -5.9196e-01,  2.4405e+00,  6.6773e+00, -4.2970e+00,\n",
       "         -3.5127e+00, -7.4902e-02, -1.7015e+00,  4.7167e+00, -4.7600e+00,\n",
       "         -1.6186e+00, -4.3748e+00,  7.8396e+00,  1.2511e+01,  1.0418e+01,\n",
       "          8.6311e+00,  8.7507e+00,  9.6082e+00,  9.5233e+00,  6.2023e+00,\n",
       "          5.7200e+00,  9.6571e+00,  6.3017e+00,  1.1114e+01],\n",
       "        [-6.1423e+00,  8.1995e-01,  3.4336e+00,  5.1417e-03, -1.0749e+00,\n",
       "          1.0537e+00, -1.1370e-01, -8.4911e-01,  1.6418e+00,  3.4287e-01,\n",
       "         -3.1544e+00,  1.5602e+00,  7.9322e+00,  7.8197e+00,  7.0285e+00,\n",
       "          7.1173e+00,  5.3953e+00,  8.6712e+00,  1.1399e+01,  1.5542e+01,\n",
       "          8.0342e+00,  1.0487e+01,  9.4625e+00,  1.0320e+01],\n",
       "        [-1.0216e-02,  3.4191e+00, -4.3623e+00,  4.2923e-01,  5.1034e+00,\n",
       "          1.7658e+00, -5.1557e+00,  4.8074e+00, -1.6546e+00,  2.1125e+00,\n",
       "          9.2858e-01,  3.2525e+00,  1.0862e+01,  6.4767e+00,  6.1627e+00,\n",
       "          1.3849e+01,  7.5092e+00,  1.0541e+01,  8.3349e+00,  9.0679e+00,\n",
       "          4.2166e+00,  8.0029e+00,  1.3931e+01,  8.7745e+00]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = AttentiveStatsPooling(12, 6)\n",
    "x = torch.randn(10, 12, 10) * -10\n",
    "b(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAMSoftmax(nn.Module):\n",
    "    def __init__(self, input_shape, n_class, margin, scale):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(n_class, input_shape, max_norm=1)\n",
    "        self.theta = nn.CosineSimilarity()\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, X):\n",
    "        # calc cosine similarity between X and weights\n",
    "        # theta = angle from cosine similarity\n",
    "        # return matrix S, where S_ij = \n",
    "        #     \\log \\frac{\n",
    "        #         \\exp{scale \\cos{theta_ij + margin}}\n",
    "        #     }{\n",
    "        #         \\exp{scale \\cos{theta_ij + margin}} + \\sum_{k != j} \\exp{scale \\cos{theta_ij}}\n",
    "        #     }\n",
    "        \n",
    "        theta = torch.acos(self.theta(X.unsqueeze(2), self.emb.weight.T.unsqueeze(0)))\n",
    "        S = torch.log(\n",
    "            torch.exp(self.scale * torch.cos(theta + self.margin)) /\n",
    "            (torch.exp(self.scale * torch.cos(theta + self.margin)) + torch.sum(torch.exp(self.scale * torch.cos(theta)), dim=1, keepdim=True) - torch.exp(self.scale * torch.cos(theta)))\n",
    "        )\n",
    "        return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -8.0715, -10.8181,  -5.8154, -10.4713, -12.4669, -10.1200, -12.2225,\n",
       "          -6.7858, -11.2043, -12.0875],\n",
       "        [ -5.1081,  -9.9891, -11.2252,  -7.4785,  -9.5597, -10.4493, -11.9912,\n",
       "         -12.8229, -11.7363,  -9.0960],\n",
       "        [-11.7375, -14.0239,  -2.1667, -14.7877, -13.6384, -16.4621, -11.0912,\n",
       "         -13.5289, -10.5784, -14.7320],\n",
       "        [-14.2263, -13.5780,  -7.3093, -11.8477, -12.7734, -16.3002,  -5.1578,\n",
       "          -8.8525,  -9.4697, -11.3346],\n",
       "        [-10.9958, -16.3771,  -8.4261, -11.0402,  -9.8127, -10.9826,  -4.3955,\n",
       "         -10.2800,  -9.4303, -11.4841],\n",
       "        [ -9.5727, -14.9544,  -4.3860, -10.7746, -11.9505, -10.8218,  -8.1231,\n",
       "         -10.9900, -10.5845, -15.5693],\n",
       "        [-12.3367, -12.0513, -14.0570, -11.0848, -12.4040, -13.6859,  -8.8491,\n",
       "         -12.5322, -11.6454,  -3.4239],\n",
       "        [ -8.8988,  -6.5334,  -7.9703, -10.3709,  -9.7687,  -8.8104,  -6.8917,\n",
       "          -8.6054,  -9.6282, -13.0748],\n",
       "        [ -9.1365, -11.6024, -11.4743, -13.6553,  -7.4237, -15.0829, -13.0049,\n",
       "         -13.0185,  -4.8542, -13.0575],\n",
       "        [ -9.0452,  -8.5522, -11.9435,  -9.7777, -14.1473,  -4.4420, -12.1289,\n",
       "         -11.2143, -11.4132,  -9.7317]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = AAMSoftmax(128, 10, 0.2, 30)\n",
    "x = torch.randn(10, 128)\n",
    "b(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcapaTDNN(nn.Module):\n",
    "    def __init__(self, input_shape: int, output_shape: int, hidden: int, embedding_shape: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(input_shape, hidden, 5, padding='same')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm1d(hidden)\n",
    "        self.block1 = EcapaBlock(hidden, 2)\n",
    "        self.block2 = EcapaBlock(hidden, 3)\n",
    "        self.block3 = EcapaBlock(hidden, 4)\n",
    "        self.conv1_2 = nn.Sequential(\n",
    "            nn.Conv1d(hidden * 3, hidden * 3, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.pool = AttentiveStatsPooling(hidden * 3, hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden * 6)\n",
    "        self.fc = nn.Linear(hidden * 6, embedding_shape)\n",
    "        self.bn2 = nn.BatchNorm1d(embedding_shape)\n",
    "        self.aam_sm = AAMSoftmax(embedding_shape, output_shape, 0.2, 30)\n",
    "            \n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.conv1(X)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn(out)\n",
    "        b1 = self.block1(out)\n",
    "        b2 = self.block2(b1)\n",
    "        b3 = self.block3(b2)\n",
    "        out = self.conv1_2(torch.cat([b1, b2, b3], dim=1))\n",
    "        out = self.pool(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.bn2(out)\n",
    "        return self.aam_sm(out), out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ECAPA model, at this point you can archive stable score (for several consecutive epochs) near 0.08 EER.\n",
    "\n",
    "You can train ECAPA with hidden size 256 to increase speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EcapaTDNN(FEATS, trainset.speakers(), 256, 300).to(DEVICE)\n",
    "opt = optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(opt, base_lr=3e-4, max_lr=1e-1, mode=\"triangular2\", step_size_up=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model weight: 10544164, batch weight: 157286400\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for param in model.parameters():\n",
    "    num += param.numel() * 4\n",
    "print(f\"model weight: {num}, batch weight: {256 * 6 * 800 * 32 * 4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Train loss 10.833275627111027. Eval score 0.21024858541547461\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710f41767eba413b91be96e38a022a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fe00254b0d42898d50057daad9977a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, opt, scheduler, batch_size, epochs, train_fun)\u001b[0m\n\u001b[0;32m     13\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_fun(model, opt, scheduler, batch_size\u001b[38;5;241m=\u001b[39mbatch_size))\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 16\u001b[0m eval_scores\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcalc_eval_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     17\u001b[0m clear_output()\n\u001b[0;32m     18\u001b[0m fig, axis \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m, in \u001b[0;36mcalc_eval_score\u001b[1;34m(model, batch_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, _, pathes \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(loader):\n\u001b[1;32m---> 12\u001b[0m         _, embds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m         embds \u001b[38;5;241m=\u001b[39m embds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m embd, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(embds, pathes):\n",
      "Cell \u001b[1;32mIn[22], line 26\u001b[0m, in \u001b[0;36mEcapaTDNN.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     24\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(out)\n\u001b[0;32m     25\u001b[0m b1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock1(out)\n\u001b[1;32m---> 26\u001b[0m b2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m b3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock3(b2)\n\u001b[0;32m     28\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1_2(torch\u001b[38;5;241m.\u001b[39mcat([b1, b2, b3], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[1;32mIn[16], line 21\u001b[0m, in \u001b[0;36mEcapaBlock.__call__\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     20\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(X)\n\u001b[1;32m---> 21\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mres2net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1_2(out)\n\u001b[0;32m     23\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mse(out)\n",
      "Cell \u001b[1;32mIn[14], line 17\u001b[0m, in \u001b[0;36mRes2Net.__call__\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     16\u001b[0m     split \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     split \u001b[38;5;241m=\u001b[39m [conv(x) \u001b[38;5;28;01mfor\u001b[39;00m conv, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdconv3, split)]\n\u001b[0;32m     18\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(split, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[1;32mIn[14], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     16\u001b[0m     split \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     split \u001b[38;5;241m=\u001b[39m [\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m conv, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdconv3, split)]\n\u001b[0;32m     18\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(split, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, opt, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model0.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive losses (10 points)\n",
    "\n",
    "You can use anyone constrative loss.\n",
    "Good article with contrastive losses https://lilianweng.github.io/posts/2021-05-31-contrastive/\n",
    "\n",
    "Base losses:\n",
    "- contrastive\n",
    "- triplet -- it gives a better quality usually\n",
    "- lifted structured loss -- better batch data utilization\n",
    "\n",
    "The main problem with contrastive loss is the positive pairs sampler.\n",
    "This is because a large number of classes provided only once per batch\n",
    "in case of large number of classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositivePairsSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, speakers: list[int], batch_size: int):\n",
    "        self.len = len(speakers) // batch_size // 2 * 2 * batch_size\n",
    "        self.batch_size = batch_size\n",
    "        idxs = torch.randperm(self.len // 2)\n",
    "        self.pos = [x.item() for xs in zip(2 * idxs[::2], 2 * idxs[::2] + 1) for x in xs]\n",
    "        self.rand = (2 * idxs[1::2]).tolist() + (2 * idxs[1::2] + 1).tolist()\n",
    "        self.idx = []\n",
    "        for i in range(len(self)):\n",
    "            self.idx += self.pos[i * self.batch_size // 2: (i + 1) * self.batch_size // 2] + self.rand[i * self.batch_size // 2: (i + 1) * self.batch_size // 2]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.len + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        # yield __len__ batches as list of indexes of samples from dataset\n",
    "        for batch in torch.chunk(torch.tensor(self.idx), len(self)):\n",
    "            yield batch.tolist()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 906,  906,  260,  260,  817,  817,  186,  186,  616,  810,  681, 1065,\n",
      "         488,  235,  278,  481])\n",
      "tensor([1070, 1070,  225,  225,  422,  422,  693,  693,  834, 1066,  452,  157,\n",
      "         219,  584, 1150,  873])\n",
      "tensor([ 879,  879,   53,   53,  234,  234,  158,  158,  952,  990, 1092,  310,\n",
      "         924,  297,  802, 1003])\n"
     ]
    }
   ],
   "source": [
    "loader = torch_data.DataLoader(\n",
    "        trainset,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "        num_workers=LOADER_WORKERS,\n",
    "        batch_sampler=PositivePairsSampler(trainset._speakers, 16)\n",
    "    )\n",
    "it = iter(loader)\n",
    "for _ in range(3):\n",
    "    _, Y, _ = next(it)\n",
    "    print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_train_stage(model, opt, scheduler, batch_size: int = 32, lam = .2, eps = 1.47):\n",
    "    # You can use any contrastive loss here to improve training\n",
    "    # You can combine contrastive loss with the NLL loss after AAM softmax to improve stability\n",
    "    loader = torch_data.DataLoader(\n",
    "        trainset,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "        num_workers=LOADER_WORKERS,\n",
    "        batch_sampler=PositivePairsSampler(trainset._speakers, batch_size)\n",
    "    )\n",
    "    loss_sum = 0.0\n",
    "    batches = 0\n",
    "    i = 0\n",
    "    for X, Y, _ in tqdm.tqdm(loader):\n",
    "        logits, embeddings = model.forward(X.to(DEVICE))\n",
    "        y = Y.to(DEVICE)\n",
    "        loss1 = F.nll_loss(logits, y)\n",
    "        pos = y[::2] == y[1::2]\n",
    "        embeddings = embeddings / torch.linalg.vector_norm(embeddings, dim=-1).unsqueeze(-1)\n",
    "        norm = torch.linalg.vector_norm(embeddings[::2] - embeddings[1::2], dim=-1)\n",
    "        loss2 = (pos * norm + (~pos) * torch.max(torch.tensor(0), eps - norm)).mean()\n",
    "        loss = loss1 + lam * loss2\n",
    "        loss_sum += loss.item()\n",
    "        batches += 1\n",
    "        loss.backward()\n",
    "        if (i + 1) % 4 == 0:\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "            opt.zero_grad()\n",
    "        i += 1\n",
    "    return loss_sum / batches\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model with contrastive loss here. At this point you can archive EER near 0.06-0.07 (it should be at least on 0.005 to 0.01 better than before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_16792\\675746989.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('model0.pt').to(DEVICE)\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('model0.pt').to(DEVICE)\n",
    "opt = optim.Adam(model.parameters(), lr = 2e-5, weight_decay=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(opt, base_lr=2e-5, max_lr=1e-1, mode=\"triangular2\", step_size_up=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c8749304b240829233eebbcff6e241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_fun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrastive_train_stage\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, opt, scheduler, batch_size, epochs, train_fun)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 13\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     15\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     16\u001b[0m     eval_scores\u001b[38;5;241m.\u001b[39mappend(calc_eval_score(model, batch_size\u001b[38;5;241m=\u001b[39mbatch_size))\n",
      "Cell \u001b[1;32mIn[29], line 14\u001b[0m, in \u001b[0;36mcontrastive_train_stage\u001b[1;34m(model, opt, scheduler, batch_size, lam, eps)\u001b[0m\n\u001b[0;32m     12\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, Y, _ \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(loader):\n\u001b[1;32m---> 14\u001b[0m     logits, embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     15\u001b[0m     y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     16\u001b[0m     loss1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(logits, y)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, opt, scheduler, train_fun=contrastive_train_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
